[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Maxime Labonne",
    "section": "",
    "text": "üëã Greetings! My name is Maxime Labonne, and I am currently a senior applied researcher at JPMorgan Chase. I hold a M.Sc. degree in computer science from INSA CVL, and a Ph.D.¬†in machine learning and cyber security from the Polytechnic Institute of Paris.\nIn this blog, I aim to provide practical tutorials and guides on AI and machine learning. I believe in starting with implementation and practice, and then diving into explanations. By retracing the thought process of the original authors of the algorithms we use every day, we can gain a deeper understanding of their design and the importance of each component.\nI hope we can have fun together and learn a few things along the way!\nIf you want to contact me, feel free to send a mail at this address.\n\nCredits:\n\nEmojis used in figures are designed by OpenMoji, the open-source emoji and icon project. License: CC BY-SA 4.0.\nVector icons are provided by Streamline (https://streamlinehq.com). License: CC BY-SA 4.0."
  },
  {
    "objectID": "book.html",
    "href": "book.html",
    "title": "Hands-On Graph Neural Networks Using Python",
    "section": "",
    "text": "I am thrilled to introduce my book on graph neural networks, which is the result of almost a year‚Äôs worth of hard work, research, and collaboration with fellow experts in the field.\nWhen I started learning about GNNs, resources were particularly scarce. This book is the guide I wish I had back then: it has been carefully crafted to provide a step-by-step guide for those new to the world of GNNs, while also offering advanced use cases and examples for experienced practitioners. What‚Äôs more, the entire code with examples and use cases is freely available on  Github, making it easy for you to get started with implementing GNNs in Python.\nIf you‚Äôre interested in graph neural networks, I highly recommend checking it out. I‚Äôm confident that you‚Äôll find it to be an invaluable resource as you explore this exciting and rapidly growing field.\n\n\n  Get your copy!"
  },
  {
    "objectID": "book.html#key-features",
    "href": "book.html#key-features",
    "title": "Hands-On Graph Neural Networks Using Python",
    "section": "üìò Key Features",
    "text": "üìò Key Features\n\nImplement state-of-the-art graph neural network architectures in Python\nCreate your own graph datasets from tabular data\nBuild powerful traffic forecasting, recommender systems, and anomaly detection applications"
  },
  {
    "objectID": "book.html#book-description",
    "href": "book.html#book-description",
    "title": "Hands-On Graph Neural Networks Using Python",
    "section": "üíº Book Description",
    "text": "üíº Book Description\nGraph neural networks are a highly effective tool for analyzing data that can be represented as a graph, such as social networks, chemical compounds, or transportation networks. The past few years have seen an explosion in the use of graph neural networks, with their application ranging from natural language processing and computer vision to recommendation systems and drug discovery.\nHands-On Graph Neural Networks Using Python begins with the fundamentals of graph theory and shows you how to create graph datasets from tabular data. As you advance, you‚Äôll explore major graph neural network architectures and learn essential concepts such as graph convolution, self-attention, link prediction, and heterogeneous graphs. Finally, the book proposes applications to solve real-life problems, enabling you to build a professional portfolio. The code is readily available online and can be easily adapted to other datasets and apps.\nBy the end of this book, you‚Äôll have learned to create graph datasets, implement graph neural networks using Python and PyTorch Geometric, and apply them to solve real-world problems, along with building and training graph neural network models for node and graph classification, link prediction, and much more!"
  },
  {
    "objectID": "book.html#what-you-will-learn",
    "href": "book.html#what-you-will-learn",
    "title": "Hands-On Graph Neural Networks Using Python",
    "section": "üîë What you will learn",
    "text": "üîë What you will learn\n\nUnderstand the fundamental concepts of graph neural networks\nImplement graph neural networks using Python and PyTorch Geometric\nClassify nodes, graphs, and edges using millions of samples\nPredict and generate realistic graph topologies\nCombine heterogeneous sources to improve performance\nForecast future events using topological information\nApply graph neural networks to solve real-world problems"
  },
  {
    "objectID": "book.html#who-this-book-is-for",
    "href": "book.html#who-this-book-is-for",
    "title": "Hands-On Graph Neural Networks Using Python",
    "section": "üí° Who this book is for",
    "text": "üí° Who this book is for\nThis book is for machine learning practitioners and data scientists interested in learning about graph neural networks and their applications, as well as students looking for a comprehensive reference on this rapidly growing field. Whether you‚Äôre new to graph neural networks or looking to take your knowledge to the next level, this book has something for you. Basic knowledge of machine learning and Python programming will help you get the most out of this book.\n\n  Get your copy!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": " Articles",
    "section": "",
    "text": "Quantize Llama models with GGML and llama.cpp\n\n\nGGML vs.¬†GPTQ vs.¬†NF4\n\n\n\n\nLarge Language Models\n\n\n \n\n\n\n\nSep 3, 2023\n\n\n9 min\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nA Beginner‚Äôs Guide to LLM Fine-Tuning\n\n\nHow to fine-tune Llama and other LLMs with one tool\n\n\n\n\nLarge Language Models\n\n\n \n\n\n\n\nAug 27, 2023\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\n4-bit LLM Quantization with GPTQ\n\n\nQuantize your own open-source LLMs to run them on consumer hardware\n\n\n\n\nLarge Language Models\n\n\n \n\n\n\n\nJul 30, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nFine-Tune Your Own Llama 2 Model in a Colab Notebook\n\n\nA practical introduction to LLM fine-tuning\n\n\n\n\nLarge Language Models\n\n\n \n\n\n\n\nJul 24, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Weight Quantization\n\n\nLarge language model optimization using 8-bit quantization\n\n\n\n\nLarge Language Models\n\n\n \n\n\n\n\nJul 6, 2023\n\n\n12 min\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nImprove ChatGPT with Knowledge Graphs\n\n\nLeveraging knowledge graphs for LLMs using LangChain\n\n\n\n\nLarge Language Models\n\n\n \n\n\n\n\nJun 20, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nDecoding Strategies in Large Language Models\n\n\nA Guide to Text Generation From Beam Search to Nucleus Sampling\n\n\n\n\nLarge Language Models\n\n\n \n\n\n\n\nJun 7, 2023\n\n\n12 min\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nOptimize Your Marketing Budget with Nonlinear Programming\n\n\nIntroduction to CVXPY to maximize marketing ROI\n\n\n\n\nlinear programming\n\n\n \n\n\n\n\nMay 21, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nCreate a Bot to Find Diamonds in Minecraft\n\n\nReinforcement Learning and Behavior Cloning in Python with MineRL\n\n\n\n\nreinforcement learning\n\n\n \n\n\n\n\nMay 25, 2022\n\n\n8 min\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Constraint Programming in Python\n\n\nThe Programming Paradigm to Find One Solution Among 8,080,104 Candidates\n\n\n\n\nlinear programming\n\n\n \n\n\n\n\nMay 2, 2022\n\n\n8 min\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nGIN: How to Design the Most Powerful Graph Neural Network\n\n\nGraph Neural Network Course: Chapter 4\n\n\n\n\ngraph neural networks\n\n\n \n\n\n\n\nApr 25, 2022\n\n\n9 min\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nGraphSAGE: Scaling up Graph Neural Networks\n\n\nGraph Neural Network Course: Chapter 3\n\n\n\n\ngraph neural networks\n\n\n \n\n\n\n\nApr 6, 2022\n\n\n10 min\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWhat is a Tensor in Machine Learning?\n\n\nThe difference between tensors, arrays, and¬†matrices\n\n\n\n\ndata science\n\n\n \n\n\n\n\nMar 28, 2022\n\n\n5 min\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nEfficiently iterating over rows in a Pandas DataFrame\n\n\nNever use iterrows and itertuples again!\n\n\n\n\ndata science\n\n\n \n\n\n\n\nMar 21, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nGraph Attention Networks: Self-Attention for GNNs\n\n\nGraph Neural Network Course: Chapter 2\n\n\n\n\ngraph neural networks\n\n\n \n\n\n\n\nMar 9, 2022\n\n\n9 min\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nInteger vs.¬†Linear Programming in Python\n\n\nMixed Integer Programming for optimization with Google OR-Tools\n\n\n\n\nlinear programming\n\n\n \n\n\n\n\nMar 5, 2022\n\n\n9 min\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Linear Programming in Python\n\n\nA guide to mathematical optimization with Google OR-Tools\n\n\n\n\nlinear programming\n\n\n \n\n\n\n\nMar 2, 2022\n\n\n8 min\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nGraph Convolutional Networks: Introduction to GNNs\n\n\nGraph Neural Network Course: Chapter 1\n\n\n\n\ngraph neural networks\n\n\n \n\n\n\n\nFeb 20, 2022\n\n\n13 min\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nQ-learning for beginners\n\n\nTrain an AI to solve the Frozen Lake environment\n\n\n\n\nreinforcement learning\n\n\n \n\n\n\n\nFeb 13, 2022\n\n\n20 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/Large Language Models/extending_context.html",
    "href": "notes/Large Language Models/extending_context.html",
    "title": "Extending the Context Window of LLMs",
    "section": "",
    "text": "üìù Article: https://kaiokendev.github.io/context"
  },
  {
    "objectID": "notes/Large Language Models/extending_context.html#problem",
    "href": "notes/Large Language Models/extending_context.html#problem",
    "title": "Extending the Context Window of LLMs",
    "section": "Problem",
    "text": "Problem\nProblem: it is hard to extend the sequence length of a model.\n\nAnil et al.¬†(2022): the length extrapolation fails in part because of ‚Äúdistracting tokens‚Äù in the input during the PARITY task.\nChi et al.¬†(2022): bias terms in positional encoding (like in ALiBi) replicate the effect of windowed attention by decaying token inter-dependency on long-range receptive fields (the tokens only focus on the tokens closest to them).\nTao et al.¬†(2023) observe that, in long sequences, rear position embeddings are updates much fewer times than front position embeddings. They add random padding to the front patch of the sequence.\nLiu et al.¬†(2023): attention in long sequences starts to drift as we move to later positions and only attends to the most recent tokens."
  },
  {
    "objectID": "notes/Large Language Models/extending_context.html#silver-linings",
    "href": "notes/Large Language Models/extending_context.html#silver-linings",
    "title": "Extending the Context Window of LLMs",
    "section": "Silver Linings",
    "text": "Silver Linings\nThe attention mechanism seems destabilized in the case of long sequences due to an imbalance of attended tokens (either skewed to the front or the back).\nSeveral solutions have been proposed:\n\nFew-shot chain-of-thought reasoning and marker tokens\nLength generalization/extrapolation can be learned ability to a certain extent (improves performance but not a silver bullet)\nLLaMa 7B has been trained for retrieval over a 32K token window by introducing landmark tokens combined with a windowed-attention (blockwise computation)."
  },
  {
    "objectID": "notes/Large Language Models/extending_context.html#potential-solutions",
    "href": "notes/Large Language Models/extending_context.html#potential-solutions",
    "title": "Extending the Context Window of LLMs",
    "section": "Potential Solutions",
    "text": "Potential Solutions\n\nChange the attention calculation: log(n) scaling (does help), relacing the softmax with ReLU in the attention equation (does not converge), etc.\nRandom Positional Encoding\nShifted Positional Encodings: shifting the tokens progressively along the desired length during the encoding step (failure)."
  },
  {
    "objectID": "notes/Large Language Models/extending_context.html#final-solution",
    "href": "notes/Large Language Models/extending_context.html#final-solution",
    "title": "Extending the Context Window of LLMs",
    "section": "Final Solution",
    "text": "Final Solution\nTransformers do not learn how to gauge position based on the relative distance or the rotational factors, but memorize the tokens and their positional scaling factors.\n\nRotary positional embedding to loop the positions around after crossing the max context length (e.g., 2048): position_ids = position_ids % 2048\nBlock repeated positions: repeating the chosen frequency for a block of positions, so [1, 2, 3, 4, 5, ‚Ä¶, L] becomes [1, 1, 1, 1, 2, 2, 2, 2, 3, ‚Ä¶, L]. This is achieved by changing the frequency update: t *= 1/4.\n\nIn other words, several tokens (4 in this example) are assigned to the same position. This (surprising) scheme can quadruple the context length with minimal performance degradation (~2%). More information about it in this paper from Meta: https://arxiv.org/abs/2306.15595"
  },
  {
    "objectID": "notes/Large Language Models/few_shot_text_classification.html",
    "href": "notes/Large Language Models/few_shot_text_classification.html",
    "title": "Report ‚Äì Few-Shot Text Classification",
    "section": "",
    "text": "üìù Article: https://few-shot-text-classification.fastforwardlabs.com/ (2020)"
  },
  {
    "objectID": "notes/Large Language Models/few_shot_text_classification.html#embedding-models",
    "href": "notes/Large Language Models/few_shot_text_classification.html#embedding-models",
    "title": "Report ‚Äì Few-Shot Text Classification",
    "section": "Embedding models",
    "text": "Embedding models\n\nStatic: Bag-of-words and tf-idf vectors\nStatic: word2vec and GloVe\nDynamic: ELMo and BERT\nSentence/paragraphs: Doc2Vec, InferSent, Universal Sentence Encoder"
  },
  {
    "objectID": "notes/Large Language Models/few_shot_text_classification.html#problem-with-bert",
    "href": "notes/Large Language Models/few_shot_text_classification.html#problem-with-bert",
    "title": "Report ‚Äì Few-Shot Text Classification",
    "section": "Problem with BERT",
    "text": "Problem with BERT\nBERT outputs an embedding vector for each input token, like the CLS token for ‚Äúclassification‚Äù. Experiments have shown that using the CLS token as sentence-level feature representation drastically underperforms aggregated GloVe embeddings in semantic similarity tests.\nInstead, we can pool together the individual embedding vectors for each word token (like we pool word2vec vectors). However, these embeddings are not optimized for similarity, nor can they be expected to capture the semantic meaning of full sentences or paragraphs.\nAnother solution consists of training BERT to specifically learn semantic similarity between sentences. This procedure provides good results but is not efficient, as BERT can only compare two text segments at a time (very slow)."
  },
  {
    "objectID": "notes/Large Language Models/few_shot_text_classification.html#sentence-bert",
    "href": "notes/Large Language Models/few_shot_text_classification.html#sentence-bert",
    "title": "Report ‚Äì Few-Shot Text Classification",
    "section": "Sentence-BERT",
    "text": "Sentence-BERT\nSentence-BERT (SBERT) addresses these issues (2019). It adds a pooling operation to the output of BERT to derive fixed sentence embeddings, followed by fine-tuning with a triplet network, in order to produce embeddings with semantically meaningful relationships. It outperforms aggregated word2vec and BERT embeddings in similarity tasks."
  },
  {
    "objectID": "notes/Large Language Models/few_shot_text_classification.html#zmap",
    "href": "notes/Large Language Models/few_shot_text_classification.html#zmap",
    "title": "Report ‚Äì Few-Shot Text Classification",
    "section": "Zmap",
    "text": "Zmap\nThe authors want to perform multi-class text classification using the embeddings of the labels. This has several interesting features that are detailed in the report, such as a dynamic number of classes (they‚Äôre no long fixed by the training process).\nThe problem is that the embeddings produced by SBERT are good for sentences and paragraphs, but quite poor for individual words. On the contrary, this is where simpler embedding techniques, such as word2vec and GloVe, perform best. Unfortunately, we cannot naively compare (e.g., using a cosine similarity) embeddings from SBERT and those from word2vec.\n\nThe idea is learning a mapping between words in SBERT space and the same words in word2vec space. It is performed by selecting a large vocabulary of words and obtain SBERT and word2vec representations for each one. We can then learn a matrix Z using the least-squares linear regression with l2 regularization between these representations."
  },
  {
    "objectID": "notes/Large Language Models/gptq.html",
    "href": "notes/Large Language Models/gptq.html",
    "title": "GPTQ ‚Äì Accurate Post-Training Quantization for Generative Pre-trained Transformers",
    "section": "",
    "text": "Tip\n\n\n\nThis paper introduces GPTQ, the first reliable quantization technique to peform 4- and even 3-bit quantization on very large language models.\nüìù Paper: https://arxiv.org/abs/2210.17323\nProblem: LLMs demonstrated breakthrough performance across complex language modelings tasks, but also extremely high computational and storage costs. Even inference requires multiple GPUs for big LLMs. For example, GPT3-175B takes 326GB of memory in FP16.\nSolution: GPTQ, one-shot weight quantization based on approximate second-order information. With this technique, you can quantize 175B parameters model in ~4 GPU hours.\nStandard approach = model compression. Little is known about compressing such models for inference. One reason is that you need model retraining for more complex methods or model pruning. This makes post-training methods appealing because you don‚Äôt need an expensive re-training stage.\nFirst to show quantization to 3-4 bits/component for very large LLMs."
  },
  {
    "objectID": "notes/Large Language Models/gptq.html#related-work",
    "href": "notes/Large Language Models/gptq.html#related-work",
    "title": "GPTQ ‚Äì Accurate Post-Training Quantization for Generative Pre-trained Transformers",
    "section": "Related work",
    "text": "Related work\nOptimal Brain Quantization generalizes the classic Optimal Brain Surgeon second-order weight pruning framework to apply to quantization. OBQ quantizes weights one-by-one, in order of quantization error, always adjusting the remaining weights. It produces good results up to 100 million parameters in a few GPU hours.\nLarge-model Quantization. Existing methods: ZeroQuant, LLM.int8(), nuQmm carefully select quantization granularity, e.g., vector-wise, and round weights to the nearest quantization level.\n\nZeroQuant proposes layer-wise knowledge distillation, similar to AdaQuant but larger model it can apply this approach has 1.3B parameters (takes 3 hours).\nLLM.int8() observes that activation outliers in a few feature dimensions break the quantization of larger models, and proposes to fix this problem by keeping these dimensions in higher precision.\nnuQmm develops efficient GPU kernels for a specific binary-coding based quantization scheme."
  },
  {
    "objectID": "notes/Large Language Models/gptq.html#background",
    "href": "notes/Large Language Models/gptq.html#background",
    "title": "GPTQ ‚Äì Accurate Post-Training Quantization for Generative Pre-trained Transformers",
    "section": "Background",
    "text": "Background\nLayer-Wise Quantization. Performs quantization layer-by-layer, solving a reconstruction problem for each layer. Given a weight matrix \\mathbf{W} and an input matrix \\mathbf{X}, we want to find a quantized weight matrix \\mathbf{\\hat{W}} to minimize the MSE: \\mathbf{\\hat{W}*} = \\arg \\min_{\\mathbf{\\hat{W}}} \\parallel\\mathbf{W X} - \\mathbf{\\hat{W} X}\\parallel_2^2 We assume that the quantization grid for \\mathbf{\\hat{W}} is fixed before the process, and that individual weights can move freely.\n\n\n\n\n\n\nTip\n\n\n\n\nLeCun et al.¬†(1990) released a paper to paper weight pruning iteratively as follows: 1. Train a network 2. Estimate the importance of each weight by watching how the loss would change upon perturbing the weight (smaller change means less importance, also called saliency). 3. Remove the weight with low importance 4. Go back to step 1, retrain the network without the removed weights (=0)\nThey showed you could remove a significant portion of LeNet‚Äôs weights for MNIST classification without a noticable increase in the loss. However, it requires retraining the model after pruning.\nOther approaches: * Frankle and Carbin (2008) proposed the Lottery Ticket Hypothesis, based on the assumption that a randomly-initialized network contains a subnetwork that, when trained in isolation, can match the accuracy of the original network. * Tanaka et al.¬†(2020) introduced SynFlow based on the idea of layer collapse.\n\n\n\nOptimal Brain Quantization. The method relies on the OBS method for solving the layer-wise quantization problem.\nThe OBQ method starts by looking at each row of weights in the weight matrix ùêñ one by one. It tries to simplify one weight at a time, while also updating the remaining weights in that row. The purpose of updating the other weights is to compensate for the changes made by simplifying one weight. This updating process helps to reduce the overall error of the row.\n\nThe corresponding objective is quadratic, whose Hessian if \\mathbf{H}_F = 2 \\mathbf{X}_F \\mathbf{X}_F^\\top.\nF denotes the set of remaining full-precision weights\nw_q denotes the greedy-optimal weight to quantize next\n\\mathbf{\\delta}_F denotes the corresponding optimal update of all weights in F\n\n\\text{quant}(w) rounds w to the nearest value on the quantization grid:\n\nOBQ quantizes weights iteratively using these two equations, until all the weights of \\mathbf{w} are quantized.\nThis process could be computationally heavy, especially for LLMs. To deal with this, the OBQ method uses a trick that avoids redoing the entire computation each time a weight is simplified. After quantizing a weight, it adjusts the matrix used in calculations (the Hessian) by removing the row and column associated with that weight (using Gaussian elimination).\n\nThe method also employs vectorization to processes multiple rows of the weight matrix at once. Despite its efficiency, the OBQ‚Äôs computation time increases significantly as the size of the weight matrix increases. This cubic growth makes it difficult to use OBQ on very large models with billions of parameters."
  },
  {
    "objectID": "notes/Large Language Models/gptq.html#the-gptq-algorithm",
    "href": "notes/Large Language Models/gptq.html#the-gptq-algorithm",
    "title": "GPTQ ‚Äì Accurate Post-Training Quantization for Generative Pre-trained Transformers",
    "section": "The GPTQ Algorithm",
    "text": "The GPTQ Algorithm\nThe GPTQ algorithm takes inspiration from the OBQ method, but with significant improvements to scale it for large language models.\n\nStep 1: Arbitrary Order Insight\nThe OBQ method selects weights (parameters in a model) for quantization in a certain order, determined by which will add the least additional error. However, GPTQ observes that for large models, quantizing weights in any fixed order can perform just as well. This is because even though some weights might introduce more error individually, they are quantized later in the process when there are few other weights left that could increase the error. So the order doesn‚Äôt matter as much as we thought!\nBased on this insight, GPTQ aims to quantize all weights in the same order for all rows of a matrix. This makes the process faster because certain computations have to be done only once for each column, rather than once for each weight.\n\n\nStep 2: Lazy Batch-Updates\nProblem: This scheme won‚Äôt be fast because it requires updating a huge matrix with very few computations for each entry. This type of operation can‚Äôt utilize the full compute capabilities of GPUs and will be slowed down by memory limitations.\nTo resolve this, GPTQ introduces ‚Äúlazy batch‚Äù updates. It turns out that the final rounding decisions for a given column are only affected by updates performed on that column, not on later columns. Therefore, GPTQ can apply the algorithm to a batch of columns at a time (like 128 columns), updating only those columns and a corresponding block of the matrix. After a block is fully processed, the algorithm performs global updates on the entire matrix.\n\n\n\nStep 3: Cholesky Reformulation\nHowever, there‚Äôs one more issue to address. When the algorithm scales up to very large models, numerical inaccuracies can become a problem. Specifically, repeated applications of a certain operation (defined by Equation 5) can accumulate numerical errors.\nTo tackle this, GPTQ uses a Cholesky decomposition, a numerically stable method for solving certain mathematical problems. It involves precomputing some required information from the matrix using the Cholesky method. This approach, combined with a slight ‚Äúdampening‚Äù (adding a small constant to diagonal elements of the matrix), helps the algorithm to avoid numerical issues.\n\n\nThe Full Algorithm\nThe full GPTQ algorithm begins with a Cholesky decomposition of the Hessian inverse (a matrix that helps decide how to adjust the weights). It then runs in loops, handling batches of columns at a time. For each column in a batch, it quantizes the weights, calculates the error, and updates the weights in the block accordingly. After processing the batch, it updates all remaining weights based on the block‚Äôs errors."
  },
  {
    "objectID": "notes/Large Language Models/gptq.html#evaluation",
    "href": "notes/Large Language Models/gptq.html#evaluation",
    "title": "GPTQ ‚Äì Accurate Post-Training Quantization for Generative Pre-trained Transformers",
    "section": "Evaluation",
    "text": "Evaluation\nThe GPTQ algorithm was tested on various language generation tasks. It was compared with other quantization methods, mainly with the current method of choice, RTN. GPTQ was used with the BLOOM and OPT model families, and models were quantized using a single NVIDIA A100 GPU.\nGPTQ performed similarly to state-of-the-art methods on small models like ResNet18 and ResNet50. When tested on larger models like BLOOM and OPT, it performed better than RTN and other methods, especially on a 3and 4-bit scale. For example, when tested on the OPT-175B model, GPTQ had a small decrease in accuracy at 4-bit, while RTN had a significant drop in accuracy. At 3-bit, RTN couldn‚Äôt keep up, while GPTQ still had good results.\nGPTQ also proved to be faster, able to quantize very large models in just a few hours compared to RTN‚Äôs several hundred hours. The study also showed that larger models seemed to be easier to quantize, which is useful because these are the ones where compression is most needed."
  },
  {
    "objectID": "notes/Large Language Models/incoder.html",
    "href": "notes/Large Language Models/incoder.html",
    "title": "InCoder ‚Äì A Generative Model for Code Infilling and Synthesis",
    "section": "",
    "text": "Tip\n\n\n\nInCoder is a 1.3B parameter LLM that can generate code (left-to-right) as well as editing (via masking and infilling).\nüìù Paper: https://arxiv.org/abs/2204.05999"
  },
  {
    "objectID": "notes/Large Language Models/incoder.html#infilling-and-synthesis-via-causal-masking",
    "href": "notes/Large Language Models/incoder.html#infilling-and-synthesis-via-causal-masking",
    "title": "InCoder ‚Äì A Generative Model for Code Infilling and Synthesis",
    "section": "Infilling and synthesis via causal masking",
    "text": "Infilling and synthesis via causal masking\nTwo main approaches for code LLM:\n\nGPT: left-to-right (causal) autoregressive language modeling objective. The issue is that these models cannot perform infilling since they only consider previous tokens.\nBERT: masked language modeling objective. These models can use left and right contexts and infill a masked region. However, their training objective is limited to generating only ~15% of a document.\n\nThe authors adopt a causal masking objective to be able to infill blocks of code conditioned on arbitrary left and right contexts.\n\nTraining\n\n\nSample a number of spans of contiguous tokens from a Poisson distribution: the idea is that most spans are very small, but it has a long tail up to 256 spans.\nEach span k is replaced with a special mask sentinel token &lt;Mask:k&gt;. This mask is repeated at the end of the document with a &lt;EOM&gt; token at the end so the LLM knows it has to generate the text corresponding to the mask k.\nThe model is trained to maximize the log probability of the masked document.\n\n\n\nInference\n\nDuring inference, the model can be used for left-to-right code generation (autoregressively) or it can insert code using a &lt;Mask:k&gt; token. For example, it means it can be used to replace variable names in a function or generate docstrings."
  },
  {
    "objectID": "notes/Large Language Models/incoder.html#models",
    "href": "notes/Large Language Models/incoder.html#models",
    "title": "InCoder ‚Äì A Generative Model for Code Infilling and Synthesis",
    "section": "Models",
    "text": "Models\nThe main model is InCoder-6.7B, trained on 248 V100 GPUs for 24 days with fully sharding model states (1 epoch). It is based on the Fairseq architecture. Hyperparameters:\n\nGPU batch size = 8\nMax token length = 2048\nMax gradient norm = 1.0\nAdam optimizer \\beta_1 = 0.9 and \\beta_2 = 0.98\nLearning reate scheduler = polynomial decay\nWarmup updates = 1500\n\nModels were trained on public code from GitHub and GitLab and StackOverlow questions, answers, and comments. Python was the primary focus but other languages were also added (28 in total). The pre-training corpus contains a total of 159 GB of code (52 GB of Python, 57 GB from StackOverflow)."
  },
  {
    "objectID": "notes/Large Language Models/incoder.html#infilling-experiments",
    "href": "notes/Large Language Models/incoder.html#infilling-experiments",
    "title": "InCoder ‚Äì A Generative Model for Code Infilling and Synthesis",
    "section": "Infilling Experiments",
    "text": "Infilling Experiments\nThe authors present an interesting technique called ‚Äúleft-to-right reranking‚Äù, where they first generate K possible competions for a given blank region. Then, they select the best candidate by substituting it into the blank and calculting the log probability averaged across the number of tokens in the completed document.\nThey describe two infilling tasks to perform on HumanEval instead of the traditional code generation:\n\nSingle-line infilling: we mask out each non-blank line of code in the canonical functions in turn and try to complete this line.\nMulti-lined infilling: same idea but with N lines that are masked out at each iteration.\n\nThe authors also perform experimentations to generate docstrings based on the CodeXGLUE dataset."
  },
  {
    "objectID": "notes/Large Language Models/inference_optimization.html",
    "href": "notes/Large Language Models/inference_optimization.html",
    "title": "Inference Optimization ‚Äì Lil‚ÄôLog",
    "section": "",
    "text": "üìù Article: https://lilianweng.github.io/posts/2023-01-10-inference-optimization/"
  },
  {
    "objectID": "notes/Large Language Models/inference_optimization.html#introduction",
    "href": "notes/Large Language Models/inference_optimization.html#introduction",
    "title": "Inference Optimization ‚Äì Lil‚ÄôLog",
    "section": "Introduction",
    "text": "Introduction\nTwo main approaches to weight quantization:\n\nPost-Training Quantization (PTQ): na√Øve approach, where a trained model‚Äôs weights are converted to lower precision without retraining.\nQuantization-Aware Training (QAT): the model‚Äôs weights are converted to lower precision during pre-training or further fine-tuning. It brings better performance, but is also more expensive and requires ‚Äúrepresentative training data‚Äù.\n\nThere is an emphasis on ‚Äúrepresentative training data‚Äù because the quantization process optimizes the model on the training data distribution. If this distribution changes, there will be a drop in accuracy.\nThis is not intrinsically different from regular training without quantization, where we also want a training set that is a good representation of the test set. In the case of quantization, these errors will simply have a much bigger impact."
  },
  {
    "objectID": "notes/Large Language Models/inference_optimization.html#challenges",
    "href": "notes/Large Language Models/inference_optimization.html#challenges",
    "title": "Inference Optimization ‚Äì Lil‚ÄôLog",
    "section": "Challenges",
    "text": "Challenges\nA simple 8-bit post-training quantization on both weights and activation leads to significant performance drop.\nIn this context, ‚Äúactivation‚Äù refers to the output values of a layer ‚Äì it corresponds to internal calculations in the transformer. It has been observed that quantizing activation is particularly harmful. A better strategy consists of only quantizing the weights to 8-bit and keep activation at full precision (FP32).\n\nBondarenko et al.¬†(2021) proposed an explanation based on observations performed on a small BERT model. They note that the FNN‚Äôs input and output have very different dynamic ranges due to strong outliers in the output tensor. This is why the per-tensor quantization for the FFN‚Äôs residual sum is likely to cause a notable error.\nThe ‚Äúresidual‚Äù refers to residual connections, which connect the output of one earlier convolutional¬†layer¬†to the input of another future convolutional layer several layers later.\nFor bigger models, we find outliers of high magnitude in all layers (~100x larger than the mean), which is why simple low-but quantization does not work."
  },
  {
    "objectID": "notes/Large Language Models/inference_optimization.html#post-training-quantization-ptq",
    "href": "notes/Large Language Models/inference_optimization.html#post-training-quantization-ptq",
    "title": "Inference Optimization ‚Äì Lil‚ÄôLog",
    "section": "Post-training quantization (PTQ)",
    "text": "Post-training quantization (PTQ)\n\nMixed-precision quantization\nTo address this challenge, we can implement quantization at different precision for weights vs.¬†activation.\nZadeh et al.¬†(2020) proposed GOBO, a small BERT model that applies post-training quantization. It assumes that model weights of each layer follow a Gaussian distribution and therefore detects outliers by tracking mean and standard deviation per layer. It stores these outliers in their original form. For the other values, it splits them into multiple bins and only corresponding bin indices of weights and the centroid values are stored.\nBondarenko et al.¬†(2021) refined this approach by using 16-but quantization on problematic activations (e.g., residual connections after FFN) but 8-bit on others.\nDettmers et al.¬†(2022) proposed LLM.int8(), which implements two mixed-precision decompositions:\n\nVector-wise quantization: during matrix multiplication, each row and column is independently scaled by the absolute maximum values and quantized to INT8.\n16-bit decomposition: outlier activation features remain in FP16 but they represent only a tiny fraction of total weights. They are defined empirically, e.g., values 20x larger than other dimensions."
  },
  {
    "objectID": "notes/Large Language Models/inference_optimization.html#quantization-at-fine-grained-granularity",
    "href": "notes/Large Language Models/inference_optimization.html#quantization-at-fine-grained-granularity",
    "title": "Inference Optimization ‚Äì Lil‚ÄôLog",
    "section": "Quantization at fine-grained granularity",
    "text": "Quantization at fine-grained granularity\n\n\nd¬†is the model size / hidden state dimension and¬†h¬†is the number of heads in one MHSA (multi-head self-attention) component\n\nThere are many levels that can be quantized. Fore example, quantizing the entire weight matrix (‚Äúper-tensor‚Äù/‚Äúper-layer‚Äù quantization) is easy to implement but does not lead to good granularity of quantization.\nShen, Dong & Ye, et al.¬†(2020) proposed Q-BERT, which applies group-wise quantization to a fine-tuned BERT model. It applies a Hessian Aware Quantization (HAWQ) quantization to each individual matrix W with repsect to each head in MHSA.\nHAWQ is a technique to identify outliers based on the idea is that parameters with higher Hessian spectrum (larger top eigenvalues) are more sensitive to quantization and thus require higher precision.\nBondarenko et al.¬†(2021) observed that outlier values only appear in a few values out of d (hidden state / model size) dimensions. This is why they proposed a per-embedding group activation quantization, where activation tensors are splitted into several evenly sized groups along the embedding dimension where elements in the same group share quantization parameters.\nYao et al.¬†(2022) introduced ZeroQuant, which combines group-wise quantization for weights (like Q-BERT) and token-wise quantization for activation. They buil a customized kernel to fuse quantization operation with its previous operator in order to avoid expensive quantization and de-quantization computation.\nFrantar et al.¬†(2022) reformulates the problem of quantization as an optimization problem. Given a weight matrix \\mathbf{W} and an input matrix \\mathbf{X}, we want to find a quantized weight matrix \\mathbf{\\hat{W}} to minimize the MSE: \\mathbf{\\hat{W}*} = \\arg \\min_{\\mathbf{\\hat{W}}} \\parallel\\mathbf{W X} - \\mathbf{\\hat{W} X}\\parallel_2^2 Their model, GPTQ, independently applies quantization to each row vector \\mathbf{w} in the weight matrix \\mathbf{W}. It iteratively quantizes more weights that are selected greedily to minimize the quantization error. The update on selected weights has a closed-form formula, using Hessian matrices. A similar method is proposed by Frantar & Alistarh (2022) with Optimal Brain Quantization (OBQ).\n\nOutlier smoothing\nXiao & Lin (2022) proposed SmoothQuant, a smart solution to smooth outlier features from activations to weights via mathematically equivalent transformation and then enable quantization on both weights and activations (W8A8).\n\nThe smoothing factor can be fused into the layers‚Äôs parameters offline."
  },
  {
    "objectID": "notes/Large Language Models/inference_optimization.html#quantization-aware-training-qat",
    "href": "notes/Large Language Models/inference_optimization.html#quantization-aware-training-qat",
    "title": "Inference Optimization ‚Äì Lil‚ÄôLog",
    "section": "Quantization-aware training (QAT)",
    "text": "Quantization-aware training (QAT)\nIn QAT, the quantization operation happens in the pre-training/fine-tuning process so the weights are directly learned in a low-bit representation. It leads to better performance but is more costly in terms of training time and computation.\nA direct approach consists of fune-tuning the model after quantization on a training set that is the same or representative of the pre-training set. The training objective can e the same as the one for pre-training (e.g., NLL/MLM in general language model training) or specific to a downstream task of interest (e.g., cross-entropy for classification).\nDistillation is another popular approach, with the full-precision model acting as the teacher and the lower-precision model as the student. It doesn‚Äôt need to use the original dataset: the Wikipedia data set or even random tokens can give decent performance gain.\nYao et al.¬†(2022) proposed a layer-by-layer knowledge distillation (LKD) to quantize the model layer by layer and uses its original, unquantized version as the teacher model. Given the same inputs, LKD minimizes the MSE between the multiplication with layer weights and the multiplication of quantized layer weights."
  },
  {
    "objectID": "notes/Large Language Models/lima.html",
    "href": "notes/Large Language Models/lima.html",
    "title": "LIMA ‚Äì Less Is More for Alignment",
    "section": "",
    "text": "Tip\n\n\n\nLIMA is a 65B parameter LLaMA model fine-tuned (supervised learning) using 1,000 samples, which outperforms DaVinci003.\nüìù Paper: https://arxiv.org/abs/2305.11206\nSuperficial Alignement Hypothesis: alignement is a process where the model learns to leverage knowledge and capabilities that were acquired during pre-training.\nThe model is fine-tuned on 1,000 examples that approximate user prompts and high-quality responses (750 from forums + 250 manually written examples), optimized for quality and diversity."
  },
  {
    "objectID": "notes/Large Language Models/lima.html#alignment-data",
    "href": "notes/Large Language Models/lima.html#alignment-data",
    "title": "LIMA ‚Äì Less Is More for Alignment",
    "section": "Alignment Data",
    "text": "Alignment Data\n\n\n\n\n\n\nTip\n\n\n\nTraining models is easy but building high-quality datasets is hard. This is the most interesting part of the paper to me.\n\n\nThe authors collect data from three popular QA websites:\n\nStack Exchange:\n\nDivide the exchanges into 75 STEM exchanges + 99 other (English, cooking, travel, etc.) + discard 5 nice exchanges.\nSample 200 QAs from each set (= exchange or STEM/other?).\nWthin each exchange, take the questions with the highest score (needs to self-contained in the title = no body).\nSelect the top answer for each question with score &gt;= 10, 1200 characters &lt; length &lt; 4096 characters, not written in the first person, and without reference to other answers.\nLinks, images, and other HTML tags (except code blocks and lists) are also removed rom the answers.\nThey randomly select the title or the description as questions since Stack Exchange has both.\n\nwikiHow:\n\nSample 200 articles while ensuring a diversity of categories.\nPrompt = title (‚ÄúHow to cook an omelette?‚Äù) and response = article‚Äôs body.\nReplace ‚ÄúThis article‚Ä¶‚Äù with ‚ÄúThe following answer‚Ä¶‚Äù\nRemove links, images, and certain sections of the text.\n\nPushshift Reddit Dataset:\n\nRestricted to two subreddits: r/AskReddit and r/WritingPrompts.\nManually select examples fro within the most upvoted posts.\nQAs from r/AskReddit are deemed not necessarily reliable, which is why they are used as a test set.\n\n\nThe authors also manually authored examples. They created two groups (A and B) to create 250 prompts each, based on personal interests. They selected 200 prompts from Group A and (50 prompts as a held-out development set) + 230 prompts from Group B (for test only).\nThey also manually wrote high-quality answers, with a uniform tone and some acknowledgement of the question, followed by the answer. In this data, they include 13 adversarial training prompts (toxic, malevolent) + a rejection in the corresponding answers.\nThey also add 50 samples from Super-Natural Instructions, which are modified to correspond to the style of the 200 manual examples."
  },
  {
    "objectID": "notes/Large Language Models/lima.html#training-lima",
    "href": "notes/Large Language Models/lima.html#training-lima",
    "title": "LIMA ‚Äì Less Is More for Alignment",
    "section": "Training LIMA",
    "text": "Training LIMA\nThey trained a 65B parameter LLaMA model on 1,000 samples. To distinguish user and assistant, they introduce an end-of-turn token (EOT) at the end of each utterance.\nHyperparameters:\n\n15 epochs using AdamW (\\beta_1 = 0.9, \\beta_2 = 0.95, and weght decay of 0.1).\nNo warmup steps, initial learning rate = 1e-5, linearly decaying to 1e-6 by the end of training.\nBatch size of 32 examples (64 for smaller models).\nTexts longer than 2048 tokens (LLaMA‚Äôs context window) are trimmed.\n\nThey also apply dropout over residual connections, starting at p_d = 0.0 at the bottom layer and linearly raising the rate to p_d = 0.3 at the last layer (p_d = 0.2 for smaller models).\nThey manually selected checkpoints between the 5th and the 10th epochs using the held-out 50 samples (not based on perplexity).\n\n\n\n\n\n\nTip\n\n\n\nThere are two interesting improvements/modifications over the original model: EOT and residual dropout."
  },
  {
    "objectID": "notes/Large Language Models/lima.html#human-evaluation",
    "href": "notes/Large Language Models/lima.html#human-evaluation",
    "title": "LIMA ‚Äì Less Is More for Alignment",
    "section": "Human Evaluation",
    "text": "Human Evaluation\nBaselines: Alpaca 65B, DaVinci003, Bard, Claude, GPT-4.\n\nGeneration‚Äôs parameters: nucleus sampling (p=0.9), temperature of 0.7, repetition penalty of previous tokens of 1.2, max token length = 2048."
  },
  {
    "objectID": "notes/Large Language Models/lima.html#multi-turn-dialogue",
    "href": "notes/Large Language Models/lima.html#multi-turn-dialogue",
    "title": "LIMA ‚Äì Less Is More for Alignment",
    "section": "Multi-Turn Dialogue",
    "text": "Multi-Turn Dialogue\nThe authors created a smal multi-turn dialogue dataset with 30 samples (10 manual, 20 based on modified comments from Stack Exchange).\nThey train a LIMA model on the 1,000 original samples + 30 multi-turn dialogue examples and show the performance of the model greatly improves (from 45.2% to 76.1% excellent responses)."
  },
  {
    "objectID": "notes/Large Language Models/local_llm.html",
    "href": "notes/Large Language Models/local_llm.html",
    "title": "Local Large Language Models ‚Äì Int8",
    "section": "",
    "text": "üìù Article: https://int8.io/local-large-language-models-beginners-guide/"
  },
  {
    "objectID": "notes/Large Language Models/local_llm.html#lora-in-peft",
    "href": "notes/Large Language Models/local_llm.html#lora-in-peft",
    "title": "Local Large Language Models ‚Äì Int8",
    "section": "LoRA in PEFT",
    "text": "LoRA in PEFT\nSummarizes the LoRA ‚Äì Low-Rank Adaptation of Large Language Models paper to reduce the number of trainable parameters during fine-tuning.\nIt does it by freezing the original weights and injecting low-rank trainable matrix deltas to the original weights. These low-rank matrices are the product of two smaller matrices (factorization), which reduces the number of trained parameters.\n\nThis is done as follows: W_0 = W_0 + \\Delta W = W_0 + AB It is much more efficient to train these smaller matrices A and B, while the original W_0 remains unchanged.\nIt is implemented in the PEFT library, where you can specify the rank of these matrices (height of A, width of B).\nroberta_for_sc = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\")\n\nconfig = LoraConfig(\n    r=8,\n    lora_dropout=0.1,\n    lora_alpha=32,\n    target_modules=['query', 'key', 'value']\n    )\n    \npeft_model = get_peft_model(\n    roberta_for_sc,\n    peft_config=config\n    )\nWe can print information about the parameters using peft_model.print_trainable_parameters(): it shows that the number of trainable parameters corresponds to 1% of the original size.\nHowever, this doesn‚Äôt translate into a training that would be 100x faster. We only save resources during the backward pass (backpropagation) as gradients will only need to update the new LoRA layers. This doesn‚Äôt change the forward pass, where we still need to use the original weights.\nLoRA‚Äôs authors report¬†25% speedup during training¬†on GPT-3 175B compared to full fine-tuning and¬†VRAM usage drop by up to 2/3. It doesn‚Äôt bring a big performance boost to single forward pass/inference."
  },
  {
    "objectID": "notes/Large Language Models/local_llm.html#llm.int8-8-bit-matrix-multiplication-for-transformers-at-scale",
    "href": "notes/Large Language Models/local_llm.html#llm.int8-8-bit-matrix-multiplication-for-transformers-at-scale",
    "title": "Local Large Language Models ‚Äì Int8",
    "section": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
    "text": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale\nDuring training, most of the computation is dedicated to matrix multiplication. The storage requirement for each matrix is determined by its size and the precision of its values. LoRA reduces the size of these matrices, and LLM.int8(), introduced by Dettmers et al.¬†(2022), reduces the precision.\n\nAbsolute maximum 8-bit quantization\nStep-by-step process:\n\nIdentify the absolute maximum value in the vector v\nCalculate the scaling factor \\frac{127}{\\text{absmax}(v)}\nMultiply each value in v by this factor\nDequantization uses divides each value by this factor instead\n\n\n\n\nInt-8 matrix multiplication\nThe simplest approach to quantize matrices would consist of using a global scaling factor.\nUnfortunately, absolute maximum 8-bit quantization is sensitive to outliers. Imagine we have a vector v = [0.3, 0.4, 0.5, 0.6, 512.0]. Because of the outlier 512.0, we would get a scaling factor of 0.24, which would give the following quantized vector q_v = [0,0,0,0,1].\nInstead, the authors propose treating each row of an input matrix (X) and each column of a weight matrix (W) as separate blocks and quantize them independently.\n\nEven with this scheme, we still find outliers in these vectors. The authors treat any value with an absmax value greater than 6 as an outlier. Instead of INT8, they use a FP16 precision to handle it (mixed-precision decomposition).\nOutlier values are kept with high precision, so they don‚Äôt interfere with the rest of the weights. The authors show that the blocking strategy + outlier handling yields close-to-zero degradation of the performance. More than 99.9% of values are still multiplied in efficient 8-bit.\nThe main advantage of using LLM.int8() is around¬†50% memory reduction¬†compared to 16-bit."
  },
  {
    "objectID": "notes/Large Language Models/local_llm.html#bit-optimizers",
    "href": "notes/Large Language Models/local_llm.html#bit-optimizers",
    "title": "Local Large Language Models ‚Äì Int8",
    "section": "8-bit Optimizers",
    "text": "8-bit Optimizers\nDettmers et al.¬†(2021) introduces the following problem: optimizer states also consume a lot of GPU memory.\nWeights + batch of input data + loss function \\rightarrow Optimizer \\rightarrow updated weights\nFor example, the momentum use in modern optimizers like Adam calculates weight updates as a linear combination of all historical updates, forgetting historical value exponentially. This is why they need to store these historical values.\n\nWhenever you fine-tune a model using a modern optimizer you will need extra memory to store some historical weights updates, too.¬†Modern optimizers use up to 75% of total memory used during training.\n\nThe authors introduce 8-bit optimizers that use less memory to store their inner states.\n\n8-bit Optimizers: Dynamic Tree Quantization\nThe core building block of 8-bit optimizers is dynamic tree quantization. It‚Äôs another way of representing a vector/matrix of float numbers with 8 bits, like absmax quantization.\nLet‚Äôs take the bfloat16 as an example first:\n\nIt uses 16 bits, where the first bit corresponds to the sign, the following 8 bits are the exponent bits (magnitude), and the last 7 bits are the fraction bits (precision). The value in the previous example is: \\text{value} = 1 \\cdot 2 \\cdot ( 1 + \\frac{1}{2} + \\frac{1}{16} + \\frac{1}{128}) = 3.140625 Dynamic Tree Quantization uses an 8-bit representation that is designed to represent numbers between -1 and 1. It has an indicator bit to dynamically determine exponent and linear quantization sizes.\n\nThe first bit indicates the sign, the consecutive zeros give the exponent, and the first 1 after that is the indicator bit.\nThis indicator bit makes the representation dynamic: exponent size and bits for linear quantization are not fixed, like in bfloat16. Using the previous example, we have: \\text{value} = 1 \\cdot e^{-2} \\cdot \\frac{9}{15} = 0.08120 The authors also introduce a variant of dynamic tree quantization, called dynamic quantization, specifically design for the Adam optimizer, where one of the internal states stored by Adam is positive and doesn‚Äôt require a sign bit (this extra bit is added to the linear quantization part instead).\n\n\n8-bit Optimizers: Block-wise quantization\nSame idea than in LLM.int8(). Block-wise quantization partition tensors into blocks to reduce the effects of outliers to a single block instead of the entire tensor.\n\n\n\n8-bit Optimizers: Stable Embedding Layer\nThe authors noticed that using classical embedding layer leads to instability problems with 8-bit optimizers. They proposed a stable embedding layer, which is a new composition of previously known ideas.\nLLM.int8() and 8-bit optimizers are implemented in bitsandbytes and can be used in HuggingFace libraries."
  },
  {
    "objectID": "notes/Large Language Models/local_llm.html#bit-quantization-gptq-and-ggml",
    "href": "notes/Large Language Models/local_llm.html#bit-quantization-gptq-and-ggml",
    "title": "Local Large Language Models ‚Äì Int8",
    "section": "4-bit Quantization: GPTQ and GGML",
    "text": "4-bit Quantization: GPTQ and GGML\nFrantar et al.¬†(2023) introduced GPTQ, which uses 4 bits (16 distinct values) to represent a floating point. This technique is specifically designed for GPT models.\nIt is formalized as an independent optimization problem for each layer. We have a single linear layer to quantize and its corresponding weight matrix W. We also have a small amount of m example inputs organized in a matrix X. We want to find the 4-bit matrix \\hat{W} such as: \\arg \\min_{\\hat{W}} \\parallel WX - \\hat{W}X \\parallel This is solved using the Optimal Brain Compression technique, by Frantar et al.¬†(2023).\nA popular implementation of this framework for CPUs can be found in ggml, which is described as a hacky version of 4-bit quantization."
  },
  {
    "objectID": "notes/Large Language Models/local_llm.html#qlora-quantization-lora",
    "href": "notes/Large Language Models/local_llm.html#qlora-quantization-lora",
    "title": "Local Large Language Models ‚Äì Int8",
    "section": "QLoRA = Quantization + LoRA",
    "text": "QLoRA = Quantization + LoRA\nDettmers et al.¬†(2023) introduced a combination between LoRA and 4-bit quantization for efficient fine-tuning.\nOne of the core components is the 4-bit NormalFloat Quantization to compress matrix weights into 4-bit precision. It is designed to yield uniform distribution over bin counts that each 4-bit vector represents, making NF4 an information-theoretically optimal data type.\nA second contribution is the double quantization, which quantizes quantization constants themselves. This makes sense because thanks to the limited size of NF4.\nIn QLoRA, only the frozen weights of the base LoRA model are 4-bit quantized, while the weights of the LoRA matrices (deltas) are kept in BF16. During both the forward and backward passes, the 4-bit weights are dequantized to BF16 for computations.\nQLoRA significantly reduces the memory requirement for fine-tuning LLMs, lowering the bar by an additional 50% and allowing even larger models to be trained locally. This makes 33B parameter models trainable on GPUs with 24GB of VRAM."
  },
  {
    "objectID": "notes/Large Language Models/longnet.html",
    "href": "notes/Large Language Models/longnet.html",
    "title": "LongNet ‚Äì Scaling Transformers to 1,000,000,000 Tokens",
    "section": "",
    "text": "Tip\n\n\n\nThis paper introduces the dilated attention mechanism, another sparse attention scheme which approximates sparse attention.\nüìù Paper: https://arxiv.org/pdf/2307.02486.pdf\nThe authors claim their technique can scale up to 1 billion tokens. However, they only show results up to 32K tokens and assume you can use 10,000 GPUs."
  },
  {
    "objectID": "notes/Large Language Models/longnet.html#dilated-attention",
    "href": "notes/Large Language Models/longnet.html#dilated-attention",
    "title": "LongNet ‚Äì Scaling Transformers to 1,000,000,000 Tokens",
    "section": "Dilated Attention",
    "text": "Dilated Attention\n\nN is the sequence length and d is the hidden dimension.\nVanilla attention maps a query and a set of keys and values to output: O = \\text{softmax}(QK^\\top)V Sparse attention restricts the query‚Äôs access to a subset of keys and values to reduce the complexity: O = \\text{softmax}(QK^\\top \\odot \\mathbb{1}_S)V Dilated attention splits the input (Q, K, V) into segments \\{(\\widetilde{Q}_i, \\widetilde{K}_i, \\widetilde{V}_i)\\}^{\\frac{N}{w}} equally with a segment length w. Each segment is then sparsified along the sequence dimension by selecting the rows with an interval r. The computation can be written as:\n\\begin{align*}\n\\widetilde{Q}_i &= [Q_{iw}, Q_{iw+r}, Q_{iw+2r}, \\ldots, Q_{(i+1)w-1}] \\\\\n\\widetilde{K}_i &= [K_{iw}, K_{iw+r}, K_{iw+2r}, \\ldots, K_{(i+1)w-1}] \\\\\n\\widetilde{V}_i &= [V_{iw}, V_{iw+r}, V_{iw+2r}, \\ldots, V_{(i+1)w-1}]\n\\end{align*} The sparsified segments \\{(\\widetilde{Q}_i, \\widetilde{K}_i, \\widetilde{V}_i)\\}^{\\frac{N}{w}} are fed into the attention in parallel, after which they are scattered and concatenated as the output O:\n\\begin{align*}\n\\widetilde{O}_i &= \\text{softmax}(\\widetilde{Q}_i \\widetilde{K}^\\top_{i}) \\widetilde{V}_i \\\\\n\\hat{O}_i &= \\{\\widetilde{O}_{i,j} | j \\mod r = 0; 0 | j \\mod r \\neq 0\\} \\\\\nO &= [\\hat{O}_0, \\hat{O}_1, \\dots, \\hat{O}_{\\frac{N}{w} - 1}]\n\\end{align*} The idea is quite visual: you have vanilla attention between tokens from the same segment, and then a sparse mechanism between tokens from different segments as follows:\n\n\n[!note] Why this particular scheme and not a more hierarchical segmentation? There is no real explanation behind this solution.\n\nInterestingly, they try to fix the errors induced by the sparse representation with the multi-head attention as follows:"
  },
  {
    "objectID": "notes/Large Language Models/longnet.html#distributed-training",
    "href": "notes/Large Language Models/longnet.html#distributed-training",
    "title": "LongNet ‚Äì Scaling Transformers to 1,000,000,000 Tokens",
    "section": "Distributed Training",
    "text": "Distributed Training\nThe authors implement a distributed training algorithm that motivates the claim about a sequence length of 1 billion tokens. The input is split into segments across multiple GPUs, then projected into queries, keys, and values on each device.\n\nThe algorithm performs the attention computation locally for segment lengths smaller or equal to the sequence length on a given device. For larger segment lengths, keys and values are distributed across devices and collected prior to attention computation.\nThis method includes an all-gather operation for key-value pairs collection, resulting in constant communication costs, independent of sequence length. The outputs from different devices are concatenated to form the final attention output.\nThe performance of the LongNet model, compared to vanilla attention, demonstrates its efficient scaling due to the linear complexity of dilated attention and its distributed algorithm, even when sequence lengths increase dramatically."
  },
  {
    "objectID": "notes/Large Language Models/longnet.html#experiments",
    "href": "notes/Large Language Models/longnet.html#experiments",
    "title": "LongNet ‚Äì Scaling Transformers to 1,000,000,000 Tokens",
    "section": "Experiments",
    "text": "Experiments\n\nLongNet was implemented for language modeling with a backbone architecture named Magneto and trained with The Stack dataset. The results were compared with the vanilla and sparse Transformers for sequence lengths from 2K to 32K. The models were tested on different sequence lengths, and LongNet consistently performed better than the other models, especially when the sequence length during training was increased.\nLongNet also outperformed when the context length was scaled up during training. While both LongNet and vanilla Transformers benefited from larger context lengths, LongNet achieved lower test loss with less computation, demonstrating its efficiency in learning long-range dependencies.\nScaling up the model size, LongNet followed the power law, showing that the dense Transformer is not a prerequisite for scaling language models. Additionally, LongNet was found to be more scalable and efficient.\nThe model was also tested for longer context prompting, with the length of the prompt scaled from 2K to 32K. Results showed that as the context window grew, the test loss of LongNet gradually decreased, indicating its superior ability to leverage longer contexts to improve language modeling."
  },
  {
    "objectID": "notes/Large Language Models/lora.html",
    "href": "notes/Large Language Models/lora.html",
    "title": "LoRA ‚Äì Low-Rank Adaptation of Large Language Models",
    "section": "",
    "text": "Tip\n\n\n\nPEFT approach based on matrix factorization.\nüìù Paper: https://arxiv.org/pdf/2106.09685.pdf\nüíª GItHub: https://github.com/microsoft/LoRA\nIdea: freeze the pretrained model weights and inject trainable rank decomposition matrices into each layer of the Transformer architecture."
  },
  {
    "objectID": "notes/Large Language Models/lora.html#i.-introduction",
    "href": "notes/Large Language Models/lora.html#i.-introduction",
    "title": "LoRA ‚Äì Low-Rank Adaptation of Large Language Models",
    "section": "I. Introduction",
    "text": "I. Introduction\nThe problem with traditional fine-tuning is that the new model contains as many parameters as in the original model. This is challenging with LLMs like GPT-3 (175B trainable parameters).\nPrevious methods: adapting only some parameters or learning external modules for new tasks. This allows us to store and load a small number of parameters for specific tasks. However, it also introduces inference latency and/or reduces the model‚Äôs usable sequence length.\nPrevious work from Li et al. (2018) and Aghajanyan et al. (2020) show that the learned over-parametrized models reside in a low intrinsic dimensions. The authors assume that the finetuned weights also have a low ‚Äúintrinsic rank‚Äù.\nAdvantages:\n\nThe same pretrained model can be used to build several LoRA modules\nImproves training efficiency\nTrainable matrices can be merged with the frozen weights when deployed (no inference latency)\nCan be combined with prior methods"
  },
  {
    "objectID": "notes/Large Language Models/lora.html#ii.-problem-statement",
    "href": "notes/Large Language Models/lora.html#ii.-problem-statement",
    "title": "LoRA ‚Äì Low-Rank Adaptation of Large Language Models",
    "section": "II. Problem Statement",
    "text": "II. Problem Statement\nWhen finetuning a model like an autoregressive language model P_{\\Phi}(y|x) parametrized by \\Phi for a specific task, we update the pretrained weights \\Phi_0 to \\Phi_0 + \\Delta \\Phi.\nThe issue is that the dimension of \\Delta\\Phi is equal to the dimension of \\Phi_0, so storing and deploying many independent isntances of fine-tuned models can be challenging.\nIn this paper, the authors propose a more parameter-efficient approach where \\Delta \\Phi is further encoded by a much smaller-sized set of parameters \\Theta with |\\Theta| \\ll |\\Phi_0| (can be as small as 0.01%)."
  },
  {
    "objectID": "notes/Large Language Models/lora.html#iii.-arent-existing-solutions-good-enough",
    "href": "notes/Large Language Models/lora.html#iii.-arent-existing-solutions-good-enough",
    "title": "LoRA ‚Äì Low-Rank Adaptation of Large Language Models",
    "section": "III. Aren‚Äôt existing solutions good enough?",
    "text": "III. Aren‚Äôt existing solutions good enough?\n\nAdapter layers introduce inference latency: there are many variants of adapters. The original design has two adapter layers per Transformer block, a more recent one has only one block + LayerNorm per block. This layers have few parameters but they have to be processed sequentially, unlike the rest of the architecture which is built for parallelism. This is especially problematic at inference time with a batch size of 1.\nDirectly optimizing the prompts is hard: prefix tunign is difficult to optimize and its performance changes non-monotonically in trainable parameters. Reserving a part of the sequence length for adaptation also reduces the sequence length."
  },
  {
    "objectID": "notes/Large Language Models/lora.html#iv.-our-method",
    "href": "notes/Large Language Models/lora.html#iv.-our-method",
    "title": "LoRA ‚Äì Low-Rank Adaptation of Large Language Models",
    "section": "IV. Our method",
    "text": "IV. Our method\nAghajanyan et al. (2020) showed that pre-trained language models have a low ‚Äúintrinsic dimensions‚Äù and can still learn efficiently despite a random projection to a smaller subspace.\nFor a pre-trained matrix W_0, they constrain the update by representing it with a low-rank decomposition h = W_0x + \\Delta Wx = W_0x + BAx During training, W_0 is frozen and does not receive gradient updates. Note that both W_0 and \\Delta W are multiplied by the input during the forward pass.\n\n\nUse a random Gaussian initialization for A and zero for B so \\Delta W = BA is zero at the beginning of training.\nScale \\Delta W x by \\frac{\\alpha}{r} during training\n\nA generalization of full fine-tuning: as we increase the number of trainable parameters, LoRA converges to training the original, unlike adapter-based methods that converge to an MLP and prefix-based methods to a model that cannot take long input sequence.\nNo additional inference latency: when deployed in production, we can explicitly compute and store W = W_0 + BA and perform inference as usual.\nPractical benefits and limitations: reduction in memory (~2/3) and storage usage, speeds up training (25%). However, it is difficult to use different tasks in the same batch of inputs with LoRA."
  },
  {
    "objectID": "notes/Large Language Models/lorahub.html",
    "href": "notes/Large Language Models/lorahub.html",
    "title": "LoraHub ‚Äì Efficient Cross-Task Generalization via Dynamic LoRA Composition",
    "section": "",
    "text": "Tip\n\n\n\nThis paper describes a framework to combine LoRA modules to achieve adaptable performance on unseen tasks. See this notebook from Crumb, which predates it.\nüìù Paper: https://arxiv.org/abs/2307.13269 üíª GitHub: https://github.com/sail-sg/lorahub"
  },
  {
    "objectID": "notes/Large Language Models/lorahub.html#problem-statement",
    "href": "notes/Large Language Models/lorahub.html#problem-statement",
    "title": "LoraHub ‚Äì Efficient Cross-Task Generalization via Dynamic LoRA Composition",
    "section": "Problem statement",
    "text": "Problem statement\nThis paper is focused on pre-trained encoder-decoder and decoder-only transformer models.\nThe goal is to improve cross-task generalization. This term regroups zero-shot learning and few-shot learning. The idea is that you could [[LoRA ‚Äì Low-Rank Adaptation of Large Language Models|LoRA]] tune a model using the examples from few-shot learning, but it is ‚Äúinefficient, time-consuming, and unstable‚Äù when the number of examples is small."
  },
  {
    "objectID": "notes/Large Language Models/lorahub.html#methodology",
    "href": "notes/Large Language Models/lorahub.html#methodology",
    "title": "LoraHub ‚Äì Efficient Cross-Task Generalization via Dynamic LoRA Composition",
    "section": "Methodology",
    "text": "Methodology\n\nIt requires several LoRA modules (r=16), which are first trained on a variety of upstream tasks (e.g., boolean expressions).\nLoraHub has two phases:\n\nCompose, where all available LoRA modules (they can be filtered) are merged into one module using weights (positive or negative). \\hat{m} = (w_1 A_1 + w_2 A_2 + \\dots + w_N A_N) (w_1 B_1 + w_2 B_2 + \\dots + w_N B_N).\nAdapt: the assembled LoRA module is used in combination with the base LLM. Its performance is assessed on few-shot examples to select the best weights w_i. The objective is to minimize the cross-entropy loss with L1 regularization. This is done with an algorithm from Shiwa, the Covariance Matrix Adaptive Evolution Strategies (CMA-ES). It is implemented with the Nevergrad optimization library.\n\n\n\n\n\n\n\nTip\n\n\n\nI‚Äôm skeptical about this optimization technique. I would like to know how it compares to a centroid-based approach for instance."
  },
  {
    "objectID": "notes/Large Language Models/lorahub.html#evaluation",
    "href": "notes/Large Language Models/lorahub.html#evaluation",
    "title": "LoraHub ‚Äì Efficient Cross-Task Generalization via Dynamic LoRA Composition",
    "section": "Evaluation",
    "text": "Evaluation\nThey use Flan-T5 large (783M parameters) as a base LLM. They trained a LoRA module for each of the 200 distinct tasks from FLAN_2022 and released them on Hugging Face. Due to this high number, they pre-filter them and randomly select 20 modules for each ‚Äúexperimental sequence.‚Äù\n\n\n\n\n\n\nTip\n\n\n\nThis pre-filtering could be easily improved.\n\n\nFinally, LoraHub is evaluated on the BBH benchmark (Exact Match as evaluation metric).\n\nIt is not as good as ICL, but consumes 5 times less tokens."
  },
  {
    "objectID": "notes/Large Language Models/multipack_sampler.html",
    "href": "notes/Large Language Models/multipack_sampler.html",
    "title": "Multipack Sampler",
    "section": "",
    "text": "üíª GitHub: https://github.com/imoneoi/multipack_sampler"
  },
  {
    "objectID": "notes/Large Language Models/multipack_sampler.html#what-does-this-code-do",
    "href": "notes/Large Language Models/multipack_sampler.html#what-does-this-code-do",
    "title": "Multipack Sampler",
    "section": "What does this code do?",
    "text": "What does this code do?\nFirst-Fit-Decreasing Bin Packing (ffd_check and ffd_with_result)\nThis algorithm tries to fit a series of items into bins such that the number of bins is minimized. The items are sorted in decreasing order and then placed in the first bin that has enough space. The ffd_check function checks if the given items can fit into a fixed number of bins with a certain capacity, while the ffd_with_result function returns the actual bins (or groups) formed.\nDynamic Batch Allocation (allocate)\nIt‚Äôs an algorithm that uses the bin packing method mentioned above. It dynamically allocates batches by trying to fit as many samples as possible into a set number of bins, taking advantage of the first-fit-decreasing strategy. The result is a series of batches that are very efficiently packed, minimizing the space wasted.\nMultipackDistributedBatchSampler\nThis is a sampler used in PyTorch‚Äôs data loading pipeline. Its main job is to generate batches of data indices for distributed training based on the lengths of data samples and the specified batch size. The batches are constructed using the dynamic batch allocation method mentioned above.\nThe key idea here is to intelligently group samples such that the total ‚Äúlength‚Äù of the samples in each batch is as close as possible to the maximum allowed batch length, reducing the amount of padding required."
  },
  {
    "objectID": "notes/Large Language Models/multipack_sampler.html#does-it-use-padding",
    "href": "notes/Large Language Models/multipack_sampler.html#does-it-use-padding",
    "title": "Multipack Sampler",
    "section": "Does it use padding?",
    "text": "Does it use padding?\nWhile the code is designed to create batches that maximize the usage of the available space, there‚Äôs still a chance for some padding to be needed. This is due to the fact that even after optimal bin packing, the samples within a bin might not perfectly sum up to the bin‚Äôs capacity. However, the padding is minimized through this strategy.\nIn simpler terms, imagine you have storage boxes of a fixed size and various items of different sizes. This algorithm is like trying to pack these items into the fewest number of boxes possible by placing the largest items first. The result might still have small empty spaces in the boxes (which is analogous to padding), but the algorithm aims to make this wasted space as small as possible."
  },
  {
    "objectID": "notes/Large Language Models/orca.html",
    "href": "notes/Large Language Models/orca.html",
    "title": "Orca ‚Äì Progressive Learning from Complex Explanation Traces of GPT-4",
    "section": "",
    "text": "Tip\n\n\n\nOrca is a 13B parameter LLM with ChatGPT level of performance thanks to a huge dataset of 5M samples with step-by-step explanations.\nüìù Paper: https://arxiv.org/abs/2306.02707\nThe model will probably never be released by Microsoft, but open-source projects try to replicate it (OpenOrca, Dolphin).\nThe authors note that while Vicuna-13B display excellent performance when evaluated with GPT-4, it performs quite poorly on reasoning benchmarks like SAT, LSAT, GRE, GMAT.\nSelf-Instruct involves using an initial set of prompts to ask an LLM to create new instructions. Low-quality or overly similar responses are removed, and the remaining instructions are recycled back into the task pool for further iterations. However, the queries generated via Self-Instruct can lack diversity and complexity.\nProblem with natural conversations: LLMs like Vicuna capture the style but not the reasoning process. This motivates the creation of a dataset with step-by-step explanations.\nUsing GPT-4 for auto-evaluation has several drawbacks, such as limited test set sizes (for example, 80 instructions in Vicuna and 218 in WizardLM) and the inherent biases of GPT-4. It tends to favor models that are instruction-tuned with its own responses, resulting in a preference for longer texts over shorter ones. It also exhibits a bias in the order of candidate responses and overestimates the abilities of smaller models.\nContributions:\nSystem instructions are sampled from a diverse instruction set including chain-of-thought reasoning steps, explain like I‚Äôm five, being helpful and informative, etc."
  },
  {
    "objectID": "notes/Large Language Models/orca.html#explanation-tuning",
    "href": "notes/Large Language Models/orca.html#explanation-tuning",
    "title": "Orca ‚Äì Progressive Learning from Complex Explanation Traces of GPT-4",
    "section": "Explanation Tuning",
    "text": "Explanation Tuning\n\nDataset Construction\nEach training sample is a triplet with system message, user message, and response.\nThe authors use the FLAN-v2 dataset as raw data. The FLAN-v2 Collection consists of five sub-collections: CoT, NiV2, T0 (training only), Flan 2021, Dialogue:\n\nThe CoT is probably the most interesting one and the authors used all of the 150K samples.\nNatural Instructions V2 (NIV2), FLAN2021, T0 were randomly sampled (~10% of the data was selected for each).\nDialog was completely skipped because it lacks context.\n\nThe resulting 5M samples are then used as inputs to generate high-quality responses with ChatGPT (5M) and GPT-4 (1M). These models are prompted with the inputs + 16 handcrafted system messages to ensure different kinds of responses:\n\n&lt;empty&gt;\nYou are an AI assistant. Provide a detailed answer so user don‚Äôt need to search outside to understand the answer.\nYou are an AI assistant. You will be given a task. You must generate a detailed and long answer.\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.\nYou are an AI assistant that follows instruction extremely well. Help as much as you can.\nYou are an AI assistant that helps people find information. Provide a detailed answer so user don‚Äôt need to search outside to understand the answer.\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\nYou should describe the task and explain your answer. While answering a multiple choice question, first output the correct answer(s). Then explain why other answers are wrong. Think like you are answering to a five year old.\nExplain how you used the definition to come up with the answer.\nYou are an AI assistant. You should describe the task and explain your answer. While answering a multiple choice question, first output the correct answer(s). Then explain why other answers are wrong. You might need to use additional knowledge to answer the question.\nYou are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-by-step and justify your answer.\nUser will you give you a task with some instruction. Your job is follow the instructions as faithfully as you can. While answering think step-by-step and justify your answer.\nYou are a teacher. Given a task, you explain in simple steps what the task is asking, any guidelines it provides and how to use those guidelines to find the answer.\nYou are an AI assistant, who knows every language and how to translate one language to another. Given a task, you explain in simple steps what the task is asking, any guidelines that it provides. You solve the task and show how you used the guidelines to solve the task.\nGiven a definition of a task and a sample input, break the definition into small parts. Each of those parts will have some instruction. Explain their meaning by showing an example that meets the criteria in the instruction. Use the following format: Part #: a key part of the definition. Usage: Sample response that meets the criteria from the key part. Explain why you think it meets the criteria.\nYou are an AI assistant that helps people find information.\n\n\nThis is motivated by curriculum learning (learning with a smaller model first, then with a big model) and technical reasons (cost, time).\n\n\nTraining\nThey use the LLaMA BPE tokenizer with padding (vocabulary size = 32,001). Multiple input examples are packed into a single sequence to maximize the used context length (2,048 tokens). They use padding tokens to get a uniform size.\nIt was trained for 160h on 20xA100 GPUs (4 epochs) on the 5M ChatGPT-generated samples + 40h on the 1M GPT-4-generated samples."
  },
  {
    "objectID": "notes/Large Language Models/orca.html#experiments",
    "href": "notes/Large Language Models/orca.html#experiments",
    "title": "Orca ‚Äì Progressive Learning from Complex Explanation Traces of GPT-4",
    "section": "Experiments",
    "text": "Experiments\nOpen-ended generation:\n\nOrca is significantly better than Vicuna.\nAGIEval:\n\nOrca doesn‚Äôt perform as well as ChatGPT.\nBigBench-Hard:\n\nOrca is on par with ChatGPT."
  },
  {
    "objectID": "notes/Large Language Models/phi1.html",
    "href": "notes/Large Language Models/phi1.html",
    "title": "phi-1 ‚Äì Textbooks Are All You Need",
    "section": "",
    "text": "Tip\n\n\n\nIt introduces phi-1, a model with 1.3B parameters that obtains a pass@1 rate of 50.6% on HumanEval thanks to a novel training process. Unfortunately, the weights are not available.\nüìù Paper: https://arxiv.org/pdf/2306.11644.pdf\nThe authors argue that high quality data can change the shape of the scaling laws, allowing small models to match the performance of bigger ones."
  },
  {
    "objectID": "notes/Large Language Models/phi1.html#the-importance-of-high-quality-data",
    "href": "notes/Large Language Models/phi1.html#the-importance-of-high-quality-data",
    "title": "phi-1 ‚Äì Textbooks Are All You Need",
    "section": "The importance of high-quality data",
    "text": "The importance of high-quality data\n\nMotivation: The authors observe that standard code datasets like The Stack, StackOverflow and CodeContests suffer from several drawbacks: samples are not self-contained but referenced, a lot of them are trivial while the most complex ones are poorly documented, and the overall distribution is skewed towards certain topics and use cases.\nThey train their solution (phi-1) on a new dataset (&lt;7B tokens) as follows:\n\nPretraining on CodeTextbook, comprised of a filtered version of The Stack and StackOverflow(~6B tokens) + a synthetic samples generated by GPT-3.5 (&lt;1B tokens)\nFine-tuning on CodeExercises, a small synthetic dataset of Python exercises and solutions also generated by GPT-3.5 (~180M tokens)\n\n\nFiltering code\nThe authors use the Python subset of the deduplicated version of The Stack and StackOverflow (35B tokens). They used GPT-4 to annotate the quality of a subset (100k samples), using the following prompt: ‚Äúdetermine its educational value for a student whose goal is to learn basic coding concepts.‚Äù\n\n\n\n\n\n\nTip\n\n\n\nThis prompt could probably be improved by asking GPT-4 to break down its reasoning into steps before outputting the final value.\n\n\nThis creates a dataset of code snippets and corresponding values. The authors produce embeddings of each code snippet using a pretrained CodeGen model, and train a random forest classifier to predict the quality of each sample.\n\n\nGenerating synthetic data\nThe authors argue the synthetic samples should be diverse (concepts, skills, scenarios, difficulty, complexity, style) and non-repetitive to reduce the risk of overfitting/memorizing and be more robust. Inspired by TinyStories, they use randomized seeds √† la Alpaca to generate samples with GPT-3.5:\n\nSynthetic training data (&lt;1B tokens): code and text with examples and constraints.\nCodeExercises (~180M tokens): Python exercises and solutions, where each exercise is a docstring of a function that needs to be completed.\n\n\n\nModel architecture and training\nphi-1 is a decoder-only transformer using rotary position embedding, FlashAttention and multi-head attention (MHA), with parallel MHA and MLP layers + codegen-350M-mono‚Äôs tokenizer.\n\n\n\n\n\n\nTip\n\n\n\nIts architecture is very much inspired by CodeGen and does not include Fill-In-the-Middle or Multi-Query-Attention like StarCoder (low-hanging fruit improvement).\n\n\nHyperparameters for phi-1 with 1.3B/350M parameters:\n\n24/20 layers\nHidden dimension = 2048/1024\nMLP-inner dimension = 8192/4096\n32/16 attention heads with dimension = 64/32\nSequence length = 2048\nObjective = next-token prediction"
  },
  {
    "objectID": "notes/Large Language Models/phi1.html#the-importance-of-fine-tuning",
    "href": "notes/Large Language Models/phi1.html#the-importance-of-fine-tuning",
    "title": "phi-1 ‚Äì Textbooks Are All You Need",
    "section": "The importance of fine-tuning",
    "text": "The importance of fine-tuning\n\nFine-tuning phi-1 on CodeExercises greatly improves the model‚Äôs performance, even for tasks that are not in the fine-tuning dataset.\nThe authors notice that the model gets better at interpreting questions and logical relationships in the prompts. Interestingly, it also becomes better at using external libraries, even when they do not appear in the fine-tuning set (e.g., Pygame and Tkinter)."
  },
  {
    "objectID": "notes/Large Language Models/phi1.html#performance-evaluation",
    "href": "notes/Large Language Models/phi1.html#performance-evaluation",
    "title": "phi-1 ‚Äì Textbooks Are All You Need",
    "section": "Performance evaluation",
    "text": "Performance evaluation\nThe authors argue HumanEval‚Äôs binary score (code passes the unit tests, or it fails) does not capture the nuances of the model‚Äôs performance. They introduce an LLM grading using GPT-4 (between 0 and 10), which does not require tests.\n\nThere‚Äôs a concern that CodeExercises might contain samples that are similar to exercises in HumanEval. The authors propose to remove these samples and retrain phi-1 on this decontaminated set.\nThey report no meaningful n-gram overlap between CodeExercises and HumanEval (4 false positives). They then use a combination of embedding and syntax-based distances to find similar code snippets:\n\nSemantics: They use the L2 distance between embeddings produced by CodeGen\nSyntax: They calculate the (string) edit distance between the abstract syntax trees (ASTs) of two code snippets.\n\nDespite this data pruning, the authors claim that phi-1 still outperforms StarCoder on HumanEval."
  },
  {
    "objectID": "notes/Large Language Models/tart.html",
    "href": "notes/Large Language Models/tart.html",
    "title": "Tart ‚Äì A plug-and-play Transformer module for task-agnostic reasoning",
    "section": "",
    "text": "Tip\n\n\n\nTart combines the performance of fine-tuning with the ease-of-use of in-context learning. It is a general framework that leverages embeddings produced by LLMs with an improved task-agnostic reasoning module.\nüìù Paper: https://arxiv.org/abs/2306.07536\nüíª GitHub: https://github.com/HazyResearch/TART"
  },
  {
    "objectID": "notes/Large Language Models/tart.html#task-adaptation-techniques",
    "href": "notes/Large Language Models/tart.html#task-adaptation-techniques",
    "title": "Tart ‚Äì A plug-and-play Transformer module for task-agnostic reasoning",
    "section": "Task Adaptation Techniques",
    "text": "Task Adaptation Techniques\n\nGiven an LLM and limited labeled data for a task, how does one adapt the model to the task? We care about the following properties:\n\nTask-agnostic: We want to use the exact same model for different tasks.\nQuality: Performance should be competitive with task-specific methods.\nData-scalable: The more data, the better the performance.\n\nExisting methods:\n\nIn-context learning: Based on input, no parameter update. It is task-agnostic, but doesn‚Äôt have the same level of performance and is constrained by the context length.\nFine-tuning: Updates all the parameters. It is task-dependent.\nAdapters: Additional set of parameters that are updated for a given task. Performance is competitive, but it is task-dependent too."
  },
  {
    "objectID": "notes/Large Language Models/tart.html#representation-vs.-reasoning",
    "href": "notes/Large Language Models/tart.html#representation-vs.-reasoning",
    "title": "Tart ‚Äì A plug-and-play Transformer module for task-agnostic reasoning",
    "section": "Representation vs.¬†Reasoning",
    "text": "Representation vs.¬†Reasoning\nWhy ICL underperforms other methods? The authors assume it is either because:\n\nLLMs cannot generate good representations for the specific task.\nLLMs cannot perform probabilistic inference or reasoning using these representations.\n\nThe researchers use linear probing, a method that involves training a task-specific linear classifier using the representations generated by the LLM, to evaluate the information content of the representations.\nThey then decompose the performance gap between FT and ICL into two components: Œîrep, which represents the performance gap due to insufficient representation capacity, and Œîreas, which represents the performance gap due to insufficient reasoning abilities. \\begin{align}\n\\Delta_{\\text{perf}} &= \\text{Acc}_{\\text{FT}} - \\text{Acc}_{\\text{ICL}} \\\\\n&= \\text{Acc}_{\\text{FT}} - \\text{Acc}_{\\text{LR}} + \\text{Acc}_{\\text{LR}} - \\text{Acc}_{\\text{ICL}} \\\\\n&= \\Delta_{\\text{rep}} + \\Delta_{\\text{reas}}\n\\end{align}\n\n\nUsing the experiments from the previous figure, they make the following observations:\n\nLLMs lack reasoning abilities, despite having sufficient information in their representations (Figure a).\nFine-tuning improves task-specific reasoning, accounting for 73.06% of the improvements (Figure b).\nAdapters only learn task-specific reasoning abilities."
  },
  {
    "objectID": "notes/Large Language Models/tart.html#tart-task-agnostic-reasoning-transformers",
    "href": "notes/Large Language Models/tart.html#tart-task-agnostic-reasoning-transformers",
    "title": "Tart ‚Äì A plug-and-play Transformer module for task-agnostic reasoning",
    "section": "Tart: Task-Agnostic Reasoning Transformers",
    "text": "Tart: Task-Agnostic Reasoning Transformers\nTart learns an LLM and task-agnostic reasoning module without any task-specific training. It has two components:\n\nA genetic task-agnostic reasoning module, training on synthetic data (Gaussian logistic regression problems) with the objective of performing probabilistic inference.\nEmbeddings from the base LLM, aggregated to use as input along with the class label.\n\n\n\n\n\n\n\nNote\n\n\n\nGaussian logistic regression consists of regressing a given feature vector to a discrete binary label.\n\n\n\n1. Reasoning module\nThis module is based on GPT-2 and taught to predict the next item in a sequence (autoregressive).\nWe denote the input sequence with labeled examples (x_1, y_1), (x_2, y_2), \\dots , (x_k, y_k), with each example z_i = (x_i, y_i) using only two tokens (one for x, one for y). In comparison, standard LLMs would use multiple tokens to encode x, limiting the number of samples in the context window.\nTraining-wise, gradient descent is used to minimize a loss calculated using the cross-entropy function. The training sequences each represent a unique logistic regression problem, with parameters and features drawn from standard normal distributions.\nThe logistic output y is computed using a sigmoid function applied to a scaled dot product of feature vector and parameters, where the scaling factor represents the noise level.\nModel hyperparameters are set to a 16-dimensional input space, with labels encoded in this space using one-hot encoding. Interestingly, the paper reduces the high dimensionality of output embeddings by performing PCA using only the training points available for the specific task at hand.\n\n64 different logistic regression problems were tested by comparing logistic regression classifiers vs.¬†their reasoning module (Figure a). The error of the reasoning module was found to decrease with the increase in the number of examples, and was within 2% of the task-specific logistic function.\nThe noise level in logistic regression problems is an indicator of the problem‚Äôs difficulty, with lower values of Œ± signifying more difficult problems. The reasoning module was found to handle easier problems (with higher noise levels) without any drop in accuracy. However, for harder problems, the module‚Äôs error increased progressively (Figure b).\n\n\n2. Embeddings\n\nThe authors compared two types of embeddings:\n\nVanilla embeddings: They use all training examples and get the averaged embedding vectors. However, performance is degraded when there are too many samples in the context window.\nLeave-One-Out (LOO): The embedding for each training point is created by placing all other training examples before it in the prompt and averaging the embeddings over the final example‚Äôs tokens."
  },
  {
    "objectID": "notes/Large Language Models/tart.html#experiments",
    "href": "notes/Large Language Models/tart.html#experiments",
    "title": "Tart ‚Äì A plug-and-play Transformer module for task-agnostic reasoning",
    "section": "Experiments",
    "text": "Experiments\n\nTart was validated on different binary classification tasks in NLP, vision, and audio. It is used with GPT-Neo (125M).\nAveraged over all tasks and model families, Tart improves upon base in-context learning performance by 18.4 points, improves upon adapter heads by 3.4 points, and is within 3.1 points of full fine-tuning."
  },
  {
    "objectID": "notes/Machine Learning/data_influence.html",
    "href": "notes/Machine Learning/data_influence.html",
    "title": "Training Data Influence Analysis and Estimation A Survey",
    "section": "",
    "text": "üìù Paper: https://arxiv.org/pdf/2212.04612.pdf\nSurvey of methods to calculate the influence of training samples."
  },
  {
    "objectID": "notes/Machine Learning/data_influence.html#pointwise-training-data-influence",
    "href": "notes/Machine Learning/data_influence.html#pointwise-training-data-influence",
    "title": "Training Data Influence Analysis and Estimation A Survey",
    "section": "Pointwise Training Data Influence",
    "text": "Pointwise Training Data Influence\nQuantifies how a single training instance affects the model‚Äôs prediction on a single test instance according to some quality measure (e.g., test loss). Œ∏^‚àó := \\text{arg min} \\frac{1}{|D|} \\sum_{(x_i, y_i) \\in D} (y_i ‚àí Œ∏^\\top x_i)^2 Early pointwise influence analysis shows that a single outlier can completely shift the parameters of a least-squares regression. Thus, this model is completely non-robust. Different models have been proposed to increase the breakdown point, including changing the average function with a median function.\nModern methods can be categorized into two classes:\n\nRetraining-based methods: measure the training data‚Äôs influence by repeatedly retraining a model f using different subsets of training set D.\nGradient-based influence estimators: estimate influence via the alignment of training and test instance gradients, either throughout or at the end of training.\n\n\nAlternative perspectives on influence\nThe concept of influence is not clearly standardized:\n\nGroup influence: think batches of training data\nJoint influence: consider multiple test instances collectively\nMemorization: defined as the self-influence I(z_i, z_i)\nCook‚Äôs distance: measures the effect of training instances on the model parameters themselves I_{Cook}(z_i) := \\theta^{(T)} - \\theta^{(T)}_{D^{\\backslash i}}\nExpected influence: average influence across different instantiations and retrainings of a model class\nInfluence ranking orders training instances from most positively influential to most negatively influential"
  },
  {
    "objectID": "notes/Machine Learning/data_influence.html#retraining-based-influence-analysis",
    "href": "notes/Machine Learning/data_influence.html#retraining-based-influence-analysis",
    "title": "Training Data Influence Analysis and Estimation A Survey",
    "section": "Retraining-Based Influence Analysis",
    "text": "Retraining-Based Influence Analysis\nMeasures influence by training a model with and without some instance. Influence is then defined as the difference in these two models‚Äô behavior.\n\nLeave-On-Out Influence\nLeave-one-out (LOO) is the simplest influence measure described in this work. LOO is also the oldest, dating back to Cook and Weisberg [CW82] who term it case deletion diagnostics. I_{LOO}(z_i, z_{te}) := L(z_{te}; Œ∏^{(T)}_{D^{\\backslash i}}) ‚àí L(z_{te}; Œ∏^{(T)} ), Measuring the entire training set‚Äôs LOO influence requires training (n + 1) models.\n\n\nDownsampling\nMitigates leave-one-out influence‚Äôs two primary weaknesses: (1) computational complexity dependent on n and (2) instability due to stochastic training variation.\nRelies on an ensemble of K submodels, each trained on a subset D^k or the full training set D.\n\nIntuitively, it corresponds to z_{te}‚Äôs average risk when z_i is not used in submodel training.\nBy holding out multiple instances simultaneously and then averaging, each Downsampling submodel provides insight into the influence of all training instances. This allows Downsampling to require (far) fewer retrainings than LOO.\n\n\nShapley Value\nIntuitively, SV is the weighted change in z_{te}‚Äôs risk when z_i is added to a random training subset.\nIt can be viewed as generalizing the leave-one-out influence, where rather than considering only the full training set D, Shapley value averages the LOO influence across all possible subsets of D.\nThe main problem is that SV is computationally intractable for non-trivial datasets, which led to numerous speed-ups in the literature:\n\nTruncated Monte Carlo Shapley (TMC-Shapley): relies on randomized subset sampling from training set D.\nGradient Shapley (G-Shapley): even faster SV estimator that assumes models are trained in just one gradient step (at the expense of lower accuracy).\nk-NN-SV and **k-NN Shapley"
  },
  {
    "objectID": "notes/Machine Learning/data_influence.html#gradient-based-influence-estimation",
    "href": "notes/Machine Learning/data_influence.html#gradient-based-influence-estimation",
    "title": "Training Data Influence Analysis and Estimation A Survey",
    "section": "Gradient-Based Influence Estimation",
    "text": "Gradient-Based Influence Estimation\nIn models trained using gradient descent, the influence of training instances can be assessed through training gradients.\nThere are two types of gradient-based methods:\n\nStatic methods estimate the effect of retraining by examining gradients with respect to final model parameters, but this approach typically requires stronger assumptions due to the limited insight a single set of parameters can provide into the optimization landscape.\nDynamic methods analyze model parameters throughout training, which while being more computationally demanding, allows for fewer assumptions.\n\nHowever, both share a common limitation: they can potentially overlook highly influential training instances.\n\nStatic Estimators\nThere are two main static estimators: influence functions (more general) and representer point (more scalable).\n\nInfluence Functions\nAnalyze how a model changes when the weight of a training instance is slightly perturbed: \\theta^{(T)}_{+ \\epsilon_i} = \\arg \\min_{\\theta} \\frac{1}{n} \\sum_{z \\in D} L(z; \\theta) + \\epsilon_i L(z_i; \\theta). Assuming the model and loss function are twice-differentiable and strictly convex, Cook and Weisberg demonstrated that an infinitesimal perturbation‚Äôs impact could be calculated using a first-order Taylor expansion: \\frac{d\\theta^{(T)}_{+\\epsilon_i}}{d\\epsilon_i} \\bigg|_{\\epsilon_i=0} = - (H^{(T)}_\\theta)^{-1} \\nabla_\\theta L(z_i; \\theta^{(T)}), where the empirical risk Hessian H^{(T)}_\\theta := \\frac{1}{n} \\sum_{z \\in D} \\nabla^2_\\theta L(z; \\theta^{(T)}) is assumed to be positive definite.\nKoh and Liang extend this result to consider the effect of this infinitesimal perturbation on z_{te}‚Äôs risk, whereby applying the chain rule, we get:\n\\begin{align*}\n\\frac{dL(z_{te}; \\theta^{(T)})}{d\\epsilon_i} \\bigg|_{\\epsilon_i=0}\n\n&= \\frac{dL(z_{te}; \\theta^{(T)})}{d\\theta^{(T)}_{+\\epsilon_i}}^\\top {\\frac{d\\theta^{(T)}_{+\\epsilon_i}}{d\\epsilon_i}} \\bigg|_{\\epsilon_i=0} \\\\\n\n&= - \\nabla_\\theta L(z_{te}; \\theta^{(T)})^\\top (H^{(T)}_\\theta)^{-1} \\nabla_\\theta L(z_i; \\theta^{(T)}).\n\\end{align*}\nRemoving training instance z_i from D is equivalent to \\epsilon_i = -\\frac{1}{n}, resulting in the pointwise influence functions estimator \\hat{I}_{IF}(z_i, z_{te}) := \\frac{1}{n} \\nabla_\\theta L(z_{te}; \\theta^{(T)})^\\top (H^{(T)}_\\theta)^{-1} \\nabla_\\theta L(z_i; \\theta^{(T)}) Intuitively, it represents the influence functions‚Äô estimate of the leave-one-out influence of z_i on z_{te}.\n\n\nRepresenter Point Methods\nRepresenter-based methods rely on kernels, which are functions that measure the similarity between two vectors. They decompose the predictions of specific model classes into the individual contributions (i.e., influence) of each training instance.\n\n\n\nDynamic Estimators\n\nTracIn ‚Äì Tracing Gradient Descent\n\n\nHyDRA ‚Äì Hypergradient Data Relevance Analysis"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": " Research notes",
    "section": "",
    "text": "Training Data Influence Analysis and Estimation A Survey\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nLocal Large Language Models ‚Äì Int8\n\n\n\n\n\n\n\nLarge Language Models\n\n\nQuantization\n\n\n\n\n\n\n\n\n\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nLoraHub ‚Äì Efficient Cross-Task Generalization via Dynamic LoRA Composition\n\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nMultipack Sampler\n\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nInCoder ‚Äì A Generative Model for Code Infilling and Synthesis\n\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nOrca ‚Äì Progressive Learning from Complex Explanation Traces of GPT-4\n\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nphi-1 ‚Äì Textbooks Are All You Need\n\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nLongNet ‚Äì Scaling Transformers to 1,000,000,000 Tokens\n\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nTart ‚Äì A plug-and-play Transformer module for task-agnostic reasoning\n\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nInference Optimization ‚Äì Lil‚ÄôLog\n\n\n\n\n\n\n\nLarge Language Models\n\n\nQuantization\n\n\n\n\n\n\n\n\n\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nExtending the Context Window of LLMs\n\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nGPTQ ‚Äì Accurate Post-Training Quantization for Generative Pre-trained Transformers\n\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nLIMA ‚Äì Less Is More for Alignment\n\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nLoRA ‚Äì Low-Rank Adaptation of Large Language Models\n\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nReport ‚Äì Few-Shot Text Classification\n\n\n\n\n\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-02-13-Q_learning.html",
    "href": "posts/2022-02-13-Q_learning.html",
    "title": "Q-learning for beginners",
    "section": "",
    "text": "The goal of this article is to teach an AI how to solve the ‚ùÑÔ∏èFrozen Lake environment using reinforcement learning. Instead of reading Wikipedia articles and explaining formulas, we‚Äôre going to start from scratch and try to recreate the ü§ñQ-learning algorithm by ourselves. We‚Äôll not just understand how it works, but more importantly why it works: why was it designed that way? What are the hidden assumptions, the details that are never explained in regular courses and tutorials?\nAt the end of this article, you‚Äôll master the Q-learning algorithm and be able to apply it to other environments and real-world problems. It‚Äôs a cool mini-project that gives a better insight into how reinforcement learning works and can hopefully inspire ideas for original and creative applications.\nLet‚Äôs start by installing the Frozen Lake environment and importing the necessary libraries: gym for the game, random to generate random numbers, and numpy to do some math.\n!pip install -q gym matplotlib\n\nimport gym\nimport random\nimport numpy as np"
  },
  {
    "objectID": "posts/2022-02-13-Q_learning.html#i.-frozen-lake",
    "href": "posts/2022-02-13-Q_learning.html#i.-frozen-lake",
    "title": "Q-learning for beginners",
    "section": "‚ùÑÔ∏è I. Frozen Lake",
    "text": "‚ùÑÔ∏è I. Frozen Lake\nNow, let‚Äôs talk about the game we‚Äôre going to be solving in this tutorial. Frozen Lake is a simple environment composed of tiles, where the AI has to move from an initial tile to a goal. Tiles can be a safe frozen lake ‚úÖ, or a hole ‚ùå that gets you stuck forever. The AI, or agent, has 4 possible actions: go ‚óÄÔ∏èLEFT, üîΩDOWN, ‚ñ∂Ô∏èRIGHT, or üîºUP. The agent must learn to avoid holes in order to reach the goal in a minimal number of actions. By default, the environment is always in the same configuration. In the environment‚Äôs code, each tile is represented by a letter as follows:\nS F F F       (S: starting point, safe)\nF H F H       (F: frozen surface, safe)\nF F F H       (H: hole, stuck forever)\nH F F G       (G: goal, safe)\n\n\n\n\nWe can try to manually solve the example above to understand the game. Let‚Äôs see if the following sequence of actions is a correct solution: RIGHT \\to RIGHT \\to RIGHT \\to DOWN \\to DOWN \\to DOWN. Our agent starts on tile S, so we move right on a frozen surface ‚úÖ, then again ‚úÖ, then once more ‚úÖ, then we go down and find a hole ‚ùå.\nActually, it‚Äôs really easy to find several correct solutions: RIGHT \\to RIGHT \\to DOWN \\to DOWN \\to DOWN \\to RIGHT is an obvious one. But we could make a sequence of actions that loops around a hole 10 times before reaching the goal. This sequence is valid, but it doesn‚Äôt meet our final requirement: the agent needs to meet the goal in a minimum number of actions. In this example, the minimum number of actions to complete the game is 6. We need to remember this fact to check if our agent really masters ‚ùÑÔ∏èFrozen Lake or not.\n\n\n\n\nLet‚Äôs initialize the environment thanks to the gym library. There are two versions of the game: one with slippery ice, where selected actions have a random chance of being disregarded by the agent; and a non-slippery one, where actions cannot be ignored. We‚Äôll use the non-slippery one to begin with because it‚Äôs easier to understand.\n\n# Initialize the non-slippery Frozen Lake environment\nenvironment = gym.make(\"FrozenLake-v1\", is_slippery=False)\nenvironment.reset()\nenvironment.render()\n\n\nSFFF\nFHFH\nFFFH\nHFFG\n\n\nWe can see that the game that was created has the exact same configuration as in our example: it is the same puzzle. The position of our agent is indicated by a red rectangle. Solving this puzzle can be done with a simple script and if‚Ä¶else conditions, which would actually be useful to compare our AI to a simpler approach. However, we want to try a more exciting solution: reinforcement learning."
  },
  {
    "objectID": "posts/2022-02-13-Q_learning.html#ii.-q-table",
    "href": "posts/2022-02-13-Q_learning.html#ii.-q-table",
    "title": "Q-learning for beginners",
    "section": "üèÅ II. Q-table",
    "text": "üèÅ II. Q-table\nIn ‚ùÑÔ∏èFrozen Lake, there are 16 tiles, which means our agent can be found in 16 different positions, called states. For each state, there are 4 possible actions: go ‚óÄÔ∏èLEFT, üîΩDOWN, ‚ñ∂Ô∏èRIGHT, and üîºUP. Learning how to play Frozen Lake is like learning which action you should choose in every state. To know which action is the best in a given state, we would like to assign a quality value to our actions. We have 16 states and 4 actions, so want to calculate 16 \\times 4 = 64 values.\nA nice way of representing it is using a table, known as a Q-table, where rows list every state s and columns list every action a. In this Q-table, each cell contains a value Q(s, a), which is the value (quality) of the action a in the state s (1 if it‚Äôs the best action possible, 0 if it‚Äôs really bad). When our agent is in a particular state s, it just has to check this table to see which action has the highest value. Taking the action with the highest value makes sense but we‚Äôll see later that we can design something even better‚Ä¶\n\n\n\nState\n‚óÄÔ∏èLEFT\nüîΩDOWN\n‚ñ∂Ô∏èRIGHT\nüîºUP\n\n\n\n\nS=0\nQ(0, ‚óÄÔ∏è)\nQ(0, üîΩ)\nQ(0, ‚ñ∂Ô∏è)\nQ(0, üîº)\n\n\n1\nQ(1, ‚óÄÔ∏è)\nQ(1, üîΩ)\nQ(1, ‚ñ∂Ô∏è)\nQ(1, üîº)\n\n\n2\nQ(2, ‚óÄÔ∏è)\nQ(2, üîΩ)\nQ(2, ‚ñ∂Ô∏è)\nQ(2, üîº)\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n14\nQ(14, ‚óÄÔ∏è)\nQ(14, üîΩ)\nQ(14, ‚ñ∂Ô∏è)\nQ(14, üîº)\n\n\nG=15\nQ(15, ‚óÄÔ∏è)\nQ(15, üîΩ)\nQ(15, ‚ñ∂Ô∏è)\nQ(15, üîº)\n\n\n\n\nExample of Q-table, where each cell contains the value Q(a, s) of the action a (column) in a given state s (row)\n\nLet‚Äôs create our Q-table and fill it with zeros since we still have no idea of the value of each action in each state.\n\n# Initialize Q-table with zeros \n# Our table has the following dimensions:\n# (rows x columns) = (states x actions) = (16 x 4)\nqtable = np.zeros((16, 4))\n\n# Alternatively, the gym library can also directly g\n# give us the number of states and actions using \n# \"env.observation_space.n\" and \"env.action_space.n\"\nnb_states = environment.observation_space.n  # = 16\nnb_actions = environment.action_space.n      # = 4\nqtable = np.zeros((nb_states, nb_actions))\n\n# Let's see how it looks\nprint('Q-table =')\nprint(qtable)\n\nQ-table =\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n\n\nGreat! We have our Q-table with 16 rows (our 16 states) and 4 columns (our 4 actions) as expected. Let‚Äôs try to see what we can do next: every value is set to zero, so we have no information at all. Let‚Äôs say that the agent takes a random action: ‚óÄÔ∏èLEFT, üîΩDOWN, ‚ñ∂Ô∏èRIGHT, or üîºUP.\nWe can use the random library with the choice method to randomly choose an action.\n\nrandom.choice([\"LEFT\", \"DOWN\", \"RIGHT\", \"UP\"])\n\n'RIGHT'\n\n\nWait, actually the agent is currently on the initial state S, which means only two actions are possible: ‚ñ∂Ô∏èRIGHT and üîΩDOWN. The agent can also take the actions üîºUP and ‚óÄÔ∏èLEFT, but it won‚Äôt move: its state doesn‚Äôt change. Therefore, we do not put any constraint on what actions are possible: the agent will naturally understand that some of them don‚Äôt do anything.\nWe can keep using random.choice(), but the gym library already implements a method to randomly choose an action. It might save us some hassle later, so let‚Äôs try it.\n\nenvironment.action_space.sample()\n\n2\n\n\nOops‚Ä¶ this time it‚Äôs a number. We could read gym‚Äôs documentation but it is quite scarce unfortunately. No worries though, we can check the source code on GitHub to understand what these numbers mean. It‚Äôs actually very straightforward:\n‚óÄÔ∏è LEFT = 0\nüîΩ DOWN = 1\n‚ñ∂Ô∏è RIGHT = 2\nüîº UP = 3\n\n\n\n\n\nOkay, now that we understand how gym connects numbers to directions, let‚Äôs try to use it to move our agent to the right ‚ñ∂Ô∏è. This time, it can be performed using the step(action) method. We can try to directly provide it the number 2, corresponding to the direction we chose (right), and check if the agent moved.\n\nenvironment.step(2)\nenvironment.render()\n\n  (Right)\nSFFF\nFHFH\nFFFH\nHFFG\n\n\nHuzzah! The red square moved from the initial state S to the right: our prediction was correct. And that‚Äôs all we need to know in order to interact with the environment:\n\nHow to randomly choose an action using action_space.sample();\nHow to implement this action and move our agent in the desired direction with step(action).\n\nTo be exhaustive, we can add:\n\nHow to display the current map to see what we‚Äôre doing with render();\nHow to restart the game when the agent falls into a hole or reaches the goal G with reset().\n\nNow that we understand how to interact with our gym environment, let‚Äôs go back to our algorithm. In reinforcement learning, agents are rewarded by the environment when they accomplish a predefined goal. In ‚ùÑÔ∏èFrozen Lake, the agent is only rewarded when it reaches the state G (see the source code). We cannot control this reward, it is set in the environment: it returns 1 when the agent reaches G, and 0 otherwise.\nLet‚Äôs print it every time we implement an action. The reward is given by the method step(action).\n\n# 1. Randomly choose an action using action_space.sample()\naction = environment.action_space.sample()\n\n# 2. Implement this action and move the agent in the desired direction\nnew_state, reward, done, info = environment.step(action)\n\n# Display the results (reward and map)\nenvironment.render()\nprint(f'Reward = {reward}')\n\n  (Down)\nSFFF\nFHFH\nFFFH\nHFFG\nReward = 0.0\n\n\nThe reward is indeed 0‚Ä¶ üò± wow, I guess we‚Äôre in a pickle, because only one state can give us a positive reward in the entire game. How are we supposed to take the right directions at the very beginning when the only validation we have is at the very end? If we ever want to see a reward of 1, we‚Äôd need to be lucky enough to find the correct sequence of actions by chance. Unfortunately, that‚Äôs exactly how it works‚Ä¶ the Q-table will remain filled with zeros until the agent randomly reaches the goal G.\nThe problem would be much simpler if we could have intermediate, smaller rewards to guide our path towards the goal G. Alas, this is actually one of the main issues of reinforcement learning: this phenomenon, called sparse rewards, makes agents very difficult to train on problems where the only reward is at the end of a long sequence of actions. Different techniques were proposed to mitigate this issue, but we‚Äôll talk about it another time."
  },
  {
    "objectID": "posts/2022-02-13-Q_learning.html#iii.-q-learning",
    "href": "posts/2022-02-13-Q_learning.html#iii.-q-learning",
    "title": "Q-learning for beginners",
    "section": "ü§ñ III. Q-learning",
    "text": "ü§ñ III. Q-learning\nLet‚Äôs go back to our problem. Okay, we need to be lucky enough to find the goal G by accident. But once it‚Äôs done, how to backpropagate the information to the initial state? The Q-learning algorithm offers a clever solution to this issue. We need to update the value of our state-action pairs (each cell in the Q-table) considering 1/ the reward for reaching the next state, and 2/ the highest possible value in the next state.\n\n\n\n\nWe know we get a reward of 1 when we move to G. As we just said, the value of the state next to G (let‚Äôs call it G-1) with the relevant action to reach G is increased thanks to the reward. Okay good, end of the episode: the agent won and we restart the game. Now, the next time the agent is in a state next to G-1, it will increase the value of this state (let‚Äôs call it G-2) with the relevant action to reach G-1. The next time the agent is in a state next to G-2, it will do the same. Rinse and repeat, until the update reaches the initial state S.\nLet‚Äôs try to find the update formula to backpropagate the values from G to S. Remember: values denote the quality of an action in a specific state (0 if it‚Äôs terrible, 1 if it‚Äôs the best action possible in this state). We try to update the value of the action a_t (for example, a_t = 0 if the action is left) in the state s_t (for example, s_t = 0 when the agent is in the initial state S). This value is just a cell in our Q-table, corresponding to the row number s_t and the column number a_t: this value is formally called Q(s_t, a_t).\nAs we said previously, we need to update it using 1/ the reward for the next state (formally noted r_t), and 2/ the maximum possible value in the next state (max_aQ(s_{t+1},a)). Therefore, the update formula must look like:\nQ_{new}(s_t, a_t) = Q(s_t, a_t) + r_t + max_aQ(s_{t+1}, a)\nThe new value is the current one + the reward + the highest value in the next state. We can manually try our formula to check if it looks correct: let‚Äôs pretend our agent is in the state G-1 next to the goal G for the first time. We can update the value corresponding to the winning action in this state G-1 with Q_{new}(G-1, a_t) = Q(G-1, a_t) + r_t + max_aQ(G, a), where Q(G-1, a_t) = 0 and max_aQ(G, a) = 0 because the Q-table is empty, and r_t = 1 because we get the only reward in this environment. We obtain Q_{new}(G-1, a_t) = 1. The next time the agent is in a state next to this one (G-2), we update it too using the formula and get the same result: Q_{new}(G-2, a_t) = 1. In the end, we backpropagate ones in the Q-table from G to S. Okay it works, but the result is binary: either it‚Äôs the wrong state-action pair or the best one. We would like more nuance‚Ä¶\nActually, we almost found the true Q-learning update formula with common sense. The nuance we‚Äôre looking for adds two parameters:\n\n\\alpha is the üí°learning rate (between 0 and 1), which is how much we should change the original Q(s_t, a_t) value. If \\alpha = 0, the value never changes, but if \\alpha = 1, the value changes extremely fast. In our attempt, we didn‚Äôt limit the learning rate so \\alpha = 1. But this is too fast in reality: the reward and the maximum value in the next state quickly overpower the current value. We need to find a balance between the importance of past and new knowledge.\n\\gamma is the üìâdiscount factor (between 0 and 1), which determines how much the agent cares about future rewards compared to immediate ones (as the saying goes, ‚Äúa bird in the hand is worth two in the bush‚Äù). If \\gamma = 0, the agent only focuses on immediate rewards , but if \\gamma = 1, any potential future reward has the same value than current ones. In ‚ùÑÔ∏èFrozen Lake, we want a high discount factor since there‚Äôs only one possible reward at the very end of the game.\n\nWith the real Q-learning algorithm, the new value is calculated as follows:\nQ_{new}(s_t, a_t) = Q(s_t, a_t) + \\alpha \\cdot (r_t + \\gamma \\cdot max_aQ(s_{t+1},a) - Q(s_t, a_t))\nOkay, let‚Äôs try this new formula before implementing it. Once again, we can pretend that our agent is next to the goal G for the first time. We can update the state-action pair to win the game using our formula: Q_{new}(G-1, a_t) = 0 + \\alpha \\cdot (1 + \\gamma \\cdot 0 - 0). We can assign arbitrary values to \\alpha and \\gamma to calculate the result. With \\alpha = 0.5 and \\gamma = 0.9, we get Q_{new}(G-1, a_t) = 0 + 0.5 \\cdot (1 + 0.9 \\cdot 0 - 0) = 0.5. The second time the agent is in this state, we would get: Q_{new}(G-1, a_t) = 0.5 + 0.5 \\cdot (1 + 0.9 \\cdot 0 - 0.5) = 0.75, then 0.875, 0.9375, 0.96875, etc.\n\n\n\n\nSo training our agent in code means:\n\nChoosing a random action (using action_space.sample()) if the values in the current state are just zeros. Otherwise, we take the action with the highest value in the current state with the function np.argmax();\nImplementing this action by moving in the desired direction with step(action);\nUpdating the value of the original state with the action we took, using information about the new state and the reward given by step(action);\n\nWe keep repeating these 3 steps until the agent gets stuck in a hole or reaches the goal G. When it happens, we just restart the environment with reset() and start a new episode until we hit 1,000 episodes. Additionally, we can plot the outcome of each run (failure if it didn‚Äôt reach the goal, success otherwise) to observe the progress of our agent.\n\n# Import matplotlib to plot the outcomes\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.dpi'] = 300\nplt.rcParams.update({'font.size': 17})\n\n# We re-initialize the Q-table\nqtable = np.zeros((environment.observation_space.n, environment.action_space.n))\n\n# Hyperparameters\nepisodes = 1000        # Total number of episodes\nalpha = 0.5            # Learning rate\ngamma = 0.9            # Discount factor\n\n# List of outcomes to plot\noutcomes = []\n\nprint('Q-table before training:')\nprint(qtable)\n\n# Training\nfor _ in range(episodes):\n    state = environment.reset()\n    done = False\n\n    # By default, we consider our outcome to be a failure\n    outcomes.append(\"Failure\")\n\n    # Until the agent gets stuck in a hole or reaches the goal, keep training it\n    while not done:\n        # Choose the action with the highest value in the current state\n        if np.max(qtable[state]) &gt; 0:\n          action = np.argmax(qtable[state])\n\n        # If there's no best action (only zeros), take a random one\n        else:\n          action = environment.action_space.sample()\n             \n        # Implement this action and move the agent in the desired direction\n        new_state, reward, done, info = environment.step(action)\n\n        # Update Q(s,a)\n        qtable[state, action] = qtable[state, action] + \\\n                                alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action])\n        \n        # Update our current state\n        state = new_state\n\n        # If we have a reward, it means that our outcome is a success\n        if reward:\n          outcomes[-1] = \"Success\"\n\nprint()\nprint('===========================================')\nprint('Q-table after training:')\nprint(qtable)\n\n# Plot outcomes\nplt.figure(figsize=(12, 5))\nplt.xlabel(\"Run number\")\nplt.ylabel(\"Outcome\")\nax = plt.gca()\nplt.bar(range(len(outcomes)), outcomes, width=1.0)\nplt.show()\n\nQ-table before training:\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n\n===========================================\nQ-table after training:\n[[0.        0.59049   0.        0.       ]\n [0.        0.        0.        0.       ]\n [0.        0.0455625 0.        0.       ]\n [0.        0.        0.        0.       ]\n [0.        0.6561    0.        0.       ]\n [0.        0.        0.        0.       ]\n [0.        0.3796875 0.        0.       ]\n [0.        0.        0.        0.       ]\n [0.        0.        0.729     0.       ]\n [0.        0.        0.81      0.       ]\n [0.        0.9       0.        0.       ]\n [0.        0.        0.        0.       ]\n [0.        0.        0.        0.       ]\n [0.        0.        0.        0.       ]\n [0.        0.        1.        0.       ]\n [0.        0.        0.        0.       ]]\n\n\n\n\n\nThe agent is trained! Each blue bar on the figure corresponds to a win, so we can see that the agent had a hard time finding the goal at the beginning of the training. But once it found it several times in a row, it began to consistently win. The trained Q-table is also very interesting: these values indicate the unique sequence of actions the agent learned to reach the goal.\nNow let‚Äôs see how it performs by evaluating it on 100 episodes. We consider that the training is over, so we don‚Äôt need to update the Q-table anymore. To see how the agent performs, we can calculate the percentage of times the it managed to reach the goal (success rate).\n\nepisodes = 100\nnb_success = 0\n\n# Evaluation\nfor _ in range(100):\n    state = environment.reset()\n    done = False\n    \n    # Until the agent gets stuck or reaches the goal, keep training it\n    while not done:\n        # Choose the action with the highest value in the current state\n        if np.max(qtable[state]) &gt; 0:\n          action = np.argmax(qtable[state])\n\n        # If there's no best action (only zeros), take a random one\n        else:\n          action = environment.action_space.sample()\n             \n        # Implement this action and move the agent in the desired direction\n        new_state, reward, done, info = environment.step(action)\n\n        # Update our current state\n        state = new_state\n\n        # When we get a reward, it means we solved the game\n        nb_success += reward\n\n# Let's check our success rate!\nprint (f\"Success rate = {nb_success/episodes*100}%\")\n\nSuccess rate = 100.0%\n\n\nNot only our agent has been trained, but it manages to hit a 100% success rate. Great job everyone, the non-slippery ‚ùÑÔ∏èFrozen Lake is solved!\nWe can even visualize the agent moving on the map by executing the code below and print the sequence of actions it took to check if it‚Äôs the best one.\n\nfrom IPython.display import clear_output\nimport time \n\nstate = environment.reset()\ndone = False\nsequence = []\n\nwhile not done:\n    # Choose the action with the highest value in the current state\n    if np.max(qtable[state]) &gt; 0:\n      action = np.argmax(qtable[state])\n\n    # If there's no best action (only zeros), take a random one\n    else:\n      action = environment.action_space.sample()\n    \n    # Add the action to the sequence\n    sequence.append(action)\n\n    # Implement this action and move the agent in the desired direction\n    new_state, reward, done, info = environment.step(action)\n\n    # Update our current state\n    state = new_state\n\n    # Update the render\n    clear_output(wait=True)\n    environment.render()\n    time.sleep(1)\n\nprint(f\"Sequence = {sequence}\")\n\n  (Right)\nSFFF\nFHFH\nFFFH\nHFFG\nSequence = [1, 1, 2, 2, 1, 2]\n\n\nThe agent can learn several correct sequence of actions: [2, 2, 1, 1, 1, 2], [1, 1, 2, 2, 1, 2], etc. The good thing is there‚Äôs only 6 actions in our sequence, which was the minimum possible number of actions we counted: it means that our agent learned to solve the game in an optimal way. In the case of [2, 2, 1, 1, 1, 2], which corresponds to RIGHT \\to RIGHT \\to DOWN \\to DOWN \\to DOWN \\to RIGHT, it‚Äôs exactly the sequence we predicted at the very beginning of the article. üì£"
  },
  {
    "objectID": "posts/2022-02-13-Q_learning.html#iv.-epsilon-greedy-algorithm",
    "href": "posts/2022-02-13-Q_learning.html#iv.-epsilon-greedy-algorithm",
    "title": "Q-learning for beginners",
    "section": "üìê IV. Epsilon-Greedy algorithm",
    "text": "üìê IV. Epsilon-Greedy algorithm\nDespite this success, there‚Äôs something that bothers me with our previous approach: the agent always chooses the action with the highest value. So whenever a state-action pair starts having a non-zero value, the agent will always choose it. The other actions will never be taken, which means we‚Äôll never update their value‚Ä¶ But what if one of these actions was better than the one the agent always takes? Shouldn‚Äôt we encourage the agent to try new things from time to time and see if it can improve?\nIn other words, we want to allow our agent to either:\n\nTake the action with the highest value (exploitation);\nChoose a random action to try to find even better ones (exploration).\n\nWe want a tradeoff between these two behaviors: if the agent only focuses on exploitation, it cannot try new solutions and thus doesn‚Äôt learn anymore. On the other hand, if the agent only takes random actions, the training is pointless since it doesn‚Äôt use the Q-table. So we want to change this parameter over time: at the beginning of the training, we want to explore the environment as much as possible. But exploration becomes less and less interesting, as the agent already knows every possible state-action pairs. This parameter represents the amount of randomness in the action selection.\nThis technique is commonly called the epsilon-greedy algorithm, where epsilon is our parameter. It is a simple but extremely efficient method to find a good tradeoff. Every time the agent has to take an action, it has a probability Œµ of choosing a random one, and a probability 1-Œµ of choosing the one with the highest value. We can decrease the value of epsilon at the end of each episode by a fixed amount (linear decay), or based on the current value of epsilon (exponential decay).\n\n\n\n\nLet‚Äôs implement a linear decay. Beforehand, I‚Äôd like to see how the curve looks like with arbitrary parameters. We‚Äôll start with Œµ = 1 to be in full exploration mode, and decrease this value by 0.001 after each episode.\n\n\n\nOkay now that we have a sound understanding of it, we can implement it for real and see how it changes the agent‚Äôs behavior.\n\n# We re-initialize the Q-table\nqtable = np.zeros((environment.observation_space.n, environment.action_space.n))\n\n# Hyperparameters\nepisodes = 1000        # Total number of episodes\nalpha = 0.5            # Learning rate\ngamma = 0.9            # Discount factor\nepsilon = 1.0          # Amount of randomness in the action selection\nepsilon_decay = 0.001  # Fixed amount to decrease\n\n# List of outcomes to plot\noutcomes = []\n\nprint('Q-table before training:')\nprint(qtable)\n\n# Training\nfor _ in range(episodes):\n    state = environment.reset()\n    done = False\n\n    # By default, we consider our outcome to be a failure\n    outcomes.append(\"Failure\")\n    \n    # Until the agent gets stuck in a hole or reaches the goal, keep training it\n    while not done:\n        # Generate a random number between 0 and 1\n        rnd = np.random.random()\n\n        # If random number &lt; epsilon, take a random action\n        if rnd &lt; epsilon:\n          action = environment.action_space.sample()\n        # Else, take the action with the highest value in the current state\n        else:\n          action = np.argmax(qtable[state])\n             \n        # Implement this action and move the agent in the desired direction\n        new_state, reward, done, info = environment.step(action)\n\n        # Update Q(s,a)\n        qtable[state, action] = qtable[state, action] + \\\n                                alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action])\n        \n        # Update our current state\n        state = new_state\n\n        # If we have a reward, it means that our outcome is a success\n        if reward:\n          outcomes[-1] = \"Success\"\n\n    # Update epsilon\n    epsilon = max(epsilon - epsilon_decay, 0)\n\nprint()\nprint('===========================================')\nprint('Q-table after training:')\nprint(qtable)\n\n# Plot outcomes\nplt.figure(figsize=(12, 5))\nplt.xlabel(\"Run number\")\nplt.ylabel(\"Outcome\")\nax = plt.gca()\nplt.bar(range(len(outcomes)), outcomes, width=1.0)\nplt.show()\n\nQ-table before training:\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n\n===========================================\nQ-table after training:\n[[0.531441   0.59049    0.59049    0.531441  ]\n [0.531441   0.         0.6561     0.59049   ]\n [0.59049    0.729      0.59049    0.6561    ]\n [0.6561     0.         0.59048879 0.59047491]\n [0.59049    0.6561     0.         0.531441  ]\n [0.         0.         0.         0.        ]\n [0.         0.81       0.         0.6561    ]\n [0.         0.         0.         0.        ]\n [0.6561     0.         0.729      0.59049   ]\n [0.65609944 0.80999966 0.81       0.        ]\n [0.729      0.9        0.         0.729     ]\n [0.         0.         0.         0.        ]\n [0.         0.         0.         0.        ]\n [0.         0.60548384 0.9        0.72407524]\n [0.81       0.9        1.         0.81      ]\n [0.         0.         0.         0.        ]]\n\n\n\n\n\nHey, the agent takes more time to consistently win the game now! And the Q-table has a lot more non-zero values than the previous one, which means the agent has learned several sequences of actions to reach the goal. It is understandable, since this new agent is forced to explore state-action pairs instead of always exploiting ones with non-zero values.\nLet‚Äôs see if it‚Äôs as successful as the previous one to win the game. In evaluation mode, we don‚Äôt want exploration anymore because the agent is trained now.\n\nepisodes = 100\nnb_success = 0\n\n# Evaluation\nfor _ in range(100):\n    state = environment.reset()\n    done = False\n    \n    # Until the agent gets stuck or reaches the goal, keep training it\n    while not done:\n        # Choose the action with the highest value in the current state\n        action = np.argmax(qtable[state])\n\n        # Implement this action and move the agent in the desired direction\n        new_state, reward, done, info = environment.step(action)\n\n        # Update our current state\n        state = new_state\n\n        # When we get a reward, it means we solved the game\n        nb_success += reward\n\n# Let's check our success rate!\nprint (f\"Success rate = {nb_success/episodes*100}%\")\n\nSuccess rate = 100.0%\n\n\nPhew, it‚Äôs another 100% success rate! We didn‚Äôt degrade the model. The benefits of this approach might not be obvious in this example, but our model became less static and more flexible. It learned different paths (sequences of actions) from S to G instead of just one as in the previous approach. More exploration can degrade performance but it‚Äôs necessary to train agents that can adapt to new environments."
  },
  {
    "objectID": "posts/2022-02-13-Q_learning.html#iv.-challenge-slippery-frozen-lake",
    "href": "posts/2022-02-13-Q_learning.html#iv.-challenge-slippery-frozen-lake",
    "title": "Q-learning for beginners",
    "section": "‚ùÑÔ∏è IV. Challenge: slippery Frozen Lake",
    "text": "‚ùÑÔ∏è IV. Challenge: slippery Frozen Lake\nWe didn‚Äôt solve the entire Frozen Lake environment: we only trained an agent on the non-slippery version, using is_slippery = False during initialization. In the slippery variant, the action the agent takes only has 33% chance of succeeding. In case of failure, one of the three other actions is randomly taken instead. This feature adds a lot of randomness to the training, which makes things more difficult for our agent. Let‚Äôs see how well our code is doing in this new environment‚Ä¶\n\n# Initialize the slippery Frozen Lake\nenvironment = gym.make(\"FrozenLake-v1\", is_slippery=True)\nenvironment.reset()\n\n# We re-initialize the Q-table\nqtable = np.zeros((environment.observation_space.n, environment.action_space.n))\n\n# Hyperparameters\nepisodes = 1000        # Total number of episodes\nalpha = 0.5            # Learning rate\ngamma = 0.9            # Discount factor\nepsilon = 1.0          # Amount of randomness in the action selection\nepsilon_decay = 0.001  # Fixed amount to decrease\n\n# List of outcomes to plot\noutcomes = []\n\nprint('Q-table before training:')\nprint(qtable)\n\n# Training\nfor _ in range(episodes):\n    state = environment.reset()\n    done = False\n\n    # By default, we consider our outcome to be a failure\n    outcomes.append(\"Failure\")\n    \n    # Until the agent gets stuck in a hole or reaches the goal, keep training it\n    while not done:\n        # Generate a random number between 0 and 1\n        rnd = np.random.random()\n\n        # If random number &lt; epsilon, take a random action\n        if rnd &lt; epsilon:\n          action = environment.action_space.sample()\n        # Else, take the action with the highest value in the current state\n        else:\n          action = np.argmax(qtable[state])\n             \n        # Implement this action and move the agent in the desired direction\n        new_state, reward, done, info = environment.step(action)\n\n        # Update Q(s,a)\n        qtable[state, action] = qtable[state, action] + \\\n                                alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action])\n        \n        # Update our current state\n        state = new_state\n\n        # If we have a reward, it means that our outcome is a success\n        if reward:\n          outcomes[-1] = \"Success\"\n\n    # Update epsilon\n    epsilon = max(epsilon - epsilon_decay, 0)\n\nprint()\nprint('===========================================')\nprint('Q-table after training:')\nprint(qtable)\n\n# Plot outcomes\nplt.figure(figsize=(12, 5))\nplt.xlabel(\"Run number\")\nplt.ylabel(\"Outcome\")\nax = plt.gca()\nplt.bar(range(len(outcomes)), outcomes, width=1.0)\nplt.show()\n\nepisodes = 100\nnb_success = 0\n\n# Evaluation\nfor _ in range(100):\n    state = environment.reset()\n    done = False\n    \n    # Until the agent gets stuck or reaches the goal, keep training it\n    while not done:\n        # Choose the action with the highest value in the current state\n        action = np.argmax(qtable[state])\n\n        # Implement this action and move the agent in the desired direction\n        new_state, reward, done, info = environment.step(action)\n\n        # Update our current state\n        state = new_state\n\n        # When we get a reward, it means we solved the game\n        nb_success += reward\n\n# Let's check our success rate!\nprint (f\"Success rate = {nb_success/episodes*100}%\")\n\nQ-table before training:\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n\n===========================================\nQ-table after training:\n[[1.05252351e-01 3.78423474e-02 3.62199013e-02 2.23498832e-02]\n [2.61798645e-02 4.26276348e-03 8.64731745e-03 5.53546712e-02]\n [5.85998622e-02 7.72357953e-03 4.80325107e-03 1.07726690e-02]\n [3.67626921e-03 4.35241422e-03 2.52407133e-03 2.06007352e-02]\n [1.19691783e-01 4.25479653e-03 2.31618416e-02 1.22786174e-02]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.24380078e-03 1.58879391e-03 3.22089381e-02 5.75193585e-04]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [3.93362525e-02 2.49045571e-02 2.96786309e-02 1.52367294e-01]\n [1.21022336e-01 3.11776736e-01 9.27730765e-02 8.23315493e-02]\n [2.70878170e-01 9.02280152e-02 5.70375682e-02 5.47929688e-02]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [8.23969532e-02 6.18706046e-02 4.71186580e-01 2.43109188e-01]\n [2.96197582e-01 9.02165586e-01 3.35387729e-01 2.46850733e-01]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\nSuccess rate = 76.0%\n\n\n\n\n\nIt‚Äôs not so good. But can you improve the performance by tweaking the different parameters we talked about? I encourage you to take this little challenge and do it on your own to have fun with reinforcement learning and check if you understood everything we said about Q-learning. And why not implementing exponential decay for the epsilon-greedy algorithm too?\nDuring this quick exercise, you might realize that slightly modifying the hyperparameters can completely destroy the results. This is another quirk of reinforcement learning: hyperparameters are quite moody, and it is important to understand their meaning if you want to tweak them. It‚Äôs always good to test and try new combinations to build your intuition and become more efficient. Good luck and have fun!"
  },
  {
    "objectID": "posts/2022-02-13-Q_learning.html#v.-conclusion",
    "href": "posts/2022-02-13-Q_learning.html#v.-conclusion",
    "title": "Q-learning for beginners",
    "section": "üîö V. Conclusion",
    "text": "üîö V. Conclusion\nQ-learning is a simple yet powerful algorithm at the core of reinforcement learning. In this article,\n\nWe learned to interact with the gym environment to choose actions and move our agent;\nWe introduced the idea of a Q-table, where rows are states, columns are actions, and cells are the value of an action in a given state;\nWe experimentally recreated the Q-learning update formula to tackle the sparse reward problem;\nWe implemented an entire training and evaluation process, that solved the Frozen Lake environment with 100% success rate;\nWe implemented the famous epsilon-greedy algorithm in order to create a tradeoff between the exploration of unknown state-action pairs and the exploitation of the most successful ones.\n\nThe Frozen Lake is a very simple environment, but others can have so many states and actions that it becomes impossible to store the Q-table in memory. This is especially the case in environments where events are not discrete, but continuous (like Super Mario Bros.¬†or Minecraft). When the problem arises, a popular technique consists of training a deep neural network to approximate the Q-table. This method adds several layers of complexity, since the neural networks are not very stable. But I will cover it in another tutorial with different techniques to stabilize them.\nUntil then, share this article if it helped you and follow me on Twitter and Medium for more practical content around machine learning and deep learning. üì£"
  },
  {
    "objectID": "posts/2022-03-02-Linear_Programming.html",
    "href": "posts/2022-03-02-Linear_Programming.html",
    "title": "Introduction to Linear Programming in Python",
    "section": "",
    "text": "Linear programming is a technique to optimize any problem with multiple variables and constraints. It‚Äôs a simple but powerful tool every data scientist should master.\nImagine you are a strategist recruiting an army. You have:\nHorsemen are stronger than bowmen, who are in turn stronger than swordsmen. The following table provides the cost and power of each unit:\nNow we have 1200 üåæfood, 800 ü™µwood, and 600 ü™ôgold. How should we maximize the power of our army considering these resources?\nWe could simply find the unit with the best power/cost ratio, take as many of them as possible, and repeat the process with the other two units. But this ‚Äúguess and check‚Äù solution might not even be optimal‚Ä¶\nNow imagine we have millions of units and resources: the previous greedy strategy is likely to completely miss the optimal solution. It is possible to use a machine learning algorithm (e.g., a genetic algorithm) to solve this problem, but we have no guarantee that the solution will be optimal either.\nFortunately for us, there is a method that can solve our problem in an optimal way: linear programming (or linear optimization), which is part of the field of operations research (OR). In this article, we‚Äôll use it to find the best numbers of swordsmen, bowmen, and horsemen to build the army with the highest power possible.\nYou can run the code from this tutorial with the following Google Colab notebook."
  },
  {
    "objectID": "posts/2022-03-02-Linear_Programming.html#i.-solvers",
    "href": "posts/2022-03-02-Linear_Programming.html#i.-solvers",
    "title": "Introduction to Linear Programming in Python",
    "section": "üß† I.¬†Solvers",
    "text": "üß† I.¬†Solvers\nIn Python, there are different libraries for linear programming such as the multi-purposed SciPy, the beginner-friendly PuLP, the exhaustive Pyomo, and many others.\nToday, we are going to use Google OR-Tools, which is quite user-friendly, comes with several prepackaged solvers, and has by far the most stars on GitHub.\nIf the installation doesn‚Äôt work, please restart the kernel and try again: it can fail sometimes. ¬Ø\\_(„ÉÑ)_/¬Ø\n\n!python -m pip install --upgrade --user -q ortools\n\nAll these libraries have a hidden benefit: they act as interfaces to use the same model with different solvers. Solvers like Gurobi, Cplex, or SCIP have their own APIs, but the models they create are tied to a specific solver.\nOR-Tools allows us to use an abstract (and quite pythonic) way of modeling our problems. We can then choose one or several solvers to find an optimal solution. The model we built is thus highly reusable!\n\n\n\n\nOR-Tools comes with its own linear programming solver, called GLOP (Google Linear Optimization Package). It is an open-source project created by Google‚Äôs Operations Research Team and written in C++.\nOther solvers are available such as SCIP, an excellent non-commercial solver created in 2005 and updated and maintained to this day. We could also use popular commercial options like Gurobi and Cplex. However, we would need to install them on top of OR-Tools and get the appropriate licenses (which can be quite costly). For now, let‚Äôs try GLOP.\n\n# Import OR-Tools wrapper for linear programming\nfrom ortools.linear_solver import pywraplp\n\n# Create a solver using the GLOP backend\nsolver = pywraplp.Solver('Maximize army power', pywraplp.Solver.GLOP_LINEAR_PROGRAMMING)"
  },
  {
    "objectID": "posts/2022-03-02-Linear_Programming.html#ii.-variables",
    "href": "posts/2022-03-02-Linear_Programming.html#ii.-variables",
    "title": "Introduction to Linear Programming in Python",
    "section": "üßÆ II. Variables",
    "text": "üßÆ II. Variables\nWe created an instance of the OR-Tools solver using GLOP. Now, how to use linear programming? The first thing we want to define is the variables we want to optimize.\nIn our example, we have three variables: the number of üó°Ô∏èswordsmen, üèπbowmen, and üêéhorsemen in the army. OR-Tools accepts three types of variables:\n\nNumVar for continuous variables;\nIntVar for integer variables;\nBoolVar for boolean variables.\n\nWe‚Äôre looking for round numbers of units, so let‚Äôs choose IntVar. We then need to specify lower and upper bounds for these variables. We want at least 0 unit, but we don‚Äôt really have an upper bound. So we can say that our upper bound is infinity (or any big number we will never reach). It can be written as:\n0 \\leq swordsmen &lt; \\infty \\\\\n0 \\leq bowmen &lt; \\infty \\\\\n0 \\leq horsemen &lt; \\infty\nLet‚Äôs translate it into code. Infinity is replaced by solver.infinity() in OR-Tools. Other than that, the syntax is quite straightforward:\n\n# Create the variables we want to optimize\nswordsmen = solver.IntVar(0, solver.infinity(), 'swordsmen')\nbowmen = solver.IntVar(0, solver.infinity(), 'bowmen')\nhorsemen = solver.IntVar(0, solver.infinity(), 'horsemen')"
  },
  {
    "objectID": "posts/2022-03-02-Linear_Programming.html#iii.-constraints",
    "href": "posts/2022-03-02-Linear_Programming.html#iii.-constraints",
    "title": "Introduction to Linear Programming in Python",
    "section": "‚õìÔ∏è III. Constraints",
    "text": "‚õìÔ∏è III. Constraints\nWe defined our variables, but the constraints are just as important.\nPerhaps counter-intuitively, adding more constraints helps the solver to find an optimal solution faster. Why is this the case? Think of the solver as a tree: constraints help it trim branches and reduce the search space.\nIn our case, we have a limited number of resources we can use to produce units. In other words, we can‚Äôt spend more resources than we have. For instance, the üåæfood spent to recruit units cannot be higher than 1200. The same is true with ü™µwood (800) and ü™ôgold (600).\nAccording to our table, units have the following costs:\n\n1 swordsman = üåæ60 + ü™µ20;\n1 bowman = üåæ80 + ü™µ10 + ü™ô40;\n1 horseman = üåæ140 + ü™ô100.\n\nWe can write one constraint per resource as follows:\n60\\times swordsmen + 80\\times bowmen + 140\\times horsemen \\leq 1200 \\\\\n20\\times swordsmen + 10\\times bowmen \\leq 800 \\\\\n40\\times bowmen + 100\\times horsemen \\leq 600\nIn OR-Tools, we simply add the constraints to our solver instance with solver.Add().\n\n# Add constraints for each resource\nsolver.Add(swordsmen*60 + bowmen*80 + horsemen*140 &lt;= 1200) # Food\nsolver.Add(swordsmen*20 + bowmen*10 &lt;= 800)                 # Wood\nsolver.Add(bowmen*40 + horsemen*100 &lt;= 600)                 # Gold"
  },
  {
    "objectID": "posts/2022-03-02-Linear_Programming.html#iv.-objective",
    "href": "posts/2022-03-02-Linear_Programming.html#iv.-objective",
    "title": "Introduction to Linear Programming in Python",
    "section": "üéØ IV. Objective",
    "text": "üéØ IV. Objective\nNow that we have our variables and constraints, we want to define our goal (or objective function).\nIn linear programming, this function has to be linear (like the constraints), so of the form ax + by + cz + d. In our example, the objective is quite clear: we want to recruit the army with the highest power. The table gives us the following power values:\n\n1 swordsman = üí™70;\n1 bowman = üí™95;\n1 horseman = üí™230.\n\nMaximizing the power of the army amounts to maximizing the sum of the power of each unit. Our objective function can be written as:\n\\max\\ 70\\times swordsmen + 95\\times bowmen + 230\\times horsemen\nIn general, there are two types of objective functions: maximizing and minimizing. In OR-Tools, we declare this goal with solver.Maximize() or solver.Minimize().\n\n# Maximize the objective function\nsolver.Maximize(swordsmen*70 + bowmen*95 + horsemen*230)\n\nAnd we‚Äôre done! There are three steps to model any linear optimization problem:\n\nDeclaring the variables to optimize with lower and upper bounds;\nAdding constraints to these variables;\nDefining the objective function to maximize or to minimize.\n\nNow that is clear, we can ask the solver to find an optimal solution for us."
  },
  {
    "objectID": "posts/2022-03-02-Linear_Programming.html#v.-optimize",
    "href": "posts/2022-03-02-Linear_Programming.html#v.-optimize",
    "title": "Introduction to Linear Programming in Python",
    "section": "ü•á V. Optimize!",
    "text": "ü•á V. Optimize!\nCalculating the optimal solution is done with solver.Solve(). This function returns a status that can be used to check that the solution is indeed optimal.\nLet‚Äôs print the highest total power we can get with the best army configuration.\n\n# Solve problem\nstatus = solver.Solve()\n\n# If an optimal solution has been found, print results\nif status == pywraplp.Solver.OPTIMAL:\n  print('================= Solution =================')\n  print(f'Solved in {solver.wall_time():.2f} milliseconds in {solver.iterations()} iterations')\n  print()\n  print(f'Optimal power = {solver.Objective().Value()} üí™power')\n  print('Army:')\n  print(f' - üó°Ô∏èSwordsmen = {swordsmen.solution_value()}')\n  print(f' - üèπBowmen = {bowmen.solution_value()}')\n  print(f' - üêéHorsemen = {horsemen.solution_value()}')\nelse:\n  print('The solver could not find an optimal solution.')\n\n================= Solution =================\nSolved in 87.00 milliseconds in 2 iterations\n\nOptimal power = 1800.0 üí™power\nArmy:\n - üó°Ô∏èSwordsmen = 6.0000000000000036\n - üèπBowmen = 0.0\n - üêéHorsemen = 5.999999999999999\n\n\nGreat! The solver found an optimal solution: our army has a total power of üí™1800 with 6 üó°Ô∏èswordsmen and 6 üêéhorsemen (sorry bowmen!).\nLet‚Äôs unpack this result:\n\nThe solver decided to take the maximum number of üêéhorsemen (6, since we only have ü™ô600 and they each cost ü™ô100);\nThe remaining resources are spent in üó°Ô∏èswordsmen: we have 1200 ‚Äì 6*140 = 360üåæfood left, which is why the solver chose 6 üó°Ô∏èswordsmen;\nWe can deduce that the horsemen are the best unit and the bowmen are the worst one because they haven‚Äôt been chosen at all.\n\nOkay, but there‚Äôs something quite weird: these numbers are not round, even though we specified that we wanted integers (IntVar). So what happened?\nUnfortunately, answering this question requires a deep dive into linear programming‚Ä¶ To keep things simple in this introduction, let‚Äôs say it‚Äôs because of GLOP. Solvers have characteristics we have to take into account, and GLOP doesn‚Äôt handle integers. This is another proof that building reusable models is more than just convenient.\nWe‚Äôll explain why GLOP has this strange behavior and how to fix it in a more advanced tutorial."
  },
  {
    "objectID": "posts/2022-03-02-Linear_Programming.html#conclusion",
    "href": "posts/2022-03-02-Linear_Programming.html#conclusion",
    "title": "Introduction to Linear Programming in Python",
    "section": "Conclusion",
    "text": "Conclusion\nWe saw through this example the five main steps of any linear optimization problem:\n\nChoosing a solver: in our case, we selected GLOP for convenience.\nDeclaring variables: the parameters to optimize were the number of swordsmen, bowmen, and horsemen.\nDeclaring constraints: each of these units has a cost. The total cost could not exceed our limited resources.\nDefining objective: the criterion to maximize was the total power of this army. It could have been something else, like the number of units.\nOptimizing: GLOP found an optimal solution to this problem in less than a second.\n\n\n\n\nThis is the main benefit of linear programming: the algorithm gives us a guarantee that the solution that was found is optimal (with a certain error). This guarantee is powerful, but comes at a cost: the model can be so complex that the solver takes years (or more) to find an optimal solution. In this scenario, we have two options:\n\nWe can stop the solver after a certain time (and probably obtain a suboptimal answer);\nWe can use a metaheuristic like a genetic algorithm to calculate an excellent solution in a short amount of time.\n\nIn the next article, we‚Äôll talk about the different types of optimization problems and generalize our approach to an entire class of them.\nI hope you enjoyed this introduction! Feel free to share it and spread the knowledge about linear optimization. Let‚Äôs connect on Twitter where I post summaries of these articles. Cheers!"
  },
  {
    "objectID": "posts/2022-03-02-Linear_Programming.html#linear-programming-course",
    "href": "posts/2022-03-02-Linear_Programming.html#linear-programming-course",
    "title": "Introduction to Linear Programming in Python",
    "section": "ü•á Linear Programming Course",
    "text": "ü•á Linear Programming Course\nüîé Course overview\nüìù Chapter 1: Introduction to Linear Programming\nüìù Chapter 2: Integer vs.¬†Linear Programming\nüìù Chapter 3: Constraint Programming\nüìù Chapter 4: Nonlinear Programming for Marketing Budget Allocation"
  },
  {
    "objectID": "posts/2022-03-05-Integer_Programming.html",
    "href": "posts/2022-03-05-Integer_Programming.html",
    "title": "Integer vs.¬†Linear Programming in Python",
    "section": "",
    "text": "Why is linear programming called that way?\nBoth terms are confusing:\nIn summary, it has nothing to do with code: linear or not. It‚Äôs about optimizing variables with various constraints.\nIn this article, we‚Äôre gonna talk about another type of optimization: integer programming. We‚Äôll see why a good understanding of the problem we face is necessary to choose the right solver. Finally, we will write a model that can take on a bigger challenge and actually solve a whole class of optimization problems.\nYou can run the code from this tutorial with the following Google Colab notebook."
  },
  {
    "objectID": "posts/2022-03-05-Integer_Programming.html#i.-optimization-problem-types",
    "href": "posts/2022-03-05-Integer_Programming.html#i.-optimization-problem-types",
    "title": "Integer vs.¬†Linear Programming in Python",
    "section": "üìä I. Optimization problem¬†types",
    "text": "üìä I. Optimization problem¬†types\nIn the introduction to linear programming, we optimized an army composition. Here was the result:\n\n\n================= Solution =================\nSolved in 87.00 milliseconds in 2 iterations\n\nOptimal power = 1800.0 üí™power\nArmy:\n - üó°Ô∏èSwordsmen = 6.0000000000000036\n - üèπBowmen = 0.0\n - üêéHorsemen = 5.999999999999999\n\n\nHow can we have 5.999‚Ä¶ horsemen? We specified that our variables should be integers with VarInt. What was wrong with our code?\nThe problem is not the model but the choice of the solver.\nGLOP is a pure linear programming solver. This means that it cannot understand the concept of integers. It is limited to continuous parameters with a linear relationship.\nThis is the difference between linear programming (LP) and integer linear programming (ILP). In summary, LP solvers can only use real numbers and not integers as variables. So why did we declare our variables as integers if it doesn‚Äôt take them into account?\nGLOP cannot solve ILP problems, but other solvers can. Actually, a lot of them are mixed integer linear programming (MILP, commonly called MIP) solvers. This means that they can consider both continuous (real numbers) and discrete (integers) variables. A particular case of discrete values is Boolean variables to represent decisions with 0‚Äì1 values.\nOther solvers like SCIP or CBC can solve both MILP and MINLP (mixed integer nonlinear programming) problems. Thanks to OR-Tools, we can use the same model and just change the solver to SCIP or CBC.\n\n# Create the linear solver using the CBC backend\nsolver = pywraplp.Solver('Maximize army power', pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING)\n\n# 1. Create the variables we want to optimize\nswordsmen = solver.IntVar(0, solver.infinity(), 'swordsmen')\nbowmen = solver.IntVar(0, solver.infinity(), 'bowmen')\nhorsemen = solver.IntVar(0, solver.infinity(), 'horsemen')\n\n# 2. Add constraints for each resource\nsolver.Add(swordsmen*60 + bowmen*80 + horsemen*140 &lt;= 1200)\nsolver.Add(swordsmen*20 + bowmen*10 &lt;= 800)\nsolver.Add(bowmen*40 + horsemen*100 &lt;= 600)\n\n# 3. Maximize the objective function\nsolver.Maximize(swordsmen*70 + bowmen*95 + horsemen*230)\n\n# Solve problem\nstatus = solver.Solve()\n\n# If an optimal solution has been found, print results\nif status == pywraplp.Solver.OPTIMAL:\n  print('================= Solution =================')\n  print(f'Solved in {solver.wall_time():.2f} milliseconds in {solver.iterations()} iterations')\n  print()\n  print(f'Optimal value = {solver.Objective().Value()} üí™power')\n  print('Army:')\n  print(f' - üó°Ô∏èSwordsmen = {swordsmen.solution_value()}')\n  print(f' - üèπBowmen = {bowmen.solution_value()}')\n  print(f' - üêéHorsemen = {horsemen.solution_value()}')\nelse:\n  print('The solver could not find an optimal solution.')\n\n================= Solution =================\nSolved in 3.00 milliseconds in 0 iterations\n\nOptimal value = 1800.0 üí™power\nArmy:\n - üó°Ô∏èSwordsmen = 6.0\n - üèπBowmen = 0.0\n - üêéHorsemen = 6.0\n\n\nStrictly speaking, our variables are still floats (type(swordsmen.solution_value()) = float) but we can see that they don‚Äôt have weird decimals anymore: the CBC solver really considered them as integers.\nIn this example, we would generally just round up these values since the error is insignificant. However, it is important to remember to choose the appropriate solver according to the studied problem:\n\nLP for continuous variables;\nMIP/MILP for a combination of continuous and discrete variables.\n\nThere are other types such as quadratic (QP) or nonlinear (NLP or MINLP, with an exponential objective function or constraints for instance) problems. They‚Äôre applied in different contexts, but follow the same principles as LP or MIP solvers."
  },
  {
    "objectID": "posts/2022-03-05-Integer_Programming.html#ii.-building-a-general-model",
    "href": "posts/2022-03-05-Integer_Programming.html#ii.-building-a-general-model",
    "title": "Integer vs.¬†Linear Programming in Python",
    "section": "üß± II. Building a general model",
    "text": "üß± II. Building a general model\nBut what if our resources change? Or if the cost of a unit evolved? What if we upgraded horsemen and their power increased?\nOne of the best perks of OR-Tools is that it uses a general-purpose programming language like Python. Instead of static numbers, we can store our parameters in objects like dictionaries or lists.\nThe code won‚Äôt be as readable, but it becomes much more flexible: actually, it can be so flexible that we can solve an entire class of optimization problems without changing the model (just the parameters).\nLet‚Äôs transform our input parameters into Python lists and feed them to the solver through a function.\n\n# Inputs\nUNITS = ['üó°Ô∏èSwordsmen', 'üèπBowmen', 'üêéHorsemen']\n\nDATA = [[60, 20, 0, 70],\n        [80, 10, 40, 95],\n        [140, 0, 100, 230]]\n\nRESOURCES = [1200, 800, 600]\n\n\ndef solve_army(UNITS, DATA, RESOURCES):\n  # Create the linear solver using the CBC backend\n  solver = pywraplp.Solver('Maximize army power', pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING)\n\n  # 1. Create the variables we want to optimize\n  units = [solver.IntVar(0, solver.infinity(), unit) for unit in UNITS]\n\n  # 2. Add constraints for each resource\n  for r, _ in enumerate(RESOURCES):\n    solver.Add(sum(DATA[u][r] * units[u] for u, _ in enumerate(units)) &lt;= RESOURCES[r])\n\n  # 3. Maximize the objective function\n  solver.Maximize(sum(DATA[u][-1] * units[u] for u, _ in enumerate(units)))\n\n  # Solve problem\n  status = solver.Solve()\n\n  # If an optimal solution has been found, print results\n  if status == pywraplp.Solver.OPTIMAL:\n    print('================= Solution =================')\n    print(f'Solved in {solver.wall_time():.2f} milliseconds in {solver.iterations()} iterations')\n    print()\n    print(f'Optimal value = {solver.Objective().Value()} üí™power')\n    print('Army:')\n    for u, _ in enumerate(units):\n      print(f' - {units[u].name()} = {units[u].solution_value()}')\n  else:\n    print('The solver could not find an optimal solution.')\n\nsolve_army(UNITS, DATA, RESOURCES)\n\n================= Solution =================\nSolved in 2.00 milliseconds in 0 iterations\n\nOptimal value = 1800.0 üí™power\nArmy:\n - üó°Ô∏èSwordsmen = 6.0\n - üèπBowmen = 0.0\n - üêéHorsemen = 6.0\n\n\nWe obtain the same results: our code seems to work. Now let‚Äôs change the parameters to tackle a slightly more complex problem.\nImagine we have a lot more resources: üåæ183000, ü™µ90512, and ü™ô80150, so we can also produce a lot more units! This is the new table:\n\n\n\nUnit\nüåæFood\nü™µWood\nü™ôGold\nüí™Attack\n‚ù§Ô∏èHealth\n\n\n\n\nüó°Ô∏èSwordsman\n60\n20\n0\n6\n70\n\n\nüõ°Ô∏èMan-at-arms\n100\n0\n20\n12\n155\n\n\nüèπBowman\n30\n50\n0\n5\n70\n\n\n‚ùåCrossbowman\n80\n0\n40\n12\n80\n\n\nüî´Handcannoneer\n120\n0\n120\n35\n150\n\n\nüêéHorseman\n100\n20\n0\n9\n125\n\n\n‚ôûKnight\n140\n0\n100\n24\n230\n\n\nüêèBattering ram\n0\n300\n0\n200\n700\n\n\nüéØSpringald\n0\n250\n250\n30\n200\n\n\n\nNotice that we transformed the üí™power into two values: üí™attack and ‚ù§Ô∏èhealth, which is a little more detailed. Health values are higher than attack values, which is why we want to add a weighting factor to make them more comparable.\nLet‚Äôs take 10 as an example, so power = 10 \\times attack + health. Our objective function becomes:\n\\text{maximize} \\ \\sum_{u \\in units} (10\\times attack + health) \\cdot u\nAdapting our code to this new problem is actually quite simple: we just have to change the input parameters and update the objective function.\n\nUNITS = [\n    'üó°Ô∏èSwordsmen',\n    'üõ°Ô∏èMen-at-arms',\n    'üèπBowmen',\n    '‚ùåCrossbowmen',\n    'üî´Handcannoneers',\n    'üêéHorsemen',\n    '‚ôûKnights',\n    'üêèBattering rams',\n    'üéØSpringalds',\n    'ü™®Mangonels',\n]\n\nDATA = [\n    [60, 20, 0, 6, 70],\n    [100, 0, 20, 12, 155],\n    [30, 50, 0, 5, 70],\n    [80, 0, 40, 12, 80],\n    [120, 0, 120, 35, 150],\n    [100, 20, 0, 9, 125],\n    [140, 0, 100, 24, 230],\n    [0, 300, 0, 200, 700],\n    [0, 250, 250, 30, 200],\n    [0, 400, 200, 12*3, 240]\n]\n\nRESOURCES = [183000, 90512, 80150]\n\n\ndef solve_army(UNITS, DATA, RESOURCES):\n  # Create the linear solver using the CBC backend\n  solver = pywraplp.Solver('Maximize army power', pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING)\n\n  # 1. Create the variables we want to optimize\n  units = [solver.IntVar(0, solver.infinity(), unit) for unit in UNITS]\n\n  # 2. Add constraints for each resource\n  for r, _ in enumerate(RESOURCES):\n    solver.Add(sum(DATA[u][r] * units[u] for u, _ in enumerate(units)) &lt;= RESOURCES[r])\n\n  # 3. Maximize the new objective function\n  solver.Maximize(sum((10*DATA[u][-2] + DATA[u][-1]) * units[u] for u, _ in enumerate(units)))\n\n  # Solve problem\n  status = solver.Solve()\n\n  # If an optimal solution has been found, print results\n  if status == pywraplp.Solver.OPTIMAL:\n    print('================= Solution =================')\n    print(f'Solved in {solver.wall_time():.2f} milliseconds in {solver.iterations()} iterations')\n    print()\n    print(f'Optimal value = {solver.Objective().Value()} üí™power')\n    print('Army:')\n    for u, _ in enumerate(units):\n      print(f' - {units[u].name()} = {units[u].solution_value()}')\n  else:\n    print('The solver could not find an optimal solution.')\n\nsolve_army(UNITS, DATA, RESOURCES)\n\n================= Solution =================\nSolved in 74.00 milliseconds in 412 iterations\n\nOptimal value = 1393145.0 üí™power\nArmy:\n - üó°Ô∏èSwordsmen = 2.0\n - üõ°Ô∏èMen-at-arms = 1283.0\n - üèπBowmen = 3.0\n - ‚ùåCrossbowmen = 0.0\n - üî´Handcannoneers = 454.0\n - üêéHorsemen = 0.0\n - ‚ôûKnights = 0.0\n - üêèBattering rams = 301.0\n - üéØSpringalds = 0.0\n - ü™®Mangonels = 0.0\n\n\nThis problem would take a long time for humans to address, but the ILP solver did it in the blink of an eye. Better than that: it also gives us the guarantee that our solution is optimal, which means that our enemy cannot find a better army composition for the same cost!\nWe could increase the number of units and give billions of resources but you get the picture: it would just take longer to obtain a solution, but it wouldn‚Äôt change the problem."
  },
  {
    "objectID": "posts/2022-03-05-Integer_Programming.html#iii.-combining-constraints",
    "href": "posts/2022-03-05-Integer_Programming.html#iii.-combining-constraints",
    "title": "Integer vs.¬†Linear Programming in Python",
    "section": "‚öîÔ∏è III. Combining constraints",
    "text": "‚öîÔ∏è III. Combining constraints\nNow, let‚Äôs say we scouted our enemy and know that their army has a üí™power of 1,000,000. We could build a much better army, but our resources are precious and it wouldn‚Äôt be very efficient: all we have to do is to build an army with a üí™power higher than 1,000,000 (even 1,000,001 would be enough).\nIn other words, the total power is now a constraint (üí™ &gt; 1,000,000) instead of the objective to maximize. The new goal is to minimize the resources we need to produce this army. However, we can reuse our input parameters since they didn‚Äôt change.\nThe new constraint can be translated as ‚Äúthe sum of the power of the selected units must be strictly greater than 1,000,000‚Äù.\n\\sum_{u \\in units} (10\\times attack + health) \\cdot u &gt; 1\\,000\\,000\nIn code, we can loop through our units and resources to design this constraint.\nThe objective function also has to change. Our goal is to minimize the sum of resources spent to build the army.\n\\text{minimize} \\ \\sum_{u \\in units} (food + wood + gold) \\cdot u\nOnce again, we can loop through our resources to implement it in OR-Tools.\n\ndef solve_army(UNITS, DATA, RESOURCES):\n  # Create the linear solver using the CBC backend\n  solver = pywraplp.Solver('Minimize resource consumption', pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING)\n\n  # 1. Create the variables we want to optimize\n  units = [solver.IntVar(0, solver.infinity(), unit) for unit in UNITS]\n\n  # 2. Add power constraint\n  solver.Add(sum((10 * DATA[u][-2] + DATA[u][-1]) * units[u] for u, _ in enumerate(units)) &gt;= 1000001)\n\n  # 3. Minimize the objective function\n  solver.Minimize(sum((DATA[u][0] + DATA[u][1] + DATA[u][2]) * units[u] for u, _ in enumerate(units)))\n\n  # Solve problem\n  status = solver.Solve()\n\n  # If an optimal solution has been found, print results\n  if status == pywraplp.Solver.OPTIMAL:\n    print('================= Solution =================')\n    print(f'Solved in {solver.wall_time():.2f} milliseconds in {solver.iterations()} iterations')\n    print()\n\n    power = sum((10 * DATA[u][-2] + DATA[u][-1]) * units[u].solution_value() for u, _ in enumerate(units))\n    print(f'Optimal value = {solver.Objective().Value()} üåæü™µü™ôresources')\n    print(f'Power = üí™{power}')\n    print('Army:')\n    for u, _ in enumerate(units):\n      print(f' - {units[u].name()} = {units[u].solution_value()}')\n    print()\n\n    food = sum((DATA[u][0]) * units[u].solution_value() for u, _ in enumerate(units))\n    wood = sum((DATA[u][1]) * units[u].solution_value() for u, _ in enumerate(units))\n    gold = sum((DATA[u][2]) * units[u].solution_value() for u, _ in enumerate(units))\n    print('Resources:')\n    print(f' - üåæFood = {food}')\n    print(f' - ü™µWood = {wood}')\n    print(f' - ü™ôGold = {gold}')\n  else:\n      print('The solver could not find an optimal solution.')\n\nsolve_army(UNITS, DATA, RESOURCES)\n\n================= Solution =================\nSolved in 4.00 milliseconds in 0 iterations\n\nOptimal value = 111300.0 üåæü™µü™ôresources\nPower = üí™1001700.0\nArmy:\n - üó°Ô∏èSwordsmen = 0.0\n - üõ°Ô∏èMen-at-arms = 0.0\n - üèπBowmen = 0.0\n - ‚ùåCrossbowmen = 0.0\n - üî´Handcannoneers = 0.0\n - üêéHorsemen = 0.0\n - ‚ôûKnights = 0.0\n - üêèBattering rams = 371.0\n - üéØSpringalds = 0.0\n - ü™®Mangonels = 0.0\n\nResources:\n - üåæFood = 0.0\n - ü™µWood = 111300.0\n - ü™ôGold = 0.0\n\n\nThe solver found an optimal solution: we need to build 371 üêèbattering rams for a total cost of 111,300 ü™µwood. Wait, what if we don‚Äôt have that much wood? In the previous section, we only had ü™µ90512: we cannot produce 371 üêèbattering rams. üò±\nSo is it possible to take these limited resources into account and still try to build the best army? Actually, it‚Äôs super easy: we just have to copy/paste the constraints from the previous section.\nIn this version, we have two types of constraints:\n\nThe total power must be greater than 1,000,000;\nWe cannot spend more than our limited resources.\n\n\ndef solve_army(UNITS, DATA, RESOURCES):\n  # Create the linear solver using the CBC backend\n  solver = pywraplp.Solver('Minimize resource consumption', pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING)\n\n  # 1. Create the variables we want to optimize\n  units = [solver.IntVar(0, solver.infinity(), unit) for unit in UNITS]\n\n  # 2. Add constraints for each resource\n  for r, _ in enumerate(RESOURCES):\n    solver.Add(sum((10 * DATA[u][-2] + DATA[u][-1]) * units[u] for u, _ in enumerate(units)) &gt;= 1000001)\n\n  # Old constraints for limited resources\n  for r, _ in enumerate(RESOURCES):\n    solver.Add(sum(DATA[u][r] * units[u] for u, _ in enumerate(units)) &lt;= RESOURCES[r])\n\n  # 3. Minimize the objective function\n  solver.Minimize(sum((DATA[u][0] + DATA[u][1] + DATA[u][2]) * units[u] for u, _ in enumerate(units)))\n\n  # Solve problem\n  status = solver.Solve()\n\n  # If an optimal solution has been found, print results\n  if status == pywraplp.Solver.OPTIMAL:\n    print('================= Solution =================')\n    print(f'Solved in {solver.wall_time():.2f} milliseconds in {solver.iterations()} iterations')\n    print()\n\n    power = sum((10 * DATA[u][-2] + DATA[u][-1]) * units[u].solution_value() for u, _ in enumerate(units))\n    print(f'Optimal value = {solver.Objective().Value()} üåæü™µü™ôresources')\n    print(f'Power = üí™{power}')\n    print('Army:')\n    for u, _ in enumerate(units):\n      print(f' - {units[u].name()} = {units[u].solution_value()}')\n    print()\n    \n    food = sum((DATA[u][0]) * units[u].solution_value() for u, _ in enumerate(units))\n    wood = sum((DATA[u][1]) * units[u].solution_value() for u, _ in enumerate(units))\n    gold = sum((DATA[u][2]) * units[u].solution_value() for u, _ in enumerate(units))\n    print('Resources:')\n    print(f' - üåæFood = {food}')\n    print(f' - ü™µWood = {wood}')\n    print(f' - ü™ôGold = {gold}')\n  else:\n      print('The solver could not find an optimal solution.')\n\nsolve_army(UNITS, DATA, RESOURCES)\n\n================= Solution =================\nSolved in 28.00 milliseconds in 1 iterations\n\nOptimal value = 172100.0 üåæü™µü™ôresources\nPower = üí™1000105.0\nArmy:\n - üó°Ô∏èSwordsmen = 1.0\n - üõ°Ô∏èMen-at-arms = 681.0\n - üèπBowmen = 0.0\n - ‚ùåCrossbowmen = 0.0\n - üî´Handcannoneers = 0.0\n - üêéHorsemen = 0.0\n - ‚ôûKnights = 0.0\n - üêèBattering rams = 301.0\n - üéØSpringalds = 0.0\n - ü™®Mangonels = 0.0\n\nResources:\n - üåæFood = 68160.0\n - ü™µWood = 90320.0\n - ü™ôGold = 13620.0\n\n\nSince we now have a limited resource of ü™µwood, the number of üêèbattering rams sadly dropped from 371 to 301. In exchange, we got 681 üõ°Ô∏èmen-at-arms and 1 lost üó°Ô∏èswordsman (welcome to them).\nThe total cost of the army is 172,100, which is much higher than the 111,300 we previously found (+65% increase) but it truly is the optimal solution under these constraints. It shows that we should produce more wood because these üêè battering rams are extremely cost-efficient!\nThis example shows how modular LP models can be. It is possible to reuse parts of the code, like constraints, in another model to combine them and solve more complex problems."
  },
  {
    "objectID": "posts/2022-03-05-Integer_Programming.html#iv.-linear-programming-vs-machine-learning",
    "href": "posts/2022-03-05-Integer_Programming.html#iv.-linear-programming-vs-machine-learning",
    "title": "Integer vs.¬†Linear Programming in Python",
    "section": "üß† IV. Linear Programming vs Machine¬†Learning",
    "text": "üß† IV. Linear Programming vs Machine¬†Learning\nLet‚Äôs talk about the elephant in the room. Why not use machine learning (in a broad sense) instead of linear programming? It‚Äôs not like this problem cannot be solved with a genetic algorithm for instance.\nMathematical optimization is often neglected in favor of machine learning techniques, but both have their merits:\n\nLinear programming can produce an optimal solution in an undetermined amount of time (it can take years), while machine learning can approximate complex functions in no time.\nThere is no training in LP, but an expert is required to build a mathematical model. Machine learning needs data, but the models can be used as black boxes to solve a problem.\nAs a rule of thumb, problems that do not have a particular time constraint and/or are not extremely complex can be advantageously solved with linear programming."
  },
  {
    "objectID": "posts/2022-03-05-Integer_Programming.html#conclusion",
    "href": "posts/2022-03-05-Integer_Programming.html#conclusion",
    "title": "Integer vs.¬†Linear Programming in Python",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, we dived deeper into our understanding of mathematical optimization.\n\nWe talked about solvers and types of optimization problems: LP, MIP, NLP;\nWe modeled and solved an extremely common optimization problem in an optimal way and generalized our model through a function;\nWe reframed this problem and merged two sets of constraints to obtain the best army composition for the lowest price;\nWe compared the pros and cons of linear programming and machine learning.\n\nThere are a lot more problems where optimization can be applied. For instance, how to create school timetables that satisfy everybody‚Äôs requirements? How to deliver 1,000 different orders in a minimum amount of time? Where to create a new metro line to maximize its usefulness?\nIn future articles, we‚Äôll talk about new types of applications for these techniques, including satisfiability and nonlinear problems.\nI hope you enjoyed this more advanced article. If you like machine learning and optimization, let‚Äôs connect on Twitter!"
  },
  {
    "objectID": "posts/2022-03-05-Integer_Programming.html#linear-programming-course",
    "href": "posts/2022-03-05-Integer_Programming.html#linear-programming-course",
    "title": "Integer vs.¬†Linear Programming in Python",
    "section": "ü•á Linear Programming Course",
    "text": "ü•á Linear Programming Course\nüîé Course overview\nüìù Chapter 1: Introduction to Linear Programming\nüìù Chapter 2: Integer vs.¬†Linear Programming\nüìù Chapter 3: Constraint Programming\nüìù Chapter 4: Nonlinear Programming for Marketing Budget Allocation"
  },
  {
    "objectID": "posts/2022-03-09-Graph_Attention_Network.html",
    "href": "posts/2022-03-09-Graph_Attention_Network.html",
    "title": "Graph Attention Networks: Self-Attention for GNNs",
    "section": "",
    "text": "Graph Attention Networks (GATs) are one of the most popular types of Graph Neural Networks.\nInstead of calculating static weights based on node degrees like Graph Convolutional Networks (GCNs), they assign dynamic weights to node features through a process called self-attention. The main idea behind GATs is that some neighbors are more important than others, regardless of their node degrees.\nIn this article, we will see how to calculate these attention scores and implement an efficient GAT in PyTorch Geometric (PyG). You can run the code of this tutorial with the following Google Colab notebook.\n# Install PyTorch Geometric\nimport torch\n!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n\n# Numpy for matrices\nimport numpy as np\nnp.random.seed(0)\n\n# Visualization\nimport networkx as nx\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.dpi'] = 300\nplt.rcParams.update({'font.size': 24})"
  },
  {
    "objectID": "posts/2022-03-09-Graph_Attention_Network.html#i.-graph-data",
    "href": "posts/2022-03-09-Graph_Attention_Network.html#i.-graph-data",
    "title": "Graph Attention Networks: Self-Attention for GNNs",
    "section": "üåê I. Graph data",
    "text": "üåê I. Graph data\n\n\nCiteSeer dataset\n\n\nLet‚Äôs perform a node classification task with a GAT. We can use three classic graph datasets (MIT license) for this work. They represent networks of research papers, where each connection is a citation.\n\nCora: it consists of 2,708 machine learning papers that belong to one of seven categories. ‚û°Ô∏è Node features represent the presence (1) or absence (0) of 1,433 words in a paper (binary bag of words).\nCiteSeer: it is a bigger but similar dataset of 3,327 scientific papers to classify into one of six categories. ‚û°Ô∏è Node features represent the presence (1) or absence (0) of 3,703 words in a paper.\nPubMed: it is an even bigger dataset with 19,717 scientific publications about diabetes from PubMed‚Äôs database, classified into three categories. ‚û°Ô∏è Node features are TF-IDF weighted word vector from a dictionary of 500 unique words.\n\nThese datasets have been widely used by the scientific community. As a challenge, we can compare our accuracy scores to those obtained in the literature (with standard deviation) using Multilayer Perceptrons (MLPs), GCNs, and GATs:\n\n\n\nDataset\nüìùCora\nüìùCiteSeer\nüìùPubMed\n\n\n\n\nMLP\n55.1%\n46.5%\n71.4%\n\n\nGCN\n81.4 ¬± 0.5%\n70.9% ¬± 0.5%\n79.0% ¬± 0.3%\n\n\nGAT\n83.0% ¬± 0.7%\n72.5% ¬± 0.7%\n79.0% ¬± 0.3%\n\n\n\nPubMed is quite large, so it would take longer to process and train a GNN on it. On the other hand, Cora is the most studied one in the literature, so let‚Äôs focus on CiteSeer as a middle ground.\nWe can directly import any of these datasets in PyTorch Geometric with the Planetoid class:\n\nfrom torch_geometric.datasets import Planetoid\n\n# Import dataset from PyTorch Geometric\ndataset = Planetoid(root=\".\", name=\"CiteSeer\")\ndata = dataset[0]\n\n# Print information about the dataset\nprint(f'Number of graphs: {len(dataset)}')\nprint(f'Number of nodes: {data.x.shape[0]}')\nprint(f'Number of features: {dataset.num_features}')\nprint(f'Number of classes: {dataset.num_classes}')\nprint(f'Has isolated nodes: {data.has_isolated_nodes()}')\n\nNumber of graphs: 1\nNumber of nodes: 3327\nNumber of features: 3703\nNumber of classes: 6\nHas isolated nodes: True\n\n\nThe CiteSeer dataset correctly exhibits the characteristics we previously described. However, some nodes are isolated (48 to be precise)! Correctly classifying these isolated nodes will be a challenge since we cannot rely on any aggregation. This is how an MLP processes nodes: it cannot consider the adjacency matrix, which decreases its accuracy.\nLet‚Äôs plot the number of connections of each node with degree:\n\nfrom torch_geometric.utils import degree\nfrom collections import Counter\n\n# Get the list of degrees for each node\ndegrees = degree(data.edge_index[0]).numpy()\n\n# Count the number of nodes for each degree\nnumbers = Counter(degrees)\n\n# Bar plot\nfig, ax = plt.subplots(figsize=(18, 6))\nax.set_xlabel('Node degree')\nax.set_ylabel('Number of nodes')\nplt.bar(numbers.keys(),\n        numbers.values(),\n        color='#0A047A')\n\n&lt;BarContainer object of 32 artists&gt;\n\n\n\n\n\nMost nodes only have 1 or 2 neighbors. It could explain why CiteSeer obtains lower accuracy scores than the two other datasets‚Ä¶"
  },
  {
    "objectID": "posts/2022-03-09-Graph_Attention_Network.html#ii.-graph-attention-layer",
    "href": "posts/2022-03-09-Graph_Attention_Network.html#ii.-graph-attention-layer",
    "title": "Graph Attention Networks: Self-Attention for GNNs",
    "section": "‚ö†Ô∏è II. Graph Attention Layer",
    "text": "‚ö†Ô∏è II. Graph Attention Layer\nIntroduced by Veliƒçkoviƒá et al. in 2017, self-attention in GATs relies on a simple idea: some nodes are more important than others. In this context, we talk about self-attention (and not just attention) because inputs are compared to each other.\n\n\n\nIn the previous figure, self-attention calculates the importance of nodes 2, 3, and 4‚Äôs features to node 1. We denote \\alpha_{ij} the importance of node j‚Äôs features to node i.\nEach node i has an attribute vector x_i. The GAT layer calculates the embedding of node 1 as a sum of attention coefficients multiplied by a shared weight matrix \\mathbf{W} :\nh_i = \\alpha_{11}\\mathbf{W}x_1 + \\alpha_{12}\\mathbf{W}x_2 + \\alpha_{13}\\mathbf{W}x_3 + \\alpha_{14}\\mathbf{W}x_4\nBut how do we calculate these attention coefficients? We could write a static formula, but there‚Äôs a smarter solution: we can learn their values with a neural network. There are four steps in this process:\n\nLinear transformation\nActivation function\nSoftmax normalization\nMulti-head attention\n\n\n1. Linear transformation\nTo calculate the attention coefficient, we need to consider pairs of nodes. An easy way to create these pairs is to concatenate attribute vectors from both nodes.\nThen, we can apply a new linear transformation with a weight matrix W_{att}:\na_{ij} = W_{att}^t[\\mathbf{W}x_i\\mathbin\\Vert \\mathbf{W}x_j]\n\n\n\n\n\n2. Activation function\nWe‚Äôre building a neural network, so the second step is to add nonlinearity with an activation function. In this case, the paper‚Äôs authors chose the \\text{LeakyReLU} function.\ne_{ij} = \\text{LeakyReLU}(a_{ij})\n\n\n\n\n\n3. Softmax normalization\nThe output of our neural network is not normalized, which is a problem since we want to compare these coefficients. For example, to be able to say if node 2 is more important to node 1 than node 3 (\\alpha_{12} &gt; \\alpha_{13}), we need to use the same scale.\nA common way to do it with neural networks is to use the softmax function. Here, we apply it to every neighboring node, including the target node itself:\n\\alpha_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i}{exp(e_{ik})}}\n\n\n\nThis equation produces the final attention coefficients \\alpha_{ij}. The only problem is‚Ä¶ self-attention is not very stable. In order to improve performance, Vaswani et al. introduced multi-head attention in the transformer architecture.\n\n\n4. Multi-head attention\nThis should not be a big surprise if you‚Äôre familiar with the transformer architecture, but transformers are a special case of GNNs. This is why GATs look so much like a simplified version of transformers. The good thing is that we can reuse some ideas from Natural Language Processing here, like multi-head attention.\n\n\n\nIn GATs, multi-head attention consists of replicating the same three steps several times in order to average or concatenate the results. Instead of a single embedding h_1, we get one embedding per attention head (denoted h_1^k for the head k). One of the two following schemes can then be applied:\n\nAverage: we sum the different h_i^k and normalize the result by the number of attention heads n;\n\nh_i = \\frac{1}{n}\\sum_{k=1}^n{h_i^k}\n\nConcatenation: we concatenate the different h_i^k.\n\nh_i = \\mathbin\\Vert_{k=1}^n{h_i^k}\nIn practice, we use the concatenation scheme when it‚Äôs a hidden layer and the average scheme when it‚Äôs the last (output) layer. Most of the time, we will stack several GAT layers to aggregate a larger neighborhood and thus combine these two schemes in the same GAT model."
  },
  {
    "objectID": "posts/2022-03-09-Graph_Attention_Network.html#iii.-implementing-a-graph-attention-network",
    "href": "posts/2022-03-09-Graph_Attention_Network.html#iii.-implementing-a-graph-attention-network",
    "title": "Graph Attention Networks: Self-Attention for GNNs",
    "section": "üß† III. Implementing a Graph Attention Network",
    "text": "üß† III. Implementing a Graph Attention Network\nLet‚Äôs now implement a GAT in PyTorch Geometric. This library has two different graph attention layers: GATConv and GATv2Conv.\nThe layer we talked about in the previous section is the GatConv layer, but in 2021 Brody et al. introduced an improved layer by modifying the order of operations. In Gatv2Conv, the weight matrix \\mathbf{W} is applied after the concatenation and the attention weight matrix W_{att} after the \\text{LeakyReLU} function. In summary:\n\nGatConv: e_{ij} = \\text{LeakyReLU}(W_{att}^t[\\mathbf{W}x_i\\mathbin\\Vert \\mathbf{W}x_j])\nGatv2Conv: e_{ij} = W_{att}^t \\text{LeakyReLU}(\\mathbf{W}[x_i\\mathbin\\Vert x_j])\n\nWhich one should you use? According to the authors, Gatv2Conv consistently outperforms GatConv and thus should be preferred. We‚Äôll follow their advice and implement this improved layer in our example.\nOkay, let‚Äôs classify the papers from CiteSeer! I tried to (roughly) reproduce the experiments of the original authors without adding too much complexity. You can find the official implementation of GAT on GitHub.\nNote that we use graph attention layers in two configurations:\n\nThe first layer concatenates 8 outputs (multi-head attention);\nThe second layer only has 1 head, which produces our final embeddings.\n\nWe‚Äôre also going to train and test a GCN with two GCN layers (and dropout) to compare the accuracy scores.\n\nimport torch.nn.functional as F\nfrom torch.nn import Linear, Dropout\nfrom torch_geometric.nn import GCNConv, GATv2Conv\n\n\nclass GCN(torch.nn.Module):\n    \"\"\"Graph Convolutional Network\"\"\"\n    def __init__(self, dim_in, dim_h, dim_out):\n      super().__init__()\n      self.gcn1 = GCNConv(dim_in, dim_h)\n      self.gcn2 = GCNConv(dim_h, dim_out)\n      self.optimizer = torch.optim.Adam(self.parameters(),\n                                        lr=0.01,\n                                        weight_decay=5e-4)\n\n    def forward(self, x, edge_index):\n        h = F.dropout(x, p=0.5, training=self.training)\n        h = self.gcn1(h, edge_index).relu()\n        h = F.dropout(h, p=0.5, training=self.training)\n        h = self.gcn2(h, edge_index)\n        return h, F.log_softmax(h, dim=1)\n\n\nclass GAT(torch.nn.Module):\n    \"\"\"Graph Attention Network\"\"\"\n    def __init__(self, dim_in, dim_h, dim_out, heads=8):\n        super().__init__()\n        self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads)\n        self.gat2 = GATv2Conv(dim_h*heads, dim_out, heads=1)\n        self.optimizer = torch.optim.Adam(self.parameters(),\n                                          lr=0.005,\n                                          weight_decay=5e-4)\n\n    def forward(self, x, edge_index):\n        h = F.dropout(x, p=0.6, training=self.training)\n        h = self.gat1(h, edge_index)\n        h = F.elu(h)\n        h = F.dropout(h, p=0.6, training=self.training)\n        h = self.gat2(h, edge_index)\n        return h, F.log_softmax(h, dim=1)\n\ndef accuracy(pred_y, y):\n    \"\"\"Calculate accuracy.\"\"\"\n    return ((pred_y == y).sum() / len(y)).item()\n\ndef train(model, data):\n    \"\"\"Train a GNN model and return the trained model.\"\"\"\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = model.optimizer\n    epochs = 200\n\n    model.train()\n    for epoch in range(epochs+1):\n        # Training\n        optimizer.zero_grad()\n        _, out = model(data.x, data.edge_index)\n        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n        acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n        loss.backward()\n        optimizer.step()\n\n        # Validation\n        val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n        val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n\n        # Print metrics every 10 epochs\n        if(epoch % 10 == 0):\n            print(f'Epoch {epoch:&gt;3} | Train Loss: {loss:.3f} | Train Acc: '\n                  f'{acc*100:&gt;6.2f}% | Val Loss: {val_loss:.2f} | '\n                  f'Val Acc: {val_acc*100:.2f}%')\n          \n    return model\n\n@torch.no_grad()\ndef test(model, data):\n    \"\"\"Evaluate the model on test set and print the accuracy score.\"\"\"\n    model.eval()\n    _, out = model(data.x, data.edge_index)\n    acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n    return acc\n\n\n%%time\n# Create GCN model\ngcn = GCN(dataset.num_features, 16, dataset.num_classes)\nprint(gcn)\n\n# Train and test\ntrain(gcn, data)\nacc = test(gcn, data)\nprint(f'\\nGCN test accuracy: {acc*100:.2f}%\\n')\n\nGCN(\n  (gcn1): GCNConv(3703, 16)\n  (gcn2): GCNConv(16, 6)\n)\nEpoch   0 | Train Loss: 1.782 | Train Acc:  20.83% | Val Loss: 1.79 | Val Acc: 17.40%\nEpoch  10 | Train Loss: 0.580 | Train Acc:  89.17% | Val Loss: 1.31 | Val Acc: 55.40%\nEpoch  20 | Train Loss: 0.165 | Train Acc:  95.00% | Val Loss: 1.30 | Val Acc: 56.20%\nEpoch  30 | Train Loss: 0.113 | Train Acc:  97.50% | Val Loss: 1.49 | Val Acc: 54.40%\nEpoch  40 | Train Loss: 0.069 | Train Acc:  99.17% | Val Loss: 1.66 | Val Acc: 54.60%\nEpoch  50 | Train Loss: 0.037 | Train Acc: 100.00% | Val Loss: 1.65 | Val Acc: 55.60%\nEpoch  60 | Train Loss: 0.053 | Train Acc:  99.17% | Val Loss: 1.50 | Val Acc: 56.60%\nEpoch  70 | Train Loss: 0.084 | Train Acc:  97.50% | Val Loss: 1.50 | Val Acc: 58.00%\nEpoch  80 | Train Loss: 0.054 | Train Acc: 100.00% | Val Loss: 1.67 | Val Acc: 54.40%\nEpoch  90 | Train Loss: 0.048 | Train Acc:  98.33% | Val Loss: 1.54 | Val Acc: 57.80%\nEpoch 100 | Train Loss: 0.062 | Train Acc:  99.17% | Val Loss: 1.62 | Val Acc: 56.20%\nEpoch 110 | Train Loss: 0.082 | Train Acc:  96.67% | Val Loss: 1.52 | Val Acc: 56.60%\nEpoch 120 | Train Loss: 0.043 | Train Acc: 100.00% | Val Loss: 1.66 | Val Acc: 55.00%\nEpoch 130 | Train Loss: 0.058 | Train Acc:  98.33% | Val Loss: 1.55 | Val Acc: 59.80%\nEpoch 140 | Train Loss: 0.058 | Train Acc:  98.33% | Val Loss: 1.68 | Val Acc: 58.40%\nEpoch 150 | Train Loss: 0.031 | Train Acc: 100.00% | Val Loss: 1.65 | Val Acc: 58.40%\nEpoch 160 | Train Loss: 0.037 | Train Acc: 100.00% | Val Loss: 1.44 | Val Acc: 64.20%\nEpoch 170 | Train Loss: 0.025 | Train Acc: 100.00% | Val Loss: 1.58 | Val Acc: 58.40%\nEpoch 180 | Train Loss: 0.036 | Train Acc:  99.17% | Val Loss: 1.65 | Val Acc: 58.00%\nEpoch 190 | Train Loss: 0.041 | Train Acc:  97.50% | Val Loss: 1.69 | Val Acc: 57.60%\nEpoch 200 | Train Loss: 0.093 | Train Acc:  95.83% | Val Loss: 1.73 | Val Acc: 56.80%\n\nGCN test accuracy: 67.70%\n\nCPU times: user 25.1 s, sys: 847 ms, total: 25.9 s\nWall time: 32.4 s\n\n\n\n%%time\n# Create GAT model\ngat = GAT(dataset.num_features, 8, dataset.num_classes)\nprint(gat)\n\n# Train and test\ntrain(gat, data)\nacc = test(gat, data)\nprint(f'\\nGAT test accuracy: {acc*100:.2f}%\\n')\n\nGAT(\n  (gat1): GATv2Conv(3703, 8, heads=8)\n  (gat2): GATv2Conv(64, 6, heads=1)\n)\nEpoch   0 | Train Loss: 1.790 | Train Acc:  17.50% | Val Loss: 1.81 | Val Acc: 12.80%\nEpoch  10 | Train Loss: 0.114 | Train Acc:  96.67% | Val Loss: 1.05 | Val Acc: 67.20%\nEpoch  20 | Train Loss: 0.040 | Train Acc:  98.33% | Val Loss: 1.21 | Val Acc: 64.80%\nEpoch  30 | Train Loss: 0.021 | Train Acc:  99.17% | Val Loss: 1.30 | Val Acc: 65.80%\nEpoch  40 | Train Loss: 0.027 | Train Acc:  99.17% | Val Loss: 1.20 | Val Acc: 67.20%\nEpoch  50 | Train Loss: 0.012 | Train Acc:  99.17% | Val Loss: 1.18 | Val Acc: 67.20%\nEpoch  60 | Train Loss: 0.009 | Train Acc: 100.00% | Val Loss: 1.11 | Val Acc: 67.00%\nEpoch  70 | Train Loss: 0.007 | Train Acc: 100.00% | Val Loss: 1.19 | Val Acc: 64.80%\nEpoch  80 | Train Loss: 0.013 | Train Acc: 100.00% | Val Loss: 1.16 | Val Acc: 66.80%\nEpoch  90 | Train Loss: 0.009 | Train Acc: 100.00% | Val Loss: 1.10 | Val Acc: 66.60%\nEpoch 100 | Train Loss: 0.013 | Train Acc: 100.00% | Val Loss: 1.07 | Val Acc: 67.20%\nEpoch 110 | Train Loss: 0.012 | Train Acc: 100.00% | Val Loss: 1.14 | Val Acc: 67.20%\nEpoch 120 | Train Loss: 0.014 | Train Acc: 100.00% | Val Loss: 1.12 | Val Acc: 66.40%\nEpoch 130 | Train Loss: 0.009 | Train Acc: 100.00% | Val Loss: 1.12 | Val Acc: 68.20%\nEpoch 140 | Train Loss: 0.007 | Train Acc: 100.00% | Val Loss: 1.19 | Val Acc: 65.40%\nEpoch 150 | Train Loss: 0.008 | Train Acc: 100.00% | Val Loss: 1.14 | Val Acc: 66.80%\nEpoch 160 | Train Loss: 0.007 | Train Acc: 100.00% | Val Loss: 1.16 | Val Acc: 68.40%\nEpoch 170 | Train Loss: 0.008 | Train Acc: 100.00% | Val Loss: 1.11 | Val Acc: 68.20%\nEpoch 180 | Train Loss: 0.006 | Train Acc: 100.00% | Val Loss: 1.13 | Val Acc: 68.60%\nEpoch 190 | Train Loss: 0.005 | Train Acc: 100.00% | Val Loss: 1.14 | Val Acc: 68.60%\nEpoch 200 | Train Loss: 0.007 | Train Acc: 100.00% | Val Loss: 1.13 | Val Acc: 68.40%\n\nGAT test accuracy: 70.00%\n\nCPU times: user 53.4 s, sys: 2.68 s, total: 56.1 s\nWall time: 55.9 s\n\n\nThis experiment is not super rigorous: we‚Äôd need to repeat it n times and report the average accuracy with a standard deviation as the final result.\nIn this example, we can see that the GAT outperforms the GCN in terms of accuracy (70.00% vs.¬†67.70%), but takes longer to train (55.9s vs.¬†32.4s). It‚Äôs a tradeoff that can cause scalability issues when working with large graphs.\nThe authors obtained 72.5% for the GAT and 70.3% for the GCN, which is significantly better than what we got. The difference can be explained by additional preprocessing steps, some tweaks in the models, and a different training setting (e.g., a patience of 100 instead of a fixed number of epochs). We kept the code as simple as possible here, but feel free to modify it to improve the results.\nBeyond the accuracy score, it is interesting to see what the GAT actually learned. We can visualize it with t-SNE plot, a powerful method to plot high-dimensional data in 2D or 3D. First, let‚Äôs see what the embeddings looked like before any training: it should be random since they‚Äôre produced by randomly initialized weight matrices.\n\n# Initialize new untrained model\nuntrained_gat = GAT(dataset.num_features, 8, dataset.num_classes)\n\n# Get embeddings\nh, _ = untrained_gat(data.x, data.edge_index)\n\n# Train TSNE\ntsne = TSNE(n_components=2, learning_rate='auto',\n         init='pca').fit_transform(h.detach())\n\n# Plot TSNE\nplt.figure(figsize=(10, 10))\nplt.axis('off')\nplt.scatter(tsne[:, 0], tsne[:, 1], s=50, c=data.y)\nplt.show()\n\n/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n  FutureWarning,\n\n\n\n\n\nIndeed, there‚Äôs no apparent structure. But do the embeddings produced by our trained model look better?\n\n# Get embeddings\nh, _ = gat(data.x, data.edge_index)\n\n# Train TSNE\ntsne = TSNE(n_components=2, learning_rate='auto',\n         init='pca').fit_transform(h.detach())\n\n# Plot TSNE\nplt.figure(figsize=(10, 10))\nplt.axis('off')\nplt.scatter(tsne[:, 0], tsne[:, 1], s=50, c=data.y)\nplt.show()\n\n/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n  FutureWarning,\n\n\n\n\n\nThe difference is noticeable: nodes belonging to the same classes cluster together. We can see six clusters, corresponding to the six classes of papers. There are outliers, but this was to be expected: our accuracy score is far from perfect.\nPreviously, I speculated that poorly connected nodes might negatively impact performance on CiteSeer. So let‚Äôs verify that by calculating the model‚Äôs accuracy for each degree.\n\nfrom torch_geometric.utils import degree\n\n# Get model's classifications\n_, out = gat(data.x, data.edge_index)\n\n# Calculate the degree of each node\ndegrees = degree(data.edge_index[0]).numpy()\n\n# Store accuracy scores and sample sizes\naccuracies = []\nsizes = []\n\n# Accuracy for degrees between 0 and 5\nfor i in range(0, 6):\n  mask = np.where(degrees == i)[0]\n  accuracies.append(accuracy(out.argmax(dim=1)[mask], data.y[mask]))\n  sizes.append(len(mask))\n\n# Accuracy for degrees &gt; 5\nmask = np.where(degrees &gt; 5)[0]\naccuracies.append(accuracy(out.argmax(dim=1)[mask], data.y[mask]))\nsizes.append(len(mask))\n\n# Bar plot\nfig, ax = plt.subplots(figsize=(18, 9))\nax.set_xlabel('Node degree')\nax.set_ylabel('Accuracy score')\nax.set_facecolor('#EFEEEA')\nplt.bar(['0','1','2','3','4','5','&gt;5'],\n        accuracies,\n        color='#0A047A')\nfor i in range(0, 7):\n    plt.text(i, accuracies[i], f'{accuracies[i]*100:.2f}%',\n             ha='center', color='#0A047A')\nfor i in range(0, 7):\n    plt.text(i, accuracies[i]//2, sizes[i],\n             ha='center', color='white')\n\n\n\n\nThese results confirm our intuition: nodes with few neighbors are indeed harder to classify. This is due to the nature of GNNs: the more relevant connections you have, the more information you can aggregate."
  },
  {
    "objectID": "posts/2022-03-09-Graph_Attention_Network.html#conclusion",
    "href": "posts/2022-03-09-Graph_Attention_Network.html#conclusion",
    "title": "Graph Attention Networks: Self-Attention for GNNs",
    "section": "Conclusion",
    "text": "Conclusion\nWhile they take longer to train, GATs often provide a substantial improvement over GCNs in terms of accuracy. The self-attention mechanism automatically calculates weights instead of static coefficients to produce better embeddings. In this article,\n\nWe learned how to calculate dynamic weights using self-attention\nWe implemented and compared two architectures (a GCN and a GAT) in PyTorch Geometric\nWe visualized what the GAT learned with a t-SNE plot and the accuracy score for each degree\n\nGATs are a standard architecture in a lot of GNN applications. However, their slow training time can become a problem when applied to massive graph datasets. Scalability is an important factor in deep learning: more data can often lead to better performance.\nIn the next article, we‚Äôll see how to improve scalability with mini-batching and a new GNN architecture, called GraphSAGE.\nIf you enjoyed this tutorial, feel free to follow me on Twitter for more GNN content. Thank you, and see you in the next article! üì£"
  },
  {
    "objectID": "posts/2022-03-09-Graph_Attention_Network.html#graph-neural-network-course",
    "href": "posts/2022-03-09-Graph_Attention_Network.html#graph-neural-network-course",
    "title": "Graph Attention Networks: Self-Attention for GNNs",
    "section": "üåê Graph Neural Network Course",
    "text": "üåê Graph Neural Network Course\nüîé Course overview\nüìù Chapter 1: Introduction to Graph Neural Networks\nüìù Chapter 2: Graph Attention Network\nüìù Chapter 3: GraphSAGE\nüìù Chapter 4: Graph Isomorphism Network"
  },
  {
    "objectID": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html",
    "href": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html",
    "title": "Efficiently iterating over rows in a Pandas DataFrame",
    "section": "",
    "text": "When I started machine learning, I followed the guidelines and created my own features by combining multiple columns in my dataset. It‚Äôs all well and good, but the way I did it was horribly inefficient. I had to wait several minutes to do the most basic operations.\nMy problem was simple: I didn‚Äôt know the fastest way to iterate over rows in Pandas.\nI often see people online using the same techniques I used to apply. It‚Äôs not elegant but it‚Äôs ok if you don‚Äôt have much data. However, if you process more than 10k rows, it quickly becomes an obvious performance issue.\nIn this article, I‚Äôm gonna give you the best way to iterate over rows in a Pandas DataFrame, with no extra code required. It‚Äôs not just about performance: it‚Äôs also about understanding what‚Äôs going on under the hood to become a better data scientist.\nLet‚Äôs import a dataset in Pandas. In this case, I chose the one I worked on when I started: it‚Äôs time to fix my past mistakes! ü©π\nYou can run the code with the following Google Colab notebook.\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('https://raw.githubusercontent.com/mlabonne/how-to-data-science/main/data/nslkdd_test.txt')\ndf\n\n\n  \n    \n      \n\n\n\n\n\n\nduration\nprotocol_type\nservice\nflag\nsrc_bytes\ndst_bytes\nland\nwrong_fragment\nurgent\nhot\n...\ndst_host_same_srv_rate\ndst_host_diff_srv_rate\ndst_host_same_src_port_rate\ndst_host_srv_diff_host_rate\ndst_host_serror_rate\ndst_host_srv_serror_rate\ndst_host_rerror_rate\ndst_host_srv_rerror_rate\nattack_type\nother\n\n\n\n\n0\n0\ntcp\nprivate\nREJ\n0\n0\n0\n0\n0\n0\n...\n0.04\n0.06\n0.00\n0.00\n0.00\n0.0\n1.00\n1.00\nneptune\n21\n\n\n1\n0\ntcp\nprivate\nREJ\n0\n0\n0\n0\n0\n0\n...\n0.00\n0.06\n0.00\n0.00\n0.00\n0.0\n1.00\n1.00\nneptune\n21\n\n\n2\n2\ntcp\nftp_data\nSF\n12983\n0\n0\n0\n0\n0\n...\n0.61\n0.04\n0.61\n0.02\n0.00\n0.0\n0.00\n0.00\nnormal\n21\n\n\n3\n0\nicmp\neco_i\nSF\n20\n0\n0\n0\n0\n0\n...\n1.00\n0.00\n1.00\n0.28\n0.00\n0.0\n0.00\n0.00\nsaint\n15\n\n\n4\n1\ntcp\ntelnet\nRSTO\n0\n15\n0\n0\n0\n0\n...\n0.31\n0.17\n0.03\n0.02\n0.00\n0.0\n0.83\n0.71\nmscan\n11\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n22539\n0\ntcp\nsmtp\nSF\n794\n333\n0\n0\n0\n0\n...\n0.72\n0.06\n0.01\n0.01\n0.01\n0.0\n0.00\n0.00\nnormal\n21\n\n\n22540\n0\ntcp\nhttp\nSF\n317\n938\n0\n0\n0\n0\n...\n1.00\n0.00\n0.01\n0.01\n0.01\n0.0\n0.00\n0.00\nnormal\n21\n\n\n22541\n0\ntcp\nhttp\nSF\n54540\n8314\n0\n0\n0\n2\n...\n1.00\n0.00\n0.00\n0.00\n0.00\n0.0\n0.07\n0.07\nback\n15\n\n\n22542\n0\nudp\ndomain_u\nSF\n42\n42\n0\n0\n0\n0\n...\n0.99\n0.01\n0.00\n0.00\n0.00\n0.0\n0.00\n0.00\nnormal\n21\n\n\n22543\n0\ntcp\nsunrpc\nREJ\n0\n0\n0\n0\n0\n0\n...\n0.08\n0.03\n0.00\n0.00\n0.00\n0.0\n0.44\n1.00\nmscan\n14\n\n\n\n\n\n22544 rows √ó 43 columns\nThis dataset has 22k rows and 43 columns with a combination of categorical and numerical values. Each row describes a connection between two computers.\nLet‚Äôs say we want to create a new feature: the total number of bytes in the connection. We just have to sum up two existing features: src_bytes and dst_bytes. Let‚Äôs see different methods to calculate this new feature."
  },
  {
    "objectID": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html#iterrows",
    "href": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html#iterrows",
    "title": "Efficiently iterating over rows in a Pandas DataFrame",
    "section": "‚ùå‚ùå 1. Iterrows",
    "text": "‚ùå‚ùå 1. Iterrows\nAccording to the official documentation, iterrows() iterates ‚Äúover the rows of a Pandas DataFrame as (index, Series) pairs‚Äù. It converts each row into a Series object, which causes two problems:\n\nIt can change the type of your data (dtypes);\nThe conversion greatly degrades performance.\n\nFor these reasons, the ill-named iterrows() is the WORST possible method to actually iterate over rows.\n\n%%timeit -n 10\n# Iterrows\ntotal = []\nfor index, row in df.iterrows():\n    total.append(row['src_bytes'] + row['dst_bytes'])\n\n10 loops, best of 5: 1.07 s per loop\n\n\nNow let‚Äôs see slightly better techniques‚Ä¶"
  },
  {
    "objectID": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html#for-loop-with-.loc-or-.iloc-3-faster",
    "href": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html#for-loop-with-.loc-or-.iloc-3-faster",
    "title": "Efficiently iterating over rows in a Pandas DataFrame",
    "section": "‚ùå 2. For loop with .loc or .iloc (3√ó faster)",
    "text": "‚ùå 2. For loop with .loc or .iloc (3√ó faster)\nThis is what I used to do when I started: a basic for loop to select rows by index (with .loc or .iloc).\nWhy is it bad? Because DataFrames are not designed for this purpose. As with the previous method, rows are converted into Pandas Series objects, which degrades performance.\nInterestingly enough, .iloc is faster than .loc. It makes sense since Python doesn‚Äôt have to check user-defined labels and directly look at where the row is stored in memory.\n\n%%timeit -n 10\n# For loop with .loc\ntotal = []\nfor index in range(len(df)):\n    total.append(df['src_bytes'].loc[index] + df['dst_bytes'].loc[index])\n\n10 loops, best of 5: 600 ms per loop\n\n\n\n%%timeit -n 10\n# For loop with .iloc\ntotal = []\nfor index in range(len(df)):\n    total.append(df['src_bytes'].iloc[index] + df['dst_bytes'].iloc[index])\n\n10 loops, best of 5: 377 ms per loop\n\n\nEven this basic for loop with .iloc is 3 times faster than the first method!"
  },
  {
    "objectID": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html#apply-4-faster",
    "href": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html#apply-4-faster",
    "title": "Efficiently iterating over rows in a Pandas DataFrame",
    "section": "‚ùå 3. Apply (4√ó faster)",
    "text": "‚ùå 3. Apply (4√ó faster)\nThe apply() method is another popular choice to iterate over rows. It creates code that is easy to understand but at a cost: performance is nearly as bad as the previous for loop.\nThis is why I would strongly advise you to avoid this function for this specific purpose (it‚Äôs fine for other applications).\nNote that I convert the DataFrame into a list using the to_list() method to obtain identical results.\n\n%%timeit -n 10\n# Apply\ndf.apply(lambda row: row['src_bytes'] + row['dst_bytes'], axis=1).to_list()\n\n10 loops, best of 5: 282 ms per loop\n\n\nThe apply() method is a for loop in disguise, which is why the performance doesn‚Äôt improve that much: it‚Äôs only 4 times faster than the first technique."
  },
  {
    "objectID": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html#itertuples-10-faster",
    "href": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html#itertuples-10-faster",
    "title": "Efficiently iterating over rows in a Pandas DataFrame",
    "section": "‚ùå 4. Itertuples (10√ó faster)",
    "text": "‚ùå 4. Itertuples (10√ó faster)\nIf you know about iterrows(), you probably know about itertuples(). According to the official documentation, it iterates ‚Äúover the rows of a DataFrame as namedtuples of the values‚Äù. In practice, it means that rows are converted into tuples, which are much lighter objects than Pandas Series.\nThis is why itertuples() is a better version of iterrows(). This time, we need to access the values with an attribute (or an index). If you want to access them with a string (e.g., if there‚Äôs a space in the string), you can use the getattr() function instead.\n\n%%timeit -n 10\n# Itertuples\ntotal = []\nfor row in df.itertuples():\n    total.append(row.src_bytes + row.dst_bytes)\n\n10 loops, best of 5: 99.3 ms per loop\n\n\nThis is starting to look better: it is now 10 times faster than iterrows()."
  },
  {
    "objectID": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html#list-comprehensions-200-faster",
    "href": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html#list-comprehensions-200-faster",
    "title": "Efficiently iterating over rows in a Pandas DataFrame",
    "section": "‚ùå 5. List comprehensions (200√ó faster)",
    "text": "‚ùå 5. List comprehensions (200√ó faster)\nList comprehensions are a fancy way to iterate over a list as a one-liner.\nFor instance, [print(i) for i in range(10)] prints numbers from 0 to 9 without any explicit for loop. I say ‚Äúexplicit‚Äù because Python actually processes it as a for loop if we look at the bytecode.\nSo why is it faster? Quite simply because we don‚Äôt call the .append() method in this version.\n\n%%timeit -n 100\n# List comprehension\n[src + dst for src, dst in zip(df['src_bytes'], df['dst_bytes'])]\n\n100 loops, best of 5: 5.54 ms per loop\n\n\nIndeed, this technique is 200 times faster than the first one! But we can still do better."
  },
  {
    "objectID": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html#pandas-vectorization-1500-faster",
    "href": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html#pandas-vectorization-1500-faster",
    "title": "Efficiently iterating over rows in a Pandas DataFrame",
    "section": "‚úÖ 6. Pandas vectorization (1500√ó faster)",
    "text": "‚úÖ 6. Pandas vectorization (1500√ó faster)\nUntil now, all the techniques used simply add up single values. Instead of adding single values, why not group them into vectors to sum them up? The difference between adding two numbers or two vectors is not significant for a CPU, which should speed things up.\nOn top of that, Pandas can process Series objects in parallel, using every CPU core available!\nThe syntax is also the simplest imaginable: this solution is extremely intuitive. Under the hood, Pandas takes care of vectorizing our data with an optimized C code using contiguous memory blocks.\n\n%%timeit -n 1000\n# Vectorization\n(df['src_bytes'] + df['dst_bytes']).to_list()\n\n1000 loops, best of 5: 734 ¬µs per loop\n\n\nThis code is 1500 times faster than iterrows() and it is even simpler to write."
  },
  {
    "objectID": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html#numpy-vectorization-1900-faster",
    "href": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html#numpy-vectorization-1900-faster",
    "title": "Efficiently iterating over rows in a Pandas DataFrame",
    "section": "‚úÖ‚úÖ 7. NumPy vectorization (1900√ó faster)",
    "text": "‚úÖ‚úÖ 7. NumPy vectorization (1900√ó faster)\nNumPy is designed to handle scientific computing. It has less overhead than Pandas methods since rows and dataframes all become np.array. It relies on the same optimizations as Pandas vectorization.\nThere are two ways of converting a Series into a np.array: using .values or .to_numpy(). The former has been deprecated for years, which is why we‚Äôre gonna use .to_numpy() in this example.\n\n%%timeit -n 1000\n# Numpy vectorization\n(df['src_bytes'].to_numpy() + df['dst_bytes'].to_numpy()).tolist()\n\n1000 loops, best of 5: 575 ¬µs per loop\n\n\nWe found our winner with a technique that is 1900 times faster than our first competitor! Let‚Äôs wrap things up."
  },
  {
    "objectID": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html#conclusion",
    "href": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html#conclusion",
    "title": "Efficiently iterating over rows in a Pandas DataFrame",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\nDon‚Äôt be like me: if you need to iterate over rows in a DataFrame, vectorization is the way to go! You can find the code to reproduce the experiments at this address. Vectorization is not harder to read, it doesn‚Äôt take longer to write, and the performance gain is incredible.\nIt‚Äôs not just about performance: understanding how each method works under the hood helped me to write better code. Performance gains are always based on the same techniques: transforming data into vectors and matrices to take advantage of parallel processing. Alas, this is often at the expense of readability. But it doesn‚Äôt have to be.\nIterating over rows is just an example but it shows that, sometimes, you can have the cake and eat it. üéÇ\nIf you liked this article, follow me on Twitter @maximelabonne for more tips about data science and machine learning!"
  },
  {
    "objectID": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html#bonus",
    "href": "posts/2022-03-21-Efficiently_iterating_over_rows_in_a_Pandas_DataFrame.html#bonus",
    "title": "Efficiently iterating over rows in a Pandas DataFrame",
    "section": "üìà Bonus",
    "text": "üìà Bonus\nWe can measure the performance of each method depending on the size of the DataFrame. I reimplemented all of them in this dummy example using perfplot to show that the leaderboard might be different under 300 rows. Anyway, such a dataset would be so small that we wouldn‚Äôt need much optimization.\n\n!pip install -q perfplot\n\nimport perfplot \nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 22})\n\n# Techniques\ndef forloop(df):\n    total = []\n    for index in range(len(df)):\n        total.append(df['col1'].iloc[index] \n                   + df['col2'].iloc[index])\n    return total\n\ndef itertuples(df):\n    total = []\n    for row in df.itertuples():\n        total.append(row[1] + row[2])\n    return total\n\ndef iterrows(df):\n    total = []\n    for index, row in df.iterrows():\n        total.append(row['col1']\n                   + row['col2'])\n    return total\n\ndef apply(df):\n    return df.apply(lambda row: row['col1']\n                              + row['col2'], axis=1).to_list()\n\ndef comprehension(df):\n    return [src + dst for src, dst in zip(df['col1'], df['col2'])]\n\ndef pd_vectorize(df):\n    return (df['col1'] + df['col2']).to_list()\n\ndef np_vectorize(df):\n    return (df['col1'].to_numpy() + df['col2'].to_numpy()).tolist()\n\n# Perfplot\nfunctions = [iterrows, forloop, apply, itertuples,\n             comprehension, pd_vectorize, np_vectorize]\ndf = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n\nout = perfplot.bench(\n      setup=lambda n: pd.concat([df]*n, ignore_index=True),\n      kernels=functions,\n      labels=[str(f.__name__) for f in functions],\n      n_range=[2**n for n in range(20)],\n      xlabel='Number of rows',\n)\n\nplt.figure(figsize=(20,12))\nout.show()\n\n\n\n\n\n\n\n\n\n\n(8.117999999999996e-06, 51.558470168)"
  },
  {
    "objectID": "posts/2022-03-28-What_is_a_Tensor_in_Deep_Learning.html",
    "href": "posts/2022-03-28-What_is_a_Tensor_in_Deep_Learning.html",
    "title": "What is a Tensor in Machine Learning?",
    "section": "",
    "text": "What is a tensor, exactly?\nMost deep learning practitioners know about them but can‚Äôt pinpoint an exact definition.\nTensorFlow, PyTorch: every deep learning framework relies on the same basic object: tensors. They‚Äôre used to store almost everything in deep learning: input data, weights, biases, predictions, etc.\nAnd yet, their definition is incredibly fuzzy: the Wikipedia category alone has over 100 pages related to tensors.\nIn this article, we‚Äôll give a definitive answer to the following question: what is a tensor in neural networks?"
  },
  {
    "objectID": "posts/2022-03-28-What_is_a_Tensor_in_Deep_Learning.html#tensors-in-computer-science",
    "href": "posts/2022-03-28-What_is_a_Tensor_in_Deep_Learning.html#tensors-in-computer-science",
    "title": "What is a Tensor in Machine Learning?",
    "section": "üíª Tensors in computer science",
    "text": "üíª Tensors in computer science\nSo why are there so many definitions?\nIt‚Äôs quite simple: different fields have different definitions. Tensors in mathematics are not quite the same as tensors in physics, which are different from tensors in computer science.\n\n\n\nThese definitions can be divided into two categories: tensors as a data structure or as objects (in an object-oriented programming sense).\n\nData structure: this is the definition we use in computer science. Tensors are multidimensional arrays that store a specific type of value.\nObjects: this is the definition used in other fields. In mathematics and physics, tensors are not just a data structure: they also have a list of properties, like a specific product.\n\nThis is why you see a lot of people (sometimes quite pedantically) saying ‚Äútensors are not n-dimensional arrays/matrices‚Äù: they don‚Äôt talk about data structures, but about objects with properties.\nEven the same words have different meanings. For instance, in computer science, a 2D tensor is a matrix (it‚Äôs a tensor of rank 2). In linear algebra, a tensor with 2 dimensions means it only stores two values. The rank also has a completely different definition: it is the maximum number of its linearly independent column (or row) vectors.\nIn computer science, we‚Äôre only interested in a definition focused on the data structure. From this point of view, tensors truly are a generalization in n dimensions of matrices.\nBut we‚Äôre still missing an important nuance when talking about tensors specifically in the context of deep learning‚Ä¶"
  },
  {
    "objectID": "posts/2022-03-28-What_is_a_Tensor_in_Deep_Learning.html#tensors-in-deep-learning",
    "href": "posts/2022-03-28-What_is_a_Tensor_in_Deep_Learning.html#tensors-in-deep-learning",
    "title": "What is a Tensor in Machine Learning?",
    "section": "üß† Tensors in deep learning",
    "text": "üß† Tensors in deep learning\n\n Icons created by Freepik and smashingstocks - Flaticon\n\nSo why are they called ‚Äútensors‚Äù instead of ‚Äúmultidimensional arrays‚Äù? Ok, it is shorter, but is it all there is to it? Actually, people make an implicit assumption when they talk about tensors.\nPyTorch‚Äôs official documentation gives us a practical answer:\n\nThe biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either CPU or GPU.\n\nIn deep learning, we need performance to compute a lot of matrix multiplications in a highly parallel way. These matrices (and n-dimensional arrays in general) are generally stored and processed on GPUs to speed up training and inference times.\nThis is what was missing in our previous definition: tensors in deep learning are not just n-dimensional arrays, there‚Äôs also the implicit assumption they can be run on a GPU."
  },
  {
    "objectID": "posts/2022-03-28-What_is_a_Tensor_in_Deep_Learning.html#numpy-vs-pytorch",
    "href": "posts/2022-03-28-What_is_a_Tensor_in_Deep_Learning.html#numpy-vs-pytorch",
    "title": "What is a Tensor in Machine Learning?",
    "section": "‚öîÔ∏è NumPy vs PyTorch",
    "text": "‚öîÔ∏è NumPy vs PyTorch\nLet‚Äôs see the difference between NumPy arrays and PyTorch tensors.\n\n\n\nThese two objects are very similar: we can initialize a 1D array and a 1D tensor with nearly the same syntax. They also share a lot of methods and can be easily converted into one another.\nYou can find the code used in this article at this address\n\nimport numpy as np\nimport torch\n\narray = np.array([1, 2, 3])\nprint(f'NumPy Array: {array}')\n\ntensor = torch.tensor([1, 2, 3])\nprint(f'PyTorch Tensor: {tensor}')\n\nNumPy Array: [1 2 3]\nPyTorch Tensor: tensor([1, 2, 3])\n\n\nInitializing 2D arrays and 2D tensors is not more complicated.\n\nx = np.array([[1, 2, 3],\n              [4, 5, 6]])\nprint(f'NumPy Array:\\n{x}')\n\nx = torch.tensor([[1, 2, 3],\n                  [4, 5, 6]])\nprint(f'\\nPyTorch Tensor:\\n{x}')\n\nNumPy Array:\n[[1 2 3]\n [4 5 6]]\n\nPyTorch Tensor:\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\nWe said that the only difference between tensors and arrays was the fact that tensors can be run on GPUs. So in the end, this distinction is based on performance. But is this boost that important?\nLet‚Äôs compare the performance between NumPy arrays and PyTorch tensors on matrix multiplication. In the following example, we randomly initialize 4D arrays/tensors and multiply them.\n\n# You need to run this code on a computer with a GPU\ndevice = torch.device(\"cuda\")\n\n# 4D arrays\narray1 = np.random.rand(100, 100, 100, 100)\narray2 = np.random.rand(100, 100, 100, 100)\n\n# 4D tensors\ntensor1 = torch.rand(100, 100, 100, 100).to(device)\ntensor2 = torch.rand(100, 100, 100, 100).to(device)\n\n\n%%timeit\nnp.matmul(array1, array2)\n\n1 loop, best of 5: 1.32 s per loop\n\n\n\n%%timeit\ntorch.matmul(tensor1, tensor2)\n\n1000 loops, best of 5: 25.2 ms per loop\n\n\nAs we can see, PyTorch tensors completed outperformed NumPy arrays: they completed the multiplication 52 times faster!\nWe could attribute this performance to different factors, such as:\n\nNumPy arrays use a float64 format, whereas PyTorch tensors leverage the more efficient float32 format. However, even when NumPy arrays are converted to float32, PyTorch tensors are still 40 times faster.\nPyTorch tensors are stored on a GPU, unlike NumPy arrays. But if we repeat the same experiment on a CPU, PyTorch tensors still manage to be 2.8 times faster on average.\n\nEven when combining both factors, PyTorch tensors prove to be 1.4 times faster, showing that NumPy arrays are truly less performant for matrix multiplication.\nThis is the true power of tensors: they‚Äôre blazingly fast! Performance might vary depending on the dimensions, the implementation, and the hardware, but this speed is the reason why tensors (and not arrays) are so common in deep learning."
  },
  {
    "objectID": "posts/2022-03-28-What_is_a_Tensor_in_Deep_Learning.html#conclusion",
    "href": "posts/2022-03-28-What_is_a_Tensor_in_Deep_Learning.html#conclusion",
    "title": "What is a Tensor in Machine Learning?",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we wrote a definition of tensors based on:\n\nTheir use in computer science (data structure);\nMore specifically, in deep learning (they can run on GPUs).\n\nHere‚Äôs how we can summarize it in one sentence:\n\nTensors are n-dimensional arrays with the implicit assumption that they can run on a GPU.\n\nFinally, we saw the difference in performance between tensors and arrays, which motivates the need for tensors in deep learning.\nSo next time someone tries to explain to you that tensors are not exactly a generalization of matrices, you‚Äôll know that they‚Äôre right in a particular definition of tensors, but not in the computer science/deep learning one.\nIf you‚Äôre looking for more data science and machine learning content in n-dimensions, please follow me on twitter @maximelabonne. üì£"
  },
  {
    "objectID": "posts/2022-04-06-GraphSAGE.html",
    "href": "posts/2022-04-06-GraphSAGE.html",
    "title": "GraphSAGE: Scaling up Graph Neural Networks",
    "section": "",
    "text": "What do UberEats and Pinterest have in common?\nThey both use GraphSAGE to power their recommender systems on a massive scale: millions and billions of nodes and edges.\nIn this tutorial, we‚Äôll use a dataset with 20k nodes instead of billions because Google Colab cannot handle our ambitions. We will stick to the original GraphSAGE architecture, but the previous variants also bring exciting features we will discuss.\nYou can run the code with the following Google Colab notebook.\n# Install PyTorch Geometric\nimport torch\n!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n\n# Visualization\nimport networkx as nx\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.dpi'] = 300\nplt.rcParams.update({'font.size': 24})"
  },
  {
    "objectID": "posts/2022-04-06-GraphSAGE.html#i.-pubmed-dataset",
    "href": "posts/2022-04-06-GraphSAGE.html#i.-pubmed-dataset",
    "title": "GraphSAGE: Scaling up Graph Neural Networks",
    "section": "üåê I. PubMed dataset",
    "text": "üåê I. PubMed dataset\n\n\n\nAs we saw in the previous article, PubMed is part of the Planetoid dataset (MIT license). Here‚Äôs a quick summary:\n\nIt contains 19,717 scientific publications about diabetes from PubMed‚Äôs database\nNode features are TF-IDF weighted word vectors with 500 dimensions, which is an efficient way of summarizing documents without transformers\nThe task is quite straightforward since it‚Äôs a multi-class classification with three categories: diabetes mellitus experimental, diabetes mellitus type 1, and diabetes mellitus type 2\n\nLet‚Äôs load the dataset and print some information about the graph.\n\nfrom torch_geometric.datasets import Planetoid\n\ndataset = Planetoid(root='.', name=\"Pubmed\")\ndata = dataset[0]\n\n# Print information about the dataset\nprint(f'Dataset: {dataset}')\nprint('-------------------')\nprint(f'Number of graphs: {len(dataset)}')\nprint(f'Number of nodes: {data.x.shape[0]}')\nprint(f'Number of features: {dataset.num_features}')\nprint(f'Number of classes: {dataset.num_classes}')\n\n# Print information about the graph\nprint(f'\\nGraph:')\nprint('------')\nprint(f'Training nodes: {sum(data.train_mask).item()}')\nprint(f'Evaluation nodes: {sum(data.val_mask).item()}')\nprint(f'Test nodes: {sum(data.test_mask).item()}')\nprint(f'Edges are directed: {data.is_directed()}')\nprint(f'Graph has isolated nodes: {data.has_isolated_nodes()}')\nprint(f'Graph has loops: {data.has_self_loops()}')\n\nDataset: Pubmed()\n-------------------\nNumber of graphs: 1\nNumber of nodes: 19717\nNumber of features: 500\nNumber of classes: 3\n\nGraph:\n------\nTraining nodes: 60\nEvaluation nodes: 500\nTest nodes: 1000\nEdges are directed: False\nGraph has isolated nodes: False\nGraph has loops: False\n\n\nAs we can see, PubMed has an insanely low number of training nodes compared to the whole graph. There are only 60 samples to learn how to classify the 1000 test nodes.\nDespite this challenge, GNNs manage to obtain high levels of accuracy. Here‚Äôs the leaderboard of known techniques (a more exhaustive benchmark can be found on PapersWithCode):\n\n\n\nModel\nüìùPubMed (accuracy)\n\n\n\n\nMultilayer Perceptron\n71.4%\n\n\nGraph Convolutional Network\n79.0% ¬± 0.3%\n\n\nGraph Attention Network\n79.0% ¬± 0.3%\n\n\nGraphSAGE\n???\n\n\n\nI couldn‚Äôt find any result for GraphSAGE on PubMed with this specific setting (60 training nodes, 1000 test nodes), so I don‚Äôt expect a great accuracy. But another metric can be just as relevant when working with large graphs: training time."
  },
  {
    "objectID": "posts/2022-04-06-GraphSAGE.html#ii.-graphsage-in-theory",
    "href": "posts/2022-04-06-GraphSAGE.html#ii.-graphsage-in-theory",
    "title": "GraphSAGE: Scaling up Graph Neural Networks",
    "section": "üßô‚Äç‚ôÇÔ∏è II. GraphSAGE in theory",
    "text": "üßô‚Äç‚ôÇÔ∏è II. GraphSAGE in theory\n\n\n\nThe GraphSAGE algorithm can be divided into two steps:\n\nNeighbor sampling;\nAggregation.\n\n\nüé∞ A. Neighbor sampling\nNeighbor sampling relies on a classic technique used to train neural networks: mini-batch gradient descent.\nMini-batch gradient descent works by breaking down a dataset into smaller batches. During training, we compute the gradient for every mini-batch instead of every epoch (batch gradient descent) or every training sample (stochastic gradient descent). Mini-batching has several benefits:\n\nImproved accuracy ‚Äî mini-batches help to reduce overfitting (gradients are averaged), as well as variance in error rates\nIncreased speed ‚Äî mini-batches are processed in parallel and take less time to train than larger batches\nImproved scalability ‚Äî an entire dataset can exceed the GPU memory, but smaller batches can get around this limitation\n\nMore advanced optimizes like Adam also rely on mini-batching. However, it is not as straightforward with graph data since splitting the dataset into smaller chunks would break essential connections between nodes.\nSo, what can we do? In recent years, researchers developed different strategies to create graph mini-batches. The one we‚Äôre interested in is called neighbor sampling). There are many other techniques you can find on PyG‚Äôs documentation, such as subgraph clustering).\n\n\n\n\nNeighbor sampling considers only a fixed number of random neighbors. Here‚Äôs the process:\n\nThe sampler randomly selects a defined number of neighbors (1 hop), neighbors of neighbors (2 hops), etc. we would like to have\nThe sampler outputs a subgraph containing the target and sampled nodes\n\nThis process is repeated for every node in a list or the entirety of the graph. However, creating a subgraph for each node is not efficient, which is why we can process them in batches instead. In this case, each subgraph is shared by multiple target nodes.\nNeighbor sampling has an added benefit. Sometimes, we observe extremely popular nodes that act like hubs, such as celebrities on social media. Calculating embeddings for these nodes can be computationally very expensive since it requires calculating the hidden vectors of thousands or even millions of neighbors. GraphSAGE fixes this issue by only considering a fixed number of neighbors.\nIn PyG, neighbor sampling is implemented through the NeighborLoader object. Let‚Äôs say we want 5 neighbors and 10 of their neighbors (num_neighbors). As we discussed, we can also specify a batch_size to speed up the process by creating subgraphs for multiple target nodes.\n\nfrom torch_geometric.loader import NeighborLoader\nfrom torch_geometric.utils import to_networkx\n\n# Create batches with neighbor sampling\ntrain_loader = NeighborLoader(\n    data,\n    num_neighbors=[5, 10],\n    batch_size=16,\n    input_nodes=data.train_mask,\n)\n\n# Print each subgraph\nfor i, subgraph in enumerate(train_loader):\n    print(f'Subgraph {i}: {subgraph}')\n\n# Plot each subgraph\nfig = plt.figure(figsize=(16,16))\nfor idx, (subdata, pos) in enumerate(zip(train_loader, [221, 222, 223, 224])):\n    G = to_networkx(subdata, to_undirected=True)\n    ax = fig.add_subplot(pos)\n    ax.set_title(f'Subgraph {idx}')\n    plt.axis('off')\n    nx.draw_networkx(G,\n                    pos=nx.spring_layout(G, seed=0),\n                    with_labels=True,\n                    node_size=200,\n                    node_color=subdata.y,\n                    cmap=\"cool\",\n                    font_size=10\n                    )\nplt.show()\n\nSubgraph 0: Data(x=[389, 500], edge_index=[2, 448], y=[389], train_mask=[389], val_mask=[389], test_mask=[389], batch_size=16)\nSubgraph 1: Data(x=[264, 500], edge_index=[2, 314], y=[264], train_mask=[264], val_mask=[264], test_mask=[264], batch_size=16)\nSubgraph 2: Data(x=[283, 500], edge_index=[2, 330], y=[283], train_mask=[283], val_mask=[283], test_mask=[283], batch_size=16)\nSubgraph 3: Data(x=[189, 500], edge_index=[2, 229], y=[189], train_mask=[189], val_mask=[189], test_mask=[189], batch_size=12)\n\n\n\n\n\nWe created four subgraphs of various sizes. It allows us to process them in parallel and they‚Äôre easier to fit on a GPU since they‚Äôre smaller.\nThe number of neighbors is an important parameter since pruning our graph removes a lot of information. How much, exactly? Well, quite a lot. We can visualize this effect by looking at the node degrees (number of neighbors).\n\nfrom torch_geometric.utils import degree\nfrom collections import Counter\n\ndef plot_degree(data):\n    # Get list of degrees for each node\n    degrees = degree(data.edge_index[0]).numpy()\n\n    # Count the number of nodes for each degree\n    numbers = Counter(degrees)\n\n    # Bar plot\n    fig, ax = plt.subplots(figsize=(14, 6))\n    ax.set_xlabel('Node degree')\n    ax.set_ylabel('Number of nodes')\n    plt.bar(numbers.keys(),\n            numbers.values(),\n            color='#0A047A')\n\n# Plot node degrees from the original graph\nplot_degree(data)\n\n# Plot node degrees from the last subgraph\nplot_degree(subdata)\n\n\n\n\n\n\n\nThe first plot shows the original distribution of node degrees, and the second one shows the distribution we obtain after neighbor sampling. In this example, we chose to only consider five neighbors, which is much lower than the original maximal value. It‚Äôs important to remember this tradeoff when talking about GraphSAGE.\nPinSAGE proposes another solution. Instead of neighbor sampling, PinSAGE simulates random walks for each node, which captures a better representation of their neighborhoods. Then, it selects a predefined number of neighbors with the highest visit counts. This technique allows PinSAGE to consider the importance of each neighbor while controlling the size of the computation graph.\n\n\nüí• B. Aggregation\nThe aggregation process determines how to combine the feature vectors to produce the node embeddings. The original paper presents three ways of aggregating features:\n\nMean aggregator\nLSTM aggregator\nPooling aggregator\n\n\n\n\n\nThe mean aggregator is the simplest one. The idea is close to a GCN approach:\n\nThe hidden features of the target node and its selected neighbors are averaged (nodes in \\mathcal{\\tilde{N}}_i).\nA linear transformation with a weight matrix \\textbf{W} is applied.\n\nIn other words, we can write:\n\\textbf{h}_i' = \\textbf{W} \\cdot mean_{j \\in \\mathcal{\\tilde{N}}_i}(\\textbf{h}_j)\nThe result can then be fed to a nonlinear activation function like ReLU.\nThe LSTM aggregator may seem counter-intuitive because this architecture is sequential: it assigns an order to our unordered nodes. This is why the authors randomly shuffle them to force the LSTM only to consider the hidden features. Nevertheless, it is the best-performing technique in their benchmarks.\nThe pooling aggregator feeds each neighbor‚Äôs hidden vector to a feedforward neural network. Then, an elementwise max operation is applied to the result to keep the highest value for each feature."
  },
  {
    "objectID": "posts/2022-04-06-GraphSAGE.html#iii.-graphsage-in-pytorch-geometric",
    "href": "posts/2022-04-06-GraphSAGE.html#iii.-graphsage-in-pytorch-geometric",
    "title": "GraphSAGE: Scaling up Graph Neural Networks",
    "section": "üß† III. GraphSAGE in PyTorch Geometric",
    "text": "üß† III. GraphSAGE in PyTorch Geometric\nWe can easily implement a GraphSAGE architecture in PyTorch Geometric with the SAGEConv layer. This implementation uses two weight matrices instead of one, like UberEats‚Äô version of GraphSAGE:\n\\textbf{h}_i' = \\textbf{W}_1\\textbf{h}_i + \\textbf{W}_2 \\cdot mean_{j \\in \\mathcal{N}_i}(\\textbf{h}_j)\nLet‚Äôs create a network with two SAGEConv layers:\n\nThe first one uses ReLU as the activation function and a dropout layer;\nThe second one directly outputs the node embeddings.\n\nAs we‚Äôre dealing with a multi-class classification task, we‚Äôll use the cross-entropy loss as our loss function. I also added an L2 regularization of 0.0005 for good measure.\nTo see the benefits of GraphSAGE, let‚Äôs compare it with a GCN and a GAT without any sampling.\n\nimport torch\nfrom torch.nn import Linear, Dropout\nfrom torch_geometric.nn import SAGEConv, GATv2Conv, GCNConv\nimport torch.nn.functional as F\n\n\nclass GraphSAGE(torch.nn.Module):\n  \"\"\"GraphSAGE\"\"\"\n  def __init__(self, dim_in, dim_h, dim_out):\n    super().__init__()\n    self.sage1 = SAGEConv(dim_in, dim_h)\n    self.sage2 = SAGEConv(dim_h, dim_out)\n    self.optimizer = torch.optim.Adam(self.parameters(),\n                                      lr=0.01,\n                                      weight_decay=5e-4)\n\n  def forward(self, x, edge_index):\n    h = self.sage1(x, edge_index).relu()\n    h = F.dropout(h, p=0.5, training=self.training)\n    h = self.sage2(h, edge_index)\n    return F.log_softmax(h, dim=1)\n\n  def fit(self, data, epochs):\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = self.optimizer\n\n    self.train()\n    for epoch in range(epochs+1):\n      total_loss = 0\n      acc = 0\n      val_loss = 0\n      val_acc = 0\n\n      # Train on batches\n      for batch in train_loader:\n        optimizer.zero_grad()\n        out = self(batch.x, batch.edge_index)\n        loss = criterion(out[batch.train_mask], batch.y[batch.train_mask])\n        total_loss += loss\n        acc += accuracy(out[batch.train_mask].argmax(dim=1), \n                        batch.y[batch.train_mask])\n        loss.backward()\n        optimizer.step()\n\n        # Validation\n        val_loss += criterion(out[batch.val_mask], batch.y[batch.val_mask])\n        val_acc += accuracy(out[batch.val_mask].argmax(dim=1), \n                            batch.y[batch.val_mask])\n\n      # Print metrics every 10 epochs\n      if(epoch % 10 == 0):\n          print(f'Epoch {epoch:&gt;3} | Train Loss: {total_loss/len(train_loader):.3f} '\n                f'| Train Acc: {acc/len(train_loader)*100:&gt;6.2f}% | Val Loss: '\n                f'{val_loss/len(train_loader):.2f} | Val Acc: '\n                f'{val_acc/len(train_loader)*100:.2f}%')\n\nclass GAT(torch.nn.Module):\n  \"\"\"Graph Attention Network\"\"\"\n  def __init__(self, dim_in, dim_h, dim_out, heads=8):\n    super().__init__()\n    self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads)\n    self.gat2 = GATv2Conv(dim_h*heads, dim_out, heads=heads)\n    self.optimizer = torch.optim.Adam(self.parameters(),\n                                      lr=0.005,\n                                      weight_decay=5e-4)\n\n  def forward(self, x, edge_index):\n    h = F.dropout(x, p=0.6, training=self.training)\n    h = self.gat1(x, edge_index)\n    h = F.elu(h)\n    h = F.dropout(h, p=0.6, training=self.training)\n    h = self.gat2(h, edge_index)\n    return F.log_softmax(h, dim=1)\n\n  def fit(self, data, epochs):\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = self.optimizer\n\n    self.train()\n    for epoch in range(epochs+1):\n        # Training\n        optimizer.zero_grad()\n        out = self(data.x, data.edge_index)\n        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n        acc = accuracy(out[data.train_mask].argmax(dim=1),\n                       data.y[data.train_mask])\n        loss.backward()\n        optimizer.step()\n\n        # Validation\n        val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n        val_acc = accuracy(out[data.val_mask].argmax(dim=1),\n                           data.y[data.val_mask])\n\n        # Print metrics every 10 epochs\n        if(epoch % 10 == 0):\n            print(f'Epoch {epoch:&gt;3} | Train Loss: {loss:.3f} | Train Acc:'\n                  f' {acc*100:&gt;6.2f}% | Val Loss: {val_loss:.2f} | '\n                  f'Val Acc: {val_acc*100:.2f}%')\n\nclass GCN(torch.nn.Module):\n  \"\"\"Graph Convolutional Network\"\"\"\n  def __init__(self, dim_in, dim_h, dim_out):\n    super().__init__()\n    self.gcn1 = GCNConv(dim_in, dim_h)\n    self.gcn2 = GCNConv(dim_h, dim_out)\n    self.optimizer = torch.optim.Adam(self.parameters(),\n                                      lr=0.01,\n                                      weight_decay=5e-4)\n\n  def forward(self, x, edge_index):\n    h = F.dropout(x, p=0.5, training=self.training)\n    h = self.gcn1(h, edge_index).relu()\n    h = F.dropout(h, p=0.5, training=self.training)\n    h = self.gcn2(h, edge_index)\n    return F.log_softmax(h, dim=1)\n\n  def fit(self, data, epochs):\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = self.optimizer\n\n    self.train()\n    for epoch in range(epochs+1):\n        # Training\n        optimizer.zero_grad()\n        out = self(data.x, data.edge_index)\n        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n        acc = accuracy(out[data.train_mask].argmax(dim=1),\n                       data.y[data.train_mask])\n        loss.backward()\n        optimizer.step()\n\n        # Validation\n        val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n        val_acc = accuracy(out[data.val_mask].argmax(dim=1),\n                           data.y[data.val_mask])\n\n        # Print metrics every 10 epochs\n        if(epoch % 10 == 0):\n            print(f'Epoch {epoch:&gt;3} | Train Loss: {loss:.3f} | Train Acc:'\n                  f' {acc*100:&gt;6.2f}% | Val Loss: {val_loss:.2f} | '\n                  f'Val Acc: {val_acc*100:.2f}%')\n            \ndef accuracy(pred_y, y):\n    \"\"\"Calculate accuracy.\"\"\"\n    return ((pred_y == y).sum() / len(y)).item()\n\n@torch.no_grad()\ndef test(model, data):\n    \"\"\"Evaluate the model on test set and print the accuracy score.\"\"\"\n    model.eval()\n    out = model(data.x, data.edge_index)\n    acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n    return acc\n\nWith GraphSAGE, we loop through batches (our four subgraphs) created by the neighbor sampling process. The way we calculate the accuracy and the validation loss is also different because of that.\n\n%%time\n\n# Create GraphSAGE\ngraphsage = GraphSAGE(dataset.num_features, 64, dataset.num_classes)\nprint(graphsage)\n\n# Train\ngraphsage.fit(data, 200)\n\n# Test\nprint(f'\\nGraphSAGE test accuracy: {test(graphsage, data)*100:.2f}%\\n')\n\nGraphSAGE(\n  (sage1): SAGEConv(500, 64)\n  (sage2): SAGEConv(64, 3)\n)\nEpoch   0 | Train Loss: 0.332 | Train Acc:  30.24% | Val Loss: 1.13 | Val Acc: 18.33%\nEpoch  10 | Train Loss: 0.020 | Train Acc: 100.00% | Val Loss: 0.63 | Val Acc: 72.50%\nEpoch  20 | Train Loss: 0.005 | Train Acc: 100.00% | Val Loss: 0.57 | Val Acc: 73.17%\nEpoch  30 | Train Loss: 0.005 | Train Acc: 100.00% | Val Loss: 0.49 | Val Acc: 79.96%\nEpoch  40 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.63 | Val Acc: 63.33%\nEpoch  50 | Train Loss: 0.009 | Train Acc: 100.00% | Val Loss: 0.61 | Val Acc: 75.56%\nEpoch  60 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 0.77 | Val Acc: 71.25%\nEpoch  70 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 0.50 | Val Acc: 79.79%\nEpoch  80 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 0.54 | Val Acc: 76.74%\nEpoch  90 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.65 | Val Acc: 76.74%\nEpoch 100 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.49 | Val Acc: 78.87%\nEpoch 110 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.59 | Val Acc: 78.87%\nEpoch 120 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 0.61 | Val Acc: 73.33%\nEpoch 130 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.74 | Val Acc: 66.67%\nEpoch 140 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.74 | Val Acc: 59.35%\nEpoch 150 | Train Loss: 0.001 | Train Acc: 100.00% | Val Loss: 0.82 | Val Acc: 65.06%\nEpoch 160 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.73 | Val Acc: 65.00%\nEpoch 170 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 0.85 | Val Acc: 67.92%\nEpoch 180 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 0.48 | Val Acc: 81.67%\nEpoch 190 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.50 | Val Acc: 85.83%\nEpoch 200 | Train Loss: 0.001 | Train Acc: 100.00% | Val Loss: 0.52 | Val Acc: 83.54%\n\nGraphSAGE test accuracy: 77.20%\n\nCPU times: user 9.17 s, sys: 370 ms, total: 9.54 s\nWall time: 12.4 s\n\n\n\n%%time\n\n# Create GCN\ngcn = GCN(dataset.num_features, 64, dataset.num_classes)\nprint(gcn)\n\n# Train\ngcn.fit(data, 200)\n\n# Test\nprint(f'\\nGCN test accuracy: {test(gcn, data)*100:.2f}%\\n')\n\nGCN(\n  (gcn1): GCNConv(500, 64)\n  (gcn2): GCNConv(64, 3)\n)\nEpoch   0 | Train Loss: 1.098 | Train Acc:  33.33% | Val Loss: 1.10 | Val Acc: 32.20%\nEpoch  10 | Train Loss: 0.736 | Train Acc:  91.67% | Val Loss: 0.87 | Val Acc: 74.60%\nEpoch  20 | Train Loss: 0.400 | Train Acc:  96.67% | Val Loss: 0.67 | Val Acc: 73.80%\nEpoch  30 | Train Loss: 0.214 | Train Acc:  93.33% | Val Loss: 0.61 | Val Acc: 76.80%\nEpoch  40 | Train Loss: 0.124 | Train Acc: 100.00% | Val Loss: 0.58 | Val Acc: 75.60%\nEpoch  50 | Train Loss: 0.092 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 77.20%\nEpoch  60 | Train Loss: 0.095 | Train Acc: 100.00% | Val Loss: 0.58 | Val Acc: 76.80%\nEpoch  70 | Train Loss: 0.087 | Train Acc: 100.00% | Val Loss: 0.58 | Val Acc: 77.20%\nEpoch  80 | Train Loss: 0.085 | Train Acc: 100.00% | Val Loss: 0.63 | Val Acc: 75.60%\nEpoch  90 | Train Loss: 0.088 | Train Acc:  98.33% | Val Loss: 0.62 | Val Acc: 76.60%\nEpoch 100 | Train Loss: 0.074 | Train Acc:  98.33% | Val Loss: 0.63 | Val Acc: 75.80%\nEpoch 110 | Train Loss: 0.085 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 76.60%\nEpoch 120 | Train Loss: 0.069 | Train Acc: 100.00% | Val Loss: 0.63 | Val Acc: 74.20%\nEpoch 130 | Train Loss: 0.062 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 76.20%\nEpoch 140 | Train Loss: 0.043 | Train Acc: 100.00% | Val Loss: 0.61 | Val Acc: 75.20%\nEpoch 150 | Train Loss: 0.045 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 75.60%\nEpoch 160 | Train Loss: 0.068 | Train Acc: 100.00% | Val Loss: 0.61 | Val Acc: 76.80%\nEpoch 170 | Train Loss: 0.070 | Train Acc: 100.00% | Val Loss: 0.60 | Val Acc: 76.80%\nEpoch 180 | Train Loss: 0.060 | Train Acc: 100.00% | Val Loss: 0.61 | Val Acc: 75.40%\nEpoch 190 | Train Loss: 0.057 | Train Acc: 100.00% | Val Loss: 0.66 | Val Acc: 75.00%\nEpoch 200 | Train Loss: 0.052 | Train Acc: 100.00% | Val Loss: 0.65 | Val Acc: 75.20%\n\nGCN test accuracy: 78.40%\n\nCPU times: user 52.4 s, sys: 606 ms, total: 53 s\nWall time: 52.6 s\n\n\n\n%%time\n\n# Create GAT\ngat = GAT(dataset.num_features, 64, dataset.num_classes)\nprint(gat)\n\n# Train\ngat.fit(data, 200)\n\n# Test\nprint(f'\\nGAT test accuracy: {test(gat, data)*100:.2f}%\\n')\n\nGAT(\n  (gat1): GATv2Conv(500, 64, heads=8)\n  (gat2): GATv2Conv(512, 3, heads=8)\n)\nEpoch   0 | Train Loss: 3.174 | Train Acc:   1.67% | Val Loss: 3.18 | Val Acc: 1.00%\nEpoch  10 | Train Loss: 0.707 | Train Acc:  86.67% | Val Loss: 0.87 | Val Acc: 71.00%\nEpoch  20 | Train Loss: 0.363 | Train Acc:  93.33% | Val Loss: 0.64 | Val Acc: 77.20%\nEpoch  30 | Train Loss: 0.178 | Train Acc:  96.67% | Val Loss: 0.58 | Val Acc: 78.40%\nEpoch  40 | Train Loss: 0.101 | Train Acc: 100.00% | Val Loss: 0.56 | Val Acc: 78.40%\nEpoch  50 | Train Loss: 0.087 | Train Acc: 100.00% | Val Loss: 0.57 | Val Acc: 77.80%\nEpoch  60 | Train Loss: 0.072 | Train Acc: 100.00% | Val Loss: 0.57 | Val Acc: 78.40%\nEpoch  70 | Train Loss: 0.076 | Train Acc: 100.00% | Val Loss: 0.58 | Val Acc: 77.40%\nEpoch  80 | Train Loss: 0.064 | Train Acc: 100.00% | Val Loss: 0.59 | Val Acc: 76.40%\nEpoch  90 | Train Loss: 0.058 | Train Acc: 100.00% | Val Loss: 0.58 | Val Acc: 77.20%\nEpoch 100 | Train Loss: 0.062 | Train Acc: 100.00% | Val Loss: 0.57 | Val Acc: 79.00%\nEpoch 110 | Train Loss: 0.050 | Train Acc: 100.00% | Val Loss: 0.59 | Val Acc: 77.80%\nEpoch 120 | Train Loss: 0.044 | Train Acc: 100.00% | Val Loss: 0.60 | Val Acc: 75.40%\nEpoch 130 | Train Loss: 0.042 | Train Acc: 100.00% | Val Loss: 0.57 | Val Acc: 78.00%\nEpoch 140 | Train Loss: 0.045 | Train Acc: 100.00% | Val Loss: 0.60 | Val Acc: 78.00%\nEpoch 150 | Train Loss: 0.038 | Train Acc: 100.00% | Val Loss: 0.60 | Val Acc: 77.20%\nEpoch 160 | Train Loss: 0.041 | Train Acc: 100.00% | Val Loss: 0.64 | Val Acc: 77.00%\nEpoch 170 | Train Loss: 0.033 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 76.00%\nEpoch 180 | Train Loss: 0.031 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 77.60%\nEpoch 190 | Train Loss: 0.028 | Train Acc: 100.00% | Val Loss: 0.64 | Val Acc: 78.40%\nEpoch 200 | Train Loss: 0.026 | Train Acc: 100.00% | Val Loss: 0.65 | Val Acc: 76.60%\n\nGAT test accuracy: 77.10%\n\nCPU times: user 17min 43s, sys: 9.46 s, total: 17min 53s\nWall time: 18min 7s\n\n\nThe three models obtain similar results in terms of accuracy. We expect the GAT to perform better because its aggregation mechanism is more nuanced, but it‚Äôs not always the case.\nThe real difference is the training time: GraphSAGE is 88 times faster than the GAT and four times faster than the GCN in this example!\nThis is the true benefit of GraphSAGE. While it loses a lot of information by pruning the graph with neighbor sampling, it greatly improves scalability. In turn, it can lead to building larger graphs that can improve accuracy.\n\n\n\nGraphSAGE is a popular framework with many flavors.\nIn this example, we have used GraphSAGE in a transductive setting. We masked information about test nodes during training, but we didn‚Äôt hide their presence in the adjacency matrix. On the contrary, in an inductive setting, the test set is never encountered during training.\nThis difference is essential: an inductive model can calculate embeddings for nodes that have never been seen before. On the other hand, a transductive model has to be re-trained, which can quickly become computationally costly. Thanks to neighbor sampling, GraphSAGE is designed to be an inductive model: it does not require seeing every neighbor to calculate an embedding.\nBesides these two settings, GraphSAGE can be trained in an unsupervised way. In this case, we can‚Äôt use the cross-entropy loss. We have to engineer a loss function that forces nodes that are nearby in the original graph to remain close to each other in the embedding space. Conversely, the same function must ensure that distant nodes in the graph must have distant representations in the embedding space. This is the loss that is presented in GraphSAGE‚Äôs paper.\nPinSAGE and UberEeats‚Äô modified GraphSAGE are also slightly different since we‚Äôre dealing with recommender systems. Their goal is to correctly rank the most relevant items (pins, restaurants) for each user. We don‚Äôt only want to get the closest embeddings, but we also have to produce the best rankings possible. This is why these systems are trained in an unsupervised way but with another loss function: a max-margin ranking loss."
  },
  {
    "objectID": "posts/2022-04-06-GraphSAGE.html#conclusion",
    "href": "posts/2022-04-06-GraphSAGE.html#conclusion",
    "title": "GraphSAGE: Scaling up Graph Neural Networks",
    "section": "Conclusion",
    "text": "Conclusion\nGraphSAGE is an incredibly fast architecture that can process large graphs. It might not be as accurate as a GCN or a GAT, but it is an essential model for handling massive amounts of data. It delivers this speed thanks to a clever combination of neighbor sampling and fast aggregation. In this article,\n\nWe explored a new dataset with PubMed, which has almost ten times more connections than the previous one (CiteSeer)\nWe explained the idea behind neighbor sampling, which only considers a predefined number of random neighbors at each hop\nWe saw the three aggregators presented in GraphSAGE‚Äôs paper and focused on the mean aggregator\nWe benchmarked three models (GraphSAGE, GAT, and GCN) in terms of accuracy and training time\n\nWe saw three architectures with the same end application: node classification. But GNNs have been successfully applied to other tasks. In the next tutorials, I‚Äôd like to use them in two different contexts: graph and edge prediction. This will be a good way to discover new datasets and applications where GNNs dominate the state of the art.\nIf you enjoyed this article, let‚Äôs connect on Twitter @maximelabonne for more graph learning content.\nThanks for your attention! üì£"
  },
  {
    "objectID": "posts/2022-04-06-GraphSAGE.html#graph-neural-network-course",
    "href": "posts/2022-04-06-GraphSAGE.html#graph-neural-network-course",
    "title": "GraphSAGE: Scaling up Graph Neural Networks",
    "section": "üåê Graph Neural Network Course",
    "text": "üåê Graph Neural Network Course\nüîé Course overview\nüìù Chapter 1: Introduction to Graph Neural Networks\nüìù Chapter 2: Graph Attention Network\nüìù Chapter 3: GraphSAGE\nüìù Chapter 4: Graph Isomorphism Network"
  },
  {
    "objectID": "posts/2022-04-25-Graph_Isomorphism_Network.html",
    "href": "posts/2022-04-25-Graph_Isomorphism_Network.html",
    "title": "GIN: How to Design the Most Powerful Graph Neural Network",
    "section": "",
    "text": "Graph Neural Networks are not limited to classifying nodes.\nOne of the most popular applications is graph classification. This is a common task when dealing with molecules: they are represented as graphs and features about each atom (node) can be used to predict the behavior of the entire molecule.\nHowever, GNNs only learn node embeddings. How to combine them in order to produce an entire graph embedding? In this article, we will:\nWe‚Äôll detail the advantages of GIN in terms of discriminative power compared to a GCN or GraphSAGE, and its connection to the Weisfeiler-Lehman test. Beyond its powerful aggregator, GIN brings exciting takeaways about GNNs in general.\nYou can run the code with the following Google Colab notebook.\n# Install PyTorch Geometric\nimport torch\n!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n\n# Visualization\nimport networkx as nx\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.dpi'] = 300\nplt.rcParams.update({'font.size': 24})"
  },
  {
    "objectID": "posts/2022-04-25-Graph_Isomorphism_Network.html#i.-proteins-dataset",
    "href": "posts/2022-04-25-Graph_Isomorphism_Network.html#i.-proteins-dataset",
    "title": "GIN: How to Design the Most Powerful Graph Neural Network",
    "section": "üåê I. PROTEINS dataset",
    "text": "üåê I. PROTEINS dataset\n\n 3D plot of a protein by DeepMind.\n\n\nPROTEINS is a popular dataset in bioinformatics. It is a collection of 1113 graphs representing proteins, where nodes are amino acids. Two nodes are connected by an edge when they are close enough (&lt; 0.6 nanometers). The goal is to classify each protein as an enzyme or not.\nEnzymes are a particular type of proteins that act as catalysts to speed up chemical reactions in the cell. They are essential for digestion (e.g., lipases), respiration (e.g., oxidases), and other crucial functions of the human body. They are also used in commercial applications, like the production of antibiotics.\nThis dataset is also available on TUDataset and implemented in PyTorch Geometric.\n\nfrom torch_geometric.datasets import TUDataset\n\ndataset = TUDataset(root='.', name='PROTEINS').shuffle()\n\n# Print information about the dataset\nprint(f'Dataset: {dataset}')\nprint('-------------------')\nprint(f'Number of graphs: {len(dataset)}')\nprint(f'Number of nodes: {dataset[0].x.shape[0]}')\nprint(f'Number of features: {dataset.num_features}')\nprint(f'Number of classes: {dataset.num_classes}')\n\nDataset: PROTEINS(1113)\n-------------------\nNumber of graphs: 1113\nNumber of nodes: 117\nNumber of features: 3\nNumber of classes: 2\n\n\nI‚Äôm not a biochemist so I‚Äôm curious about these proteins. Let‚Äôs plot one as a graph to see what it looks like.\n\nfrom torch_geometric.utils import to_networkx\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\n\nG = to_networkx(dataset[2], to_undirected=True)\n\n# 3D spring layout\npos = nx.spring_layout(G, dim=3, seed=0)\n\n# Extract node and edge positions from the layout\nnode_xyz = np.array([pos[v] for v in sorted(G)])\nedge_xyz = np.array([(pos[u], pos[v]) for u, v in G.edges()])\n\n# Create the 3D figure\nfig = plt.figure(figsize=(16,16))\nax = fig.add_subplot(111, projection=\"3d\")\n\n# Suppress tick labels\nfor dim in (ax.xaxis, ax.yaxis, ax.zaxis):\n    dim.set_ticks([])\n\n# Plot the nodes - alpha is scaled by \"depth\" automatically\nax.scatter(*node_xyz.T, s=500, c=\"#0A047A\")\n\n# Plot the edges\nfor vizedge in edge_xyz:\n    ax.plot(*vizedge.T, color=\"tab:gray\")\n\n# fig.tight_layout()\nplt.show()\n\n\n\n\nThe previous 3D structure is randomly generated: obtaining the correct 3D representation is a problem so difficult it‚Äôs the whole point of AlphaFold.\nGraphs are not the only way to represent molecules. The simplified molecular-input line-entry system (SMILES) is another popular method, which uses a line (string) notation. It is obtained by printing the nodes encountered in a depth-first tree traversal of a slightly modified molecular graph.\nResearchers often use this representation when working with molecules or chemical compounds. Fortunately for us, the PROTEINS dataset is already encoded in the form of graphs. Otherwise, we could have to translate the SMILES strings into networkx graphs.\nIt doesn‚Äôt mean we‚Äôll directly feed the PROTEINS dataset to our GNN. If GraphSAGE taught us anything, it‚Äôs that mini-batching is incredibly efficient. It is now an indispensable tool whenever we implement a GNN.\n\nfrom torch_geometric.loader import DataLoader\n\n# Create training, validation, and test sets\ntrain_dataset = dataset[:int(len(dataset)*0.8)]\nval_dataset   = dataset[int(len(dataset)*0.8):int(len(dataset)*0.9)]\ntest_dataset  = dataset[int(len(dataset)*0.9):]\n\nprint(f'Training set   = {len(train_dataset)} graphs')\nprint(f'Validation set = {len(val_dataset)} graphs')\nprint(f'Test set       = {len(test_dataset)} graphs')\n\n# Create mini-batches\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\nprint('\\nTrain loader:')\nfor i, subgraph in enumerate(train_loader):\n    print(f' - Subgraph {i}: {subgraph}')\n\nprint('\\nValidation loader:')\nfor i, subgraph in enumerate(val_loader):\n    print(f' - Subgraph {i}: {subgraph}')\n\nprint('\\nTest loader:')\nfor i, subgraph in enumerate(test_loader):\n    print(f' - Subgraph {i}: {subgraph}')\n\nTraining set   = 890 graphs\nValidation set = 111 graphs\nTest set       = 112 graphs\n\nTrain loader:\n - Subgraph 0: DataBatch(edge_index=[2, 7966], x=[2114, 3], y=[64], batch=[2114], ptr=[65])\n - Subgraph 1: DataBatch(edge_index=[2, 8492], x=[2263, 3], y=[64], batch=[2263], ptr=[65])\n - Subgraph 2: DataBatch(edge_index=[2, 9518], x=[2589, 3], y=[64], batch=[2589], ptr=[65])\n - Subgraph 3: DataBatch(edge_index=[2, 10846], x=[3008, 3], y=[64], batch=[3008], ptr=[65])\n - Subgraph 4: DataBatch(edge_index=[2, 9618], x=[2586, 3], y=[64], batch=[2586], ptr=[65])\n - Subgraph 5: DataBatch(edge_index=[2, 7572], x=[2027, 3], y=[64], batch=[2027], ptr=[65])\n - Subgraph 6: DataBatch(edge_index=[2, 10512], x=[2875, 3], y=[64], batch=[2875], ptr=[65])\n - Subgraph 7: DataBatch(edge_index=[2, 7034], x=[1855, 3], y=[64], batch=[1855], ptr=[65])\n - Subgraph 8: DataBatch(edge_index=[2, 11966], x=[3313, 3], y=[64], batch=[3313], ptr=[65])\n - Subgraph 9: DataBatch(edge_index=[2, 9898], x=[2764, 3], y=[64], batch=[2764], ptr=[65])\n - Subgraph 10: DataBatch(edge_index=[2, 8798], x=[2411, 3], y=[64], batch=[2411], ptr=[65])\n - Subgraph 11: DataBatch(edge_index=[2, 9922], x=[2736, 3], y=[64], batch=[2736], ptr=[65])\n - Subgraph 12: DataBatch(edge_index=[2, 10772], x=[2787, 3], y=[64], batch=[2787], ptr=[65])\n - Subgraph 13: DataBatch(edge_index=[2, 11140], x=[2782, 3], y=[58], batch=[2782], ptr=[59])\n\nValidation loader:\n - Subgraph 0: DataBatch(edge_index=[2, 8240], x=[2088, 3], y=[64], batch=[2088], ptr=[65])\n - Subgraph 1: DataBatch(edge_index=[2, 5626], x=[1503, 3], y=[47], batch=[1503], ptr=[48])\n\nTest loader:\n - Subgraph 0: DataBatch(edge_index=[2, 7946], x=[2156, 3], y=[64], batch=[2156], ptr=[65])\n - Subgraph 1: DataBatch(edge_index=[2, 6222], x=[1614, 3], y=[48], batch=[1614], ptr=[49])\n\n\nPROTEINS is not a huge dataset, but mini-batching will speed up the training nonetheless. We could use a GCN or a GAT, but there‚Äôs a new architecture I‚Äôd like to introduce: the Graph Isomorphism Network."
  },
  {
    "objectID": "posts/2022-04-25-Graph_Isomorphism_Network.html#ii.-graph-isomorphism-network-gin",
    "href": "posts/2022-04-25-Graph_Isomorphism_Network.html#ii.-graph-isomorphism-network-gin",
    "title": "GIN: How to Design the Most Powerful Graph Neural Network",
    "section": "üçæ II. Graph Isomorphism Network (GIN)",
    "text": "üçæ II. Graph Isomorphism Network (GIN)\nGIN was designed by researchers trying to maximize the representational (or discriminative) power of a GNN. But how do you define a ‚Äúrepresentational power‚Äù?\n\nA. Weisfeiler-Lehman test\nA way to characterize the ‚Äúpower‚Äù of a GNN is to use the Weisfeiler-Lehman (WL) graph isomorphism test. Isomorphic graphs mean that they have the same structure: identical connections but a permutation of nodes. The WL test is able to tell if two graphs are non-isomorphic, but it cannot guarantee that they are isomorphic.\n\n Two isomorphic graphs.\n\n\nThis might not seem like much, but it can be extremely difficult to tell two large graphs apart. In fact, this problem is not known to be solvable in polynomial time, nor to be NP-complete. It might even be somewhere in between, in the computational complexity class NP-intermediate (if it only exists).\nOkay, but how is it related to GNNs? Some researchers in graph learning noticed that this test and the way GNNs learn are oddly similar. In the WL test,\n\nEvery node starts with the same label;\nLabels from neighboring nodes are aggregated and hashed to produce a new label;\nThe previous step is repeated until the labels stop changing.\n\nIf you‚Äôre interested in the WL test, I would recommend this blog post by David Bieber and this article by Michael Bronstein.\nNot only this test is similar to how feature vectors are aggregated in GNNs, but its ability to tell graphs apart makes it more powerful than a lot of architectures, including GCNs and GraphSAGE. This is what inspired Xu et al. to design a new aggregator that is proven to be as good as the WL test.\n\n\nB. One aggregator to rule them all\nTo be as good as the WL test, this new aggregator must produce different node embeddings when dealing with non-isomorphic graphs.\nWe‚Äôll skip the math-heavy part of the paper, but the solution they found is to use two injective functions. Which ones? We don‚Äôt know, we can just learn them with a MLP!\n\nWith GATs, we used a neural network to learn the best weighting factors for a given task;\nWith GINs, we now learn the approximation of two injective functions thanks to the Universal Approximation Theorem.\n\nHere‚Äôs how to calculate the hidden vector of a particular node i with GIN:\nh_i = MLP\\bigg((1+…õ) \\cdot x_i + \\sum_{j \\in \\mathcal{N}_i}x_j\\bigg)\nIn this formula, …õ determines the importance of the target node compared to its neighbors (it has the same importance if …õ = 0). It can be a learnable parameter or a fixed scalar.\nNote that we talk about MLPs to highlight the fact there is more than one layer. According to the authors, one layer is not sufficient for graph learning in general.\n\n\nC. Global pooling\nGlobal pooling or graph-level readout consists of producing a graph embedding using the node embeddings calculated by the GNN.\nA simple way to obtain a graph embedding h_G is to use the mean, sum, or max of every node embedding h_i:\n\\text{Mean}: h_G = \\frac{1}{N} \\sum_{i=0}^N h_i \\\\\n\\text{Sum}: h_G = \\sum_{i=0}^N h_i \\\\\n\\text{Max}: h_G = {max}_{i=0}^N(h_i)\nThe authors make two important points about graph-level readout:\n\nTo consider all structural information, it is necessary to keep embeddings from previous layers;\nThe sum operator is surprisingly more expressive than the mean and the max.\n\nThese observations lead them to propose the following global pooling method:\nh_G = \\sum_{i=0}^N{h_i^0}\\ || \\ \\dots \\ || \\ \\sum_{i=0}^N{h_i^k}\nFor each layer, embeddings nodes are summed and the result is concatenated. This solution combines the expressiveness of the sum operator with the memory of previous iterations from the concatenation."
  },
  {
    "objectID": "posts/2022-04-25-Graph_Isomorphism_Network.html#iii.-gin-in-pytorch-geometric",
    "href": "posts/2022-04-25-Graph_Isomorphism_Network.html#iii.-gin-in-pytorch-geometric",
    "title": "GIN: How to Design the Most Powerful Graph Neural Network",
    "section": "üß† III. GIN in PyTorch Geometric",
    "text": "üß† III. GIN in PyTorch Geometric\nIt is always interesting to see the differences between the original design and its implementations.\nThere is a GINConv layer in PyTorch Geometric with different parameters:\n\nnn: the MLP that is used to approximate our two injective functions;\neps: the initial value of …õ, which is 0 by default;\ntrain_eps: a True/False statement to determine if …õ is trainable, which is False by default.\n\nYou can see that …õ is entirely removed by default in this implementation: it‚Äôs a hyperparameter we can tune, but probably not an essential one.\nThere is a second GIN layer in PyTorch Geometric, called GINEConv. It comes from this paper‚Äôs implementation of GIN, which applies a ReLU function to the neighbors‚Äô features. We won‚Äôt use it in this tutorial, since the benefits are not clear.\nWe still need to design a MLP for the GINConv layer. Here‚Äôs the design we‚Äôll implement, inspired by the original paper:\n\n\n\nThe paper stacks 5 layers but we‚Äôll be more humble with 3 layers instead. Here is what the entire architecture looks like:\n\n\n\nI could not find any implementation of GIN with graph embedding concatenation, so here is my version (it improves the accuracy by 1% on average). Let‚Äôs compare it to a GCN with a simple mean pooling (and no concatenation).\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\nfrom torch_geometric.nn import GCNConv, GINConv\nfrom torch_geometric.nn import global_mean_pool, global_add_pool\n\n\nclass GCN(torch.nn.Module):\n    \"\"\"GCN\"\"\"\n    def __init__(self, dim_h):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(dataset.num_node_features, dim_h)\n        self.conv2 = GCNConv(dim_h, dim_h)\n        self.conv3 = GCNConv(dim_h, dim_h)\n        self.lin = Linear(dim_h, dataset.num_classes)\n\n    def forward(self, x, edge_index, batch):\n        # Node embeddings \n        h = self.conv1(x, edge_index)\n        h = h.relu()\n        h = self.conv2(h, edge_index)\n        h = h.relu()\n        h = self.conv3(h, edge_index)\n\n        # Graph-level readout\n        hG = global_mean_pool(h, batch)\n\n        # Classifier\n        h = F.dropout(hG, p=0.5, training=self.training)\n        h = self.lin(h)\n        \n        return hG, F.log_softmax(h, dim=1)\n\nclass GIN(torch.nn.Module):\n    \"\"\"GIN\"\"\"\n    def __init__(self, dim_h):\n        super(GIN, self).__init__()\n        self.conv1 = GINConv(\n            Sequential(Linear(dataset.num_node_features, dim_h),\n                       BatchNorm1d(dim_h), ReLU(),\n                       Linear(dim_h, dim_h), ReLU()))\n        self.conv2 = GINConv(\n            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n                       Linear(dim_h, dim_h), ReLU()))\n        self.conv3 = GINConv(\n            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n                       Linear(dim_h, dim_h), ReLU()))\n        self.lin1 = Linear(dim_h*3, dim_h*3)\n        self.lin2 = Linear(dim_h*3, dataset.num_classes)\n\n    def forward(self, x, edge_index, batch):\n        # Node embeddings \n        h1 = self.conv1(x, edge_index)\n        h2 = self.conv2(h1, edge_index)\n        h3 = self.conv3(h2, edge_index)\n\n        # Graph-level readout\n        h1 = global_add_pool(h1, batch)\n        h2 = global_add_pool(h2, batch)\n        h3 = global_add_pool(h3, batch)\n\n        # Concatenate graph embeddings\n        h = torch.cat((h1, h2, h3), dim=1)\n\n        # Classifier\n        h = self.lin1(h)\n        h = h.relu()\n        h = F.dropout(h, p=0.5, training=self.training)\n        h = self.lin2(h)\n        \n        return h, F.log_softmax(h, dim=1)\n\ngcn = GCN(dim_h=32)\ngin = GIN(dim_h=32)\n\n\ndef train(model, loader):\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(),\n                                      lr=0.01,\n                                      weight_decay=0.01)\n    epochs = 100\n\n    model.train()\n    for epoch in range(epochs+1):\n        total_loss = 0\n        acc = 0\n        val_loss = 0\n        val_acc = 0\n\n        # Train on batches\n        for data in loader:\n          optimizer.zero_grad()\n          _, out = model(data.x, data.edge_index, data.batch)\n          loss = criterion(out, data.y)\n          total_loss += loss / len(loader)\n          acc += accuracy(out.argmax(dim=1), data.y) / len(loader)\n          loss.backward()\n          optimizer.step()\n\n          # Validation\n          val_loss, val_acc = test(model, val_loader)\n\n    # Print metrics every 10 epochs\n    if(epoch % 10 == 0):\n        print(f'Epoch {epoch:&gt;3} | Train Loss: {total_loss:.2f} '\n              f'| Train Acc: {acc*100:&gt;5.2f}% '\n              f'| Val Loss: {val_loss:.2f} '\n              f'| Val Acc: {val_acc*100:.2f}%')\n          \n    test_loss, test_acc = test(model, test_loader)\n    print(f'Test Loss: {test_loss:.2f} | Test Acc: {test_acc*100:.2f}%')\n    \n    return model\n\n@torch.no_grad()\ndef test(model, loader):\n    criterion = torch.nn.CrossEntropyLoss()\n    model.eval()\n    loss = 0\n    acc = 0\n\n    for data in loader:\n        _, out = model(data.x, data.edge_index, data.batch)\n        loss += criterion(out, data.y) / len(loader)\n        acc += accuracy(out.argmax(dim=1), data.y) / len(loader)\n\n    return loss, acc\n\ndef accuracy(pred_y, y):\n    \"\"\"Calculate accuracy.\"\"\"\n    return ((pred_y == y).sum() / len(y)).item()\n\ngcn = train(gcn, train_loader)\ngin = train(gin, train_loader)\n\nEpoch 100 | Train Loss: 0.67 | Train Acc: 60.61% | Val Loss: 0.70 | Val Acc: 54.50%\nTest Loss: 0.69 | Test Acc: 55.99%\nEpoch 100 | Train Loss: 0.49 | Train Acc: 75.61% | Val Loss: 0.53 | Val Acc: 78.99%\nTest Loss: 0.60 | Test Acc: 66.93%\n\n\nThis time, there‚Äôs no competition!\nThe GIN architecture completely outperforms the GCN. This gap (10% accuracy on average) is due to several reasons:\n\nGIN‚Äôs aggregator is specifically designed to discriminate graphs that the GCN‚Äôs aggregator cannot;\nGraph hidden vectors from every layer are concatenated instead of only considering the last one;\nThe sum operator is superior to the mean operator (at least in theory).\n\nLet‚Äôs visualize the proteins we classified with the GCN and the GIN.\n\nfig, ax = plt.subplots(4, 4, figsize=(16,16))\nfig.suptitle('GCN - Graph classification')\n\nfor i, data in enumerate(dataset[1113-16:]):\n    # Calculate color (green if correct, red otherwise)\n    _, out = gcn(data.x, data.edge_index, data.batch)\n    color = \"green\" if out.argmax(dim=1) == data.y else \"red\"\n\n    # Plot graph\n    ix = np.unravel_index(i, ax.shape)\n    ax[ix].axis('off')\n    G = to_networkx(dataset[i], to_undirected=True)\n    nx.draw_networkx(G,\n                    pos=nx.spring_layout(G, seed=0),\n                    with_labels=False,\n                    node_size=150,\n                    node_color=color,\n                    width=0.8,\n                    ax=ax[ix]\n                    )\n\n\n\n\n\nfig, ax = plt.subplots(4, 4, figsize=(16,16))\nfig.suptitle('GIN - Graph classification')\n\nfor i, data in enumerate(dataset[1113-16:]):\n    # Calculate color (green if correct, red otherwise)\n    _, out = gin(data.x, data.edge_index, data.batch)\n    color = \"green\" if out.argmax(dim=1) == data.y else \"red\"\n\n    # Plot graph\n    ix = np.unravel_index(i, ax.shape)\n    ax[ix].axis('off')\n    G = to_networkx(dataset[i], to_undirected=True)\n    nx.draw_networkx(G,\n                    pos=nx.spring_layout(G, seed=0),\n                    with_labels=False,\n                    node_size=150,\n                    node_color=color,\n                    width=0.8,\n                    ax=ax[ix]\n                    )\n\n\n\n\nInterestingly enough, the two models make different mistakes. This is a common result in machine learning when different algorithms are applied to the same problem.\nWe can take advantage of this behavior by creating an ensemble. There are many ways of combining our graph embeddings. The simplest method is to take the mean of the normalized output vectors.\n\ngcn.eval()\ngin.eval()\nacc_gcn = 0\nacc_gin = 0\nacc = 0\n\nfor data in test_loader:\n    # Get classifications\n    _, out_gcn = gcn(data.x, data.edge_index, data.batch)\n    _, out_gin = gin(data.x, data.edge_index, data.batch)\n    out = (out_gcn + out_gin)/2\n\n    # Calculate accuracy scores\n    acc_gcn += accuracy(out_gcn.argmax(dim=1), data.y) / len(test_loader)\n    acc_gin += accuracy(out_gin.argmax(dim=1), data.y) / len(test_loader)\n    acc += accuracy(out.argmax(dim=1), data.y) / len(test_loader)\n\n# Print results\nprint(f'GCN accuracy:     {acc_gcn*100:.2f}%')\nprint(f'GIN accuracy:     {acc_gin*100:.2f}%')\nprint(f'GCN+GIN accuracy: {acc*100:.2f}%')\n\nGCN accuracy:     55.99%\nGIN accuracy:     66.93%\nGCN+GIN accuracy: 67.45%\n\n\nThis time, we‚Äôre lucky enough to see the accuracy improved.\nObviously, it‚Äôs not always the case. More sophisticated methods involve building an entirely different ML algorithm for classification, such as a Random Forest. This classifier takes graph embeddings as inputs and outputs the final classification."
  },
  {
    "objectID": "posts/2022-04-25-Graph_Isomorphism_Network.html#conclusion",
    "href": "posts/2022-04-25-Graph_Isomorphism_Network.html#conclusion",
    "title": "GIN: How to Design the Most Powerful Graph Neural Network",
    "section": "Conclusion",
    "text": "Conclusion\nGraph Isomorphism Networks are an important step in the understanding of GNNs.\nThey not only improve the accuracy scores on several benchmarks but also provide a theoretical framework to explain why one architecture is better than another. In this article,\n\nWe saw a new task with graph classification, performed with global pooling;\nWe introduced the WL test and its connection with the new GIN layer;\nWe implemented a GIN and a GCN and made an simple ensemble with their classifications.\n\nAlthough GINs achieve good performance, especially with social graphs, their theoretical superiority doesn‚Äôt always translate well in the real world. It is true with other ‚Äúprovably powerful‚Äù architectures, which tend to underperform in practice, such as the 3WLGNN.\nIf you enjoyed this article, please leave a few claps and follow me on Twitter for more graph content! üì£"
  },
  {
    "objectID": "posts/2022-04-25-Graph_Isomorphism_Network.html#graph-neural-network-course",
    "href": "posts/2022-04-25-Graph_Isomorphism_Network.html#graph-neural-network-course",
    "title": "GIN: How to Design the Most Powerful Graph Neural Network",
    "section": "üåê Graph Neural Network Course",
    "text": "üåê Graph Neural Network Course\nüîé Course overview\nüìù Chapter 1: Introduction to Graph Neural Networks\nüìù Chapter 2: Graph Attention Network\nüìù Chapter 3: GraphSAGE\nüìù Chapter 4: Graph Isomorphism Network"
  },
  {
    "objectID": "posts/2022-05-02-Constraint_Programming.html",
    "href": "posts/2022-05-02-Constraint_Programming.html",
    "title": "Introduction to Constraint Programming in Python",
    "section": "",
    "text": "Constraint Programming is a technique to find every solution that respects a set of predefined constraints.\nIt is an invaluable tool for data scientists to solve a huge variety of problems, such as scheduling, timetabling, sequencing, etc. In this article, we‚Äôll see how to use CP in two different ways:\nWe‚Äôll use CP-SAT from Google OR-Tools, an excellent free and open source CP solver. Note that it is different from MPSolver, which is dedicated to Linear and Mixed Integer Programming. The difference between CP and LP is quite confusing, we‚Äôll touch on this topic at the end of the article.\nYou can run the code with the following Google Colab notebook.\n!python -m pip install --upgrade --user -q ortools"
  },
  {
    "objectID": "posts/2022-05-02-Constraint_Programming.html#i.-satisfiability-with-the-3-scouts-problem",
    "href": "posts/2022-05-02-Constraint_Programming.html#i.-satisfiability-with-the-3-scouts-problem",
    "title": "Introduction to Constraint Programming in Python",
    "section": "ü™ñ I. Satisfiability with the 3 scouts problem",
    "text": "ü™ñ I. Satisfiability with the 3 scouts problem\n\n\n\nIn the previous article, we created an army to defeat our opponent. But there was one small problem: we had to guess how powerful his army was.\nThis time, let‚Äôs send scouts to know the exact number. Our 3 scouts observed the enemy camp, and this is what they tell us:\n\nScout 1: ‚Äúthe number of soldiers is a multiple of 13‚Äù;\nScout 2: ‚Äúthe number of soldiers is a multiple of 19‚Äù;\nScout 3: ‚Äúthe number of soldiers is a multiple of 37‚Äù;\nThey all agree that the number of soldiers doesn‚Äôt exceed 10,000.\n\nOur scouts have a personal way of counting soldiers, but we can combine these three observations to make a model.\nLet‚Äôs call the number of soldiers army. We can translate our problem into the following congruence system:\n\n    army \\equiv 0 \\mod 13 \\\\\n    army \\equiv 0 \\mod 19 \\\\\n    army \\equiv 0 \\mod 37\n\nIf you‚Äôre not familiar with this notation, this is what it means in programming terms:\n\n    army\\ \\% \\ 13 = 0 \\\\\n    army\\ \\% \\ 19 = 0 \\\\\n    army\\ \\% \\ 37 = 0\n\nLet‚Äôs implement it with OR-Tools. The first thing we need to do is to import and create the CP-SAT model and solver.\n\nfrom ortools.sat.python import cp_model\n\n# Instantiate model and solver\nmodel = cp_model.CpModel()\nsolver = cp_model.CpSolver()\n\nThe modeling process is very similar to what we did in Linear Programming.\nThe first step to create our CP model is to declare the variables. In this example, we only have one: army, the number of soldiers.\nWe have to give lower and upper bounds. The lower bound is 1 since we know there‚Äôs an army, and the upper bound is 10,000 according to the scouts:\n1 \\leq army \\leq 10\\ 000\nIn OR-Tools, we use the NewIntVar method to create this variable.\n\n# 1. Variable\narmy = model.NewIntVar(1, 10000, 'army')\n\nThe second step is to declare the constraints.\nWe identified three constraints in this example. Modulo is a special operator, so we need a specific function to handle it with CP-SAT: AddModuloEquality. You can find a reference guide at this address if you need other methods.\n\n# 2. Constraints\n# variable % mod = target ‚Üí (target, variable, mod)\nmodel.AddModuloEquality(0, army, 13)\nmodel.AddModuloEquality(0, army, 19)\nmodel.AddModuloEquality(0, army, 37)\n\nUnlike Linear Programming, we don‚Äôt have to define an objective function here.\nThe reason is simple: there is nothing to optimize! We just want to find a feasible solution that satisfies our constraints, but there is no ‚Äúgood‚Äù or ‚Äúbad‚Äù answers. This is a key feature of Constraint Programming.\nOur model is complete, we can now ask OR-Tools to solve it.\n\n# Find the variable that satisfies these constraints\nstatus = solver.Solve(model)\n\n# If a solution has been found, print results\nif status == cp_model.OPTIMAL or status == cp_model.FEASIBLE:\n    print('================= Solution =================')\n    print(f'Solved in {solver.WallTime():.2f} milliseconds')\n    print()\n    print(f'ü™ñ Army = {solver.Value(army)}')\n    print()\n    print('Check solution:')\n    print(f' - Constraint 1: {solver.Value(army)} % 13 = {solver.Value(army) % 13}')\n    print(f' - Constraint 2: {solver.Value(army)} % 19 = {solver.Value(army) % 19}')\n    print(f' - Constraint 3: {solver.Value(army)} % 37 = {solver.Value(army) % 37}')\n\nelse:\n    print('The solver could not find a solution.')\n\n================= Solution =================\nSolved in 0.01 milliseconds\n\nü™ñ Army = 9139\n\nCheck solution:\n - Constraint 1: 9139 % 13 = 0\n - Constraint 2: 9139 % 19 = 0\n - Constraint 3: 9139 % 37 = 0\n\n\nWe obtained our solution in less than a millisecond: there are 9,139 soldiers in the enemy army.\nWe limited the search space with an upper bound of 10,000, which gave us a unique solution. But is it still the case if we push this limit?\nAnother perk of CP is the ability to find every possible solution to a problem. This might take a long time when the search space is large because the solver has to brute force the entire space (instead of reducing it with heuristics). Let‚Äôs explore this feature by printing every possible solution with a new upper bound of 100,000.\nWith OR-Tools, we ask the solver to look for every possible solution thanks to the enumerate_all_solutions parameter. We then assign it a callback class that prints every solution the solver finds.\n\nmodel = cp_model.CpModel()\nsolver = cp_model.CpSolver()\n\n# 1. Variable\narmy = model.NewIntVar(1, 100000, 'army')\n\n# 2. Constraints\nmodel.AddModuloEquality(0, army, 13)\nmodel.AddModuloEquality(0, army, 19)\nmodel.AddModuloEquality(0, army, 37)\n\n\nclass PrintSolutions(cp_model.CpSolverSolutionCallback):\n    \"\"\"Callback to print every solution.\"\"\"\n\n    def __init__(self, variable):\n        cp_model.CpSolverSolutionCallback.__init__(self)\n        self.__variable = variable\n\n    def on_solution_callback(self):\n        print(self.Value(self.__variable))\n\n# Solve with callback\nsolution_printer = PrintSolutions(army)\nsolver.parameters.enumerate_all_solutions = True\nstatus = solver.Solve(model, solution_printer)\n\n9139\n18278\n27417\n36556\n45695\n54834\n63973\n73112\n82251\n91390\n\n\nWe found 10 solutions! This was to be expected since we increased the upper bound tenfold: these solutions all are multiples of 9,139.\nAs you can see, this example has nothing to do with optimization: it‚Äôs a pure satisfiability problem. On another note, this congruence system can be solved manually with the Chinese remainder theorem. But CP is not limited to that‚Ä¶"
  },
  {
    "objectID": "posts/2022-05-02-Constraint_Programming.html#ii.-optimization-and-beer",
    "href": "posts/2022-05-02-Constraint_Programming.html#ii.-optimization-and-beer",
    "title": "Introduction to Constraint Programming in Python",
    "section": "üçª II. Optimization and beer",
    "text": "üçª II. Optimization and beer\n\n\n\nLet‚Äôs see another problem: our army will face the enemy in a few days. In the meantime, the quartermaster has to prepare the rations that will be used during the campaign.\nThe space in the supply wagons is limited and some rations are more popular than others. There are three possible rations:\n\nü•ñ Bread: it takes only 1 space but soldiers don‚Äôt like it that much with a popularity of 3;\nü•© Meat: it takes 3 spaces and has a popularity of 10;\nüç∫ Beer: it takes 7 spaces but soldiers love it with a popularity of 26.\n\n\n\n\nThe supply wagons have a capacity of 19 spaces. How to select the best rations to maximize the popularity?\nThis is an optimization problem we‚Äôve already seen: actually, it is a variant of the famous knapsack problem. We could reuse the code from the previous article and just change the input parameters.\nThis time, we‚Äôll solve it using Constraint Programming. This paradigm is not limited to finding feasible solutions. It can also perform optimization using different algorithms to handle this overhead.\nLet‚Äôs create a model of the problem. First of all, we have to declare three variables: ü•ñbread, ü•©meat, and üç∫beer. It‚Äôs possible to have 0 of them, but their number cannot exceed the maximal capacity.\n0 \\leq bread \\leq capacity \\\\\n0 \\leq meat \\leq capacity \\\\\n0 \\leq beer \\leq capacity\n\n# Instantiate model and solver\nmodel = cp_model.CpModel()\nsolver = cp_model.CpSolver()\n\n# 1. Variables\ncapacity = 19\nbread = model.NewIntVar(0, capacity, 'bread')\nmeat  = model.NewIntVar(0, capacity, 'meat')\nbeer  = model.NewIntVar(0, capacity, 'beer')\n\nThis time, we only have one constraint: the space occupied by the bread, the meat, and the beer cannot exceed the wagons‚Äô capacity (19).\n1 \\times bread + 3 \\times meat + 7 \\times beer \\leq 19\n\n# 2. Constraints\nmodel.Add(1 * bread\n        + 3 * meat \n        + 7 * beer &lt;= capacity)\n\nWe want to maximize the total popularity of the rations that are selected:\nmax\\ 3 \\times bread + 10 \\times meat + 26 \\times beer\n\n# 3. Objective\nmodel.Maximize(3  * bread\n             + 10 * meat\n             + 26 * beer)\n\nThe model is complete, CP-SAT can solve the problem!\n\n# Solve problem\nstatus = solver.Solve(model)\n\n# If an optimal solution has been found, print results\nif status == cp_model.OPTIMAL:\n    print('================= Solution =================')\n    print(f'Solved in {solver.WallTime():.2f} milliseconds')\n    print()\n    print(f'Optimal value = {3*solver.Value(bread)+10*solver.Value(meat)+26*solver.Value(beer)} popularity')\n    print('Food:')\n    print(f' - ü•ñBread = {solver.Value(bread)}')\n    print(f' - ü•©Meat  = {solver.Value(meat)}')\n    print(f' - üç∫Beer  = {solver.Value(beer)}')\nelse:\n    print('The solver could not find an optimal solution.')\n\n================= Solution =================\nSolved in 0.01 milliseconds\n\nOptimal value = 68 popularity\nFood:\n - ü•ñBread = 2\n - ü•©Meat  = 1\n - üç∫Beer  = 2\n\n\nWe obtained the highest popularity (68) possible with a capacity of 19.\nIs the constraint respected? Let‚Äôs quickly check it: 1√ó2ü•ñ+3√ó1ü•©+7√ó2üç∫ = 19, which is indeed ‚â§ 19.\nOkay, I‚Äôd like to ask another question: how many solutions to this problem are there? Once again, we can answer it with a specific callback to count them.\n\nclass CountSolutions(cp_model.CpSolverSolutionCallback):\n    \"\"\"Count the number of solutions.\"\"\"\n\n    def __init__(self):\n        cp_model.CpSolverSolutionCallback.__init__(self)\n        self.__solution_count = 0\n\n    def on_solution_callback(self):\n        self.__solution_count += 1\n\n    def solution_count(self):\n        return self.__solution_count\n\nsolution_printer = CountSolutions()\n\n# Instantiate model and solver\nmodel = cp_model.CpModel()\nsolver = cp_model.CpSolver()\n\n# 1. Variables\ncapacity = 19\n\nbread = model.NewIntVar(0, capacity, 'Bread')\nmeat  = model.NewIntVar(0, capacity, 'Meat')\nbeer  = model.NewIntVar(0, capacity, 'Beer')\n\n# 2. Constraints\nmodel.Add(1 * bread\n        + 3 * meat \n        + 7 * beer &lt;= capacity)\n\n# Print results\nsolver.parameters.enumerate_all_solutions = True\nstatus = solver.Solve(model, solution_printer)\nprint(solution_printer.solution_count())\n\n121\n\n\nWe found 121 solutions with a capacity of 19. But this number quickly increases: with a capacity of 1000, there are 8,080,104 possible solutions! And yet, CP-SAT finds the optimal solution in less than a second. How is it possible?\nCP solvers do not brute force the problem with an exhaustive search, but combine heuristics and combinatorial search instead. More specifically, the three most popular techniques for constraint satisfaction problems are backtracking, constraint propagation, and local search.\nCP-SAT is quite particular since it combines CP and SAT: it is part of a broader trend of merging CP, LP, SAT, and metaheuristics.\nWe said that the previous problem could be solved with Linear Programming, so let‚Äôs compare the code of both solutions:\n\n\n\nAs you can see, the syntax is quite similar but it‚Äôs not the same: model/solver vs.¬†solver, NewIntVar instead of IntVar, etc. There‚Äôs a bit of translation to do, but it‚Äôs easily manageable.\nThese two techniques are incredibly close to each other: they both handle variables with constraints and perform optimization using math and heuristics. However, CP is limited to discrete parameters, while LP handles continuous ones. On the other hand, you can implement specialized constraints like ‚Äúall different‚Äù in CP, but not in LP. Here is a summary of the main differences between these two technologies:\n\n\n\nIf you want to know more about this topic, I would recommend this article by Irvin J. Lustig and Jean-Fran√ßois Puget. CPLEX‚Äôs documentation also details the differences at this address, in terms of modeling and optimization."
  },
  {
    "objectID": "posts/2022-05-02-Constraint_Programming.html#conclusion",
    "href": "posts/2022-05-02-Constraint_Programming.html#conclusion",
    "title": "Introduction to Constraint Programming in Python",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\nConstraint Programming is another incredible technique in the mathematical optimization toolbox. It is a radically different approach compared to traditional, declarative programming. In this article,\n\nWe saw two applications of CP with satisfiability and optimization;\nWe implemented CP models in OR-Tools and played with the callback function;\nWe highlighted the differences between CP and LP.\n\nWe limited ourselves to simple problems in this introduction, but CP has amazing applications in complex scheduling and routing problems. This is a topic I‚Äôd love to address in a future article.\nIf you‚Äôre interested to know more about it, feel free to follow me on Twitter @maximelabonne. Thanks for your attention!"
  },
  {
    "objectID": "posts/2022-05-02-Constraint_Programming.html#linear-programming-course",
    "href": "posts/2022-05-02-Constraint_Programming.html#linear-programming-course",
    "title": "Introduction to Constraint Programming in Python",
    "section": "ü•á Linear Programming Course",
    "text": "ü•á Linear Programming Course\nüîé Course overview\nüìù Chapter 1: Introduction to Linear Programming\nüìù Chapter 2: Integer vs.¬†Linear Programming\nüìù Chapter 3: Constraint Programming\nüìù Chapter 4: Nonlinear Programming for Marketing Budget Allocation"
  },
  {
    "objectID": "posts/2022-05-25-Minecraft.html",
    "href": "posts/2022-05-25-Minecraft.html",
    "title": "Create a Bot to Find Diamonds in Minecraft",
    "section": "",
    "text": "Minecraft is an incredible challenge for Reinforcement Learning.\nIt‚Äôs a huge game, with many mechanics and complex sequences of actions. It takes an entire wiki with over 8000 pages just to teach humans how to play Minecraft. So how good can be machine learning?\nThis is the question we‚Äôll answer in this article. We‚Äôll design a bot and try to achieve one of the most difficult challenges in Minecraft: finding diamonds from scratch. To make things even worse, we will take on this challenge in randomly generated worlds so we can‚Äôt learn a particular seed.\nWhat we‚Äôre gonna talk about is not limited to Minecraft. It can be applied to similar complex environments. More specifically, we will implement two different techniques that will become the backbone of our intelligent agent.\nBut before we can train an agent, we need to understand how to interact with the environment. Let‚Äôs start with a scripted bot to get familiar with the syntax. We‚Äôll use MineRL, a fantastic library to build AI applications in Minecraft.\nThe code used in this article is available on Google Colab. It is a simplified and finetuned version of the excellent notebooks made by the organizers of the MineRL 2021 competition (MIT License).\n# # Install JDK, OpenGL, etc.\n!sudo add-apt-repository -y ppa:openjdk-r/ppa &gt; /dev/null 2&gt;&1\n!sudo apt purge openjdk-* &gt; /dev/null 2&gt;&1\n!sudo apt install openjdk-8-jdk xvfb xserver-xephyr vnc4server python-opengl ffmpeg &gt; /dev/null 2&gt;&1\n\n# # Install MineRL, the virtual display, and a video renderer\n!pip install -q -U minerl pyvirtualdisplay colabgymrender imageio==2.4.1\n\n# RL environment\nimport gym\nimport minerl\n\n# Visualization\nfrom colabgymrender.recorder import Recorder\nfrom pyvirtualdisplay import Display\n\n# Others\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport logging\nlogging.disable(logging.ERROR)\n\n# Create virtual display\ndisplay = Display(visible=0, size=(400, 300))\ndisplay.start()"
  },
  {
    "objectID": "posts/2022-05-25-Minecraft.html#i.-scripted-bot",
    "href": "posts/2022-05-25-Minecraft.html#i.-scripted-bot",
    "title": "Create a Bot to Find Diamonds in Minecraft",
    "section": "üìú I. Scripted bot",
    "text": "üìú I. Scripted bot\nMineRL allows us to launch Minecraft in Python and interact with the game. This is done through the popular gym library.\n\nenv = gym.make('MineRLObtainDiamond-v0')\nenv = Recorder(env, './video', fps=60)\nenv.seed(21)\nobs = env.reset()\nenv.release()\nenv.play()\n\n\n\n\nWe are in front of a tree. As you can see, the resolution is quite low. A low resolution means fewer pixels, which speeds things up. Fortunately for us, neural networks don‚Äôt need a 4K resolution to understand what‚Äôs happening on screen.\nNow, we would like to interact with the game. What can our agent do? Here‚Äôs the list of possible actions:\n\n\n\nThe first step to find diamonds is to get wood to make a crafting table and a wooden pickaxe.\nLet‚Äôs try to get closer to the tree. It means that we need to hold the ‚Äúforward‚Äù button for less than a second. With MineRL, there are 20 actions processed per second: we don‚Äôt need a full second so let‚Äôs process it 5 times, and wait for 40 more ticks.\n\n\n\n\n# Define the sequence of actions\nscript = ['forward'] * 5 + [''] * 40\n\nenv = gym.make('MineRLObtainDiamond-v0')\nenv = Recorder(env, './video', fps=60)\nenv.seed(21)\nobs = env.reset()\n\nfor action in script:\n    # Get the action space (dict of possible actions)\n    action_space = env.action_space.noop()\n\n    # Activate the selected action in the script\n    action_space[action] = 1\n\n    # Update the environment with the new action space\n    obs, reward, done, _ = env.step(action_space)\n\nenv.release()\nenv.play()\n\n\n\n\nGreat, let‚Äôs chop this tree now. We need four actions in total:\n\nForward to go in front of the tree;\nAttack to chop the tree;\nCamera to look up or down;\nJump to get the final piece of wood.\n\n\n\n\nHandling the camera can be a hassle. To simplify the syntax, we‚Äôre gonna use the str_to_act function from this GitHub repository (MIT license). This is what the new script looks like:\n\nscript = []\nscript += [''] * 20 \nscript += ['forward'] * 5\nscript += ['attack'] * 61\nscript += ['camera:[-10,0]'] * 7  # Look up\nscript += ['attack'] * 240\nscript += ['jump']\nscript += ['forward'] * 10        # Jump forward\nscript += ['camera:[-10,0]'] * 2  # Look up\nscript += ['attack'] * 150\nscript += ['camera:[10,0]'] * 7   # Look down\nscript += [''] * 40\n\n\n# Code from https://github.com/KarolisRam/MineRL2021-Intro-baselines\ndef str_to_act(env, actions):\n    action_space = env.action_space.noop()\n    for action in actions.split():\n        if ':' in action:\n            k, v = action.split(':')\n            if k == 'camera':\n                action_space[k] = eval(v)\n            else:\n                action_space[k] = v\n        else:\n            action_space[action] = 1\n    return action_space\n    \nenv = gym.make('MineRLObtainDiamond-v0')\nenv = Recorder(env, './video', fps=60)\nenv.seed(21)\nobs = env.reset()\n \nfor action in tqdm(script):\n    obs, reward, done, _ = env.step(str_to_act(env, action))\n\nenv.release()\nenv.play()\n\n\nThe agent efficiently chopped the entire tree. This is a good start, but we would like to do it in a more automated way‚Ä¶"
  },
  {
    "objectID": "posts/2022-05-25-Minecraft.html#ii.-deep-learning",
    "href": "posts/2022-05-25-Minecraft.html#ii.-deep-learning",
    "title": "Create a Bot to Find Diamonds in Minecraft",
    "section": "üß† II. Deep Learning",
    "text": "üß† II. Deep Learning\nOur bot works well in a fixed environment, but what happens if we change the seed or its starting point?\nEverything is scripted so the agent would probably try to chop a non-existent tree.\nThis approach is too static for our requirements: we need something that can adapt to new environments. Instead of scripting orders, we want an AI that knows how to chop trees. Naturally, reinforcement learning is a pertinent framework to train this agent. More specifically, deep RL seems to be the solution since we‚Äôre processing images to select the best actions.\nThere are two ways of implementing it:\n\nPure deep RL: the agent is trained from scratch by interacting with the environment. It is rewarded every time it chops a tree.\nImitation learning: the agent learns how to chop trees from a dataset. In this case, it is a sequence of actions to chop trees made by a human.\n\nThe two approaches have the same outcome, but they‚Äôre not equivalent. According to the authors of the MineRL 2021 competition, it takes 8 hours for the pure RL solution and 15 minutes for the imitation learning agent to reach the same level of performance.\nWe don‚Äôt have that much time to spend, so we‚Äôre going for the Imitation Learning solution. This technique is also called Behavior Cloning, which is the simplest form of imitation.\nNote that Imitation Learning is not always more efficient than RL. If you want to know more about it, Kumar et al.¬†wrote a great blog post about this topic.\n\n\n\nThe problem is reduced to a multi-class classification task. Our dataset consists of mp4 videos, so we‚Äôll use a Convolutional Neural Network (CNN) to translate these images into relevant actions. Our goal is also to limit the number of actions (classes) that can be taken so the CNN has fewer options, which means it‚Äôll be trained more efficiently.\n\nimport torch\nimport torch.nn as nn\n\n\nclass CNN(nn.Module):\n    def __init__(self, input_shape, output_dim):\n        super().__init__()\n        n_input_channels = input_shape[0]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, output_dim)\n        )\n\n    def forward(self, observations):\n        return self.cnn(observations)\n\ndef dataset_action_batch_to_actions(dataset_actions, camera_margin=5):\n    camera_actions = dataset_actions[\"camera\"].squeeze()\n    attack_actions = dataset_actions[\"attack\"].squeeze()\n    forward_actions = dataset_actions[\"forward\"].squeeze()\n    jump_actions = dataset_actions[\"jump\"].squeeze()\n    batch_size = len(camera_actions)\n    actions = np.zeros((batch_size,), dtype=int)\n\n    for i in range(len(camera_actions)):\n        if camera_actions[i][0] &lt; -camera_margin:\n            actions[i] = 3\n        elif camera_actions[i][0] &gt; camera_margin:\n            actions[i] = 4\n        elif camera_actions[i][1] &gt; camera_margin:\n            actions[i] = 5\n        elif camera_actions[i][1] &lt; -camera_margin:\n            actions[i] = 6\n        elif forward_actions[i] == 1:\n            if jump_actions[i] == 1:\n                actions[i] = 2\n            else:\n                actions[i] = 1\n        elif attack_actions[i] == 1:\n            actions[i] = 0\n        else:\n            actions[i] = -1\n    return actions\n\nclass ActionShaping(gym.ActionWrapper):\n    def __init__(self, env, camera_angle=10):\n        super().__init__(env)\n        self.camera_angle = camera_angle\n        self._actions = [\n            [('attack', 1)],\n            [('forward', 1)],\n            [('jump', 1)],\n            [('camera', [-self.camera_angle, 0])],\n            [('camera', [self.camera_angle, 0])],\n            [('camera', [0, self.camera_angle])],\n            [('camera', [0, -self.camera_angle])],\n        ]\n        self.actions = []\n        for actions in self._actions:\n            act = self.env.action_space.noop()\n            for a, v in actions:\n                act[a] = v\n                act['attack'] = 1\n            self.actions.append(act)\n        self.action_space = gym.spaces.Discrete(len(self.actions))\n\n    def action(self, action):\n        return self.actions[action]\n\nIn this example, we manually define 7 relevant actions: attack, forward, jump, and move the camera (left, right, up, down). Another popular approach is to apply K-means in order to automatically retrieve the most relevant actions taken by humans. In any case, the objective is to discard the least useful actions to complete our objective, such as crafting in our example.\nLet‚Äôs train our CNN on the MineRLTreechop-v0 dataset. Other datasets can be found at this address. We chose a learning rate of 0.0001 and 6 epochs with a batch size of 32.\n\n%%time\n\n# Get data\nminerl.data.download(directory='data', environment='MineRLTreechop-v0')\ndata = minerl.data.make(\"MineRLTreechop-v0\", data_dir='data', num_workers=2)\n\n# Model\nmodel = CNN((3, 64, 64), 7).cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop\nstep = 0\nlosses = []\nfor state, action, _, _, _ \\\n          in tqdm(data.batch_iter(num_epochs=6, batch_size=32, seq_len=1)):\n    # Get pov observations\n    obs = state['pov'].squeeze().astype(np.float32)\n    # Transpose and normalize\n    obs = obs.transpose(0, 3, 1, 2) / 255.0\n\n    # Translate batch of actions for the ActionShaping wrapper\n    actions = dataset_action_batch_to_actions(action)\n\n    # Remove samples with no corresponding action\n    mask = actions != -1\n    obs = obs[mask]\n    actions = actions[mask]\n\n    # Update weights with backprop\n    logits = model(torch.from_numpy(obs).float().cuda())\n    loss = criterion(logits, torch.from_numpy(actions).long().cuda())\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Print loss\n    step += 1\n    losses.append(loss.item())\n    if (step % 2000) == 0:\n        mean_loss = sum(losses) / len(losses)\n        tqdm.write(f'Step {step:&gt;5} | Training loss = {mean_loss:.3f}')\n        losses.clear()\n\ntorch.save(model.state_dict(), 'model.pth')\ndel data\n\nDownload: https://minerl.s3.amazonaws.com/v4/MineRLTreechop-v0.tar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1511.0/1510.73792 [00:25&lt;00:00, 58.46MB/s]\n\n\n\n\n\nStep  2000 | Training loss = 0.901\nStep  4000 | Training loss = 0.878\nStep  6000 | Training loss = 0.836\nStep  8000 | Training loss = 0.826\nStep 10000 | Training loss = 0.828\nStep 12000 | Training loss = 0.805\nStep 14000 | Training loss = 0.804\nStep 16000 | Training loss = 0.773\nStep 18000 | Training loss = 0.791\nStep 20000 | Training loss = 0.789\nStep 22000 | Training loss = 0.789\nStep 24000 | Training loss = 0.816\nStep 26000 | Training loss = 0.785\nStep 28000 | Training loss = 0.769\nStep 30000 | Training loss = 0.789\nStep 32000 | Training loss = 0.777\nStep 34000 | Training loss = 0.763\nStep 36000 | Training loss = 0.738\nStep 38000 | Training loss = 0.744\nStep 40000 | Training loss = 0.751\nStep 42000 | Training loss = 0.763\nStep 44000 | Training loss = 0.764\nStep 46000 | Training loss = 0.744\nStep 48000 | Training loss = 0.732\nStep 50000 | Training loss = 0.740\nStep 52000 | Training loss = 0.748\nStep 54000 | Training loss = 0.678\nStep 56000 | Training loss = 0.765\nStep 58000 | Training loss = 0.727\nStep 60000 | Training loss = 0.735\nStep 62000 | Training loss = 0.707\nStep 64000 | Training loss = 0.716\nStep 66000 | Training loss = 0.718\nStep 68000 | Training loss = 0.710\nStep 70000 | Training loss = 0.692\nStep 72000 | Training loss = 0.693\nStep 74000 | Training loss = 0.687\nStep 76000 | Training loss = 0.695\nCPU times: user 15min 21s, sys: 55.3 s, total: 16min 16s\nWall time: 26min 46s\n\n\nOur model is trained. We can now instantiate an environment and see how it behaves. If the training was successful, it should frantically cut all the trees in sight.\nThis time, we‚Äôll use the ActionShaping wrapper to map the array of numbers created with dataset_action_batch_to_actions to discrete actions in MineRL.\nOur model needs a pov observation in the correct format and outputs logits. These logits can be turned into a probability distribution over a set of 7 actions with the softmax function. We then randomly choose an action based on the probabilities. The selected action is implemented in MineRL thanks to env.step(action).\nThis process is repeated as many times as we want. Let‚Äôs do it 1000 times and watch the result.\n\nmodel = CNN((3, 64, 64), 7).cuda()\nmodel.load_state_dict(torch.load('model.pth'))\n\nenv = gym.make('MineRLObtainDiamond-v0')\nenv1 = Recorder(env, './video', fps=60)\nenv = ActionShaping(env1)\n\naction_list = np.arange(env.action_space.n)\n\nobs = env.reset()\n\nfor step in tqdm(range(1000)):\n    # Get input in the correct format\n    obs = torch.from_numpy(obs['pov'].transpose(2, 0, 1)[None].astype(np.float32) / 255).cuda()\n    # Turn logits into probabilities\n    probabilities = torch.softmax(model(obs), dim=1)[0].detach().cpu().numpy()\n    # Sample action according to the probabilities\n    action = np.random.choice(action_list, p=probabilities)\n\n    obs, reward, _, _ = env.step(action)\n\nenv1.release()\nenv1.play()\n\n\nOur agent is quite chaotic but it manages to chop trees in this new, unseen environment. Now, how to find diamonds?"
  },
  {
    "objectID": "posts/2022-05-25-Minecraft.html#iii.-script-imitation-learning",
    "href": "posts/2022-05-25-Minecraft.html#iii.-script-imitation-learning",
    "title": "Create a Bot to Find Diamonds in Minecraft",
    "section": "‚õèÔ∏è III. Script + Imitation Learning",
    "text": "‚õèÔ∏è III. Script + Imitation Learning\nA simple yet powerful approach consists of combining scripted actions with artificial intelligence. Learn the boring stuff, script the knowledge.\nIn this paradigm, we‚Äôll use the CNN to get a healthy amount of wood (3000 steps). Then, we can script a sequence to craft planks, sticks, a crafting table, a wooden pickaxe, and start mining stone (it should be below our feet). This stone can then be used to craft a stone pickaxe, which can mine iron ore.\n\n\n\nThis is when things get complicated: iron ore is quite rare, so we would need to run the game for a while to find a deposit. Then, we would have to craft a furnace and melt it to get the iron pickaxe. Finally, we would have to go even deeper and be even luckier to obtain a diamond without falling into lava.\nAs you can see, it‚Äôs doable but the outcome is fairly random. We could train another agent to find diamonds, and even a third one to create the iron pickaxe. If you‚Äôre interested in more complex approaches, you can read the results of the MineRL Diamond 2021 Competition by Kanervisto et al.¬†It describes several solutions using different clever techniques, including end-to-end deep learning architectures. Nonetheless, it is a complex problem and no team managed to consistently find diamonds, if at all.\nThis is why we will limit ourselves to obtaining a stone pickaxe in the following example, but you can modify the code to go further.\n\n# Craft 4 planks, 2 sticks, 2 crafting tables, and place it\nscript = []\nscript += ['craft:planks'] * 6\nscript += ['craft:stick'] * 2\nscript += ['craft:crafting_table'] * 2\nscript += ['camera:[10,0]'] * 18\nscript += ['attack'] * 20\nscript += [''] * 10\nscript += ['jump']\nscript += [''] * 5\nscript += ['place:crafting_table']\nscript += [''] * 10\n\n# Craft a wooden pickaxe and equip it\nscript += ['camera:[-1,0]']\nscript += ['nearbyCraft:wooden_pickaxe']\nscript += ['camera:[1,0]']\nscript += [''] * 10\nscript += ['equip:wooden_pickaxe']\nscript += [''] * 10\n\n# Dig stone\nscript += ['attack'] * 500\n\n# Craft stone pickaxe\nscript += [''] * 10\nscript += ['jump']\nscript += [''] * 5\nscript += ['place:crafting_table']\nscript += [''] * 10\nscript += ['camera:[-1,0]']\nscript += ['nearbyCraft:stone_pickaxe']\nscript += ['camera:[1,0]']\nscript += [''] * 10\nscript += ['equip:stone_pickaxe']\nscript += [''] * 10\n\n\nmodel = CNN((3, 64, 64), 7).cuda()\nmodel.load_state_dict(torch.load('model.pth'))\n\nenv_script = gym.make('MineRLObtainDiamond-v0')\nenv_cnn = Recorder(env_script, './video', fps=60)\nenv_script = ActionShaping(env_cnn)\n\naction_list = np.arange(env_script.action_space.n)\n\nfor _ in range(10):\n    obs = env_script.reset()\n    done = False\n\n    # 1. Get wood with the CNN\n    for i in tqdm(range(3000)):\n        obs = torch.from_numpy(obs['pov'].transpose(2, 0, 1)[None].astype(np.float32) / 255).cuda()\n        probabilities = torch.softmax(model(obs), dim=1)[0].detach().cpu().numpy()\n        action = np.random.choice(action_list, p=probabilities)\n        obs, reward, done, _ = env_script.step(action)\n        if done:\n            break\n\n    # 2. Craft stone pickaxe with scripted actions\n    if not done:\n        for action in tqdm(script):\n            obs, reward, done, _ = env_cnn.step(str_to_act(env_cnn, action))\n            if done:\n                break\n\n    print(obs[\"inventory\"])\n    env_cnn.release()\n    env_cnn.play()\n\n\nWe can see our agent chopping wood like a madman during the first 3000 steps, then our script takes over and completes the task. It might not be obvious, but the command print(obs.inventory) shows a stone pickaxe. Note that this is a cherry-picked example: most of the runs don‚Äôt end that well.\nThere are several reasons why the agent may fail: it can spawn in a hostile environment (water, lava, etc.), in an area without wood, or even fall and die. Playing with different seeds will give you a good understanding of the complexity of this problem and, hopefully, ideas to build event better agents."
  },
  {
    "objectID": "posts/2022-05-25-Minecraft.html#conclusion",
    "href": "posts/2022-05-25-Minecraft.html#conclusion",
    "title": "Create a Bot to Find Diamonds in Minecraft",
    "section": "Conclusion",
    "text": "Conclusion\nI hope you enjoyed this little guide to reinforcement learning in Minecraft. Beyond its obvious popularity, Minecraft is an interesting environment to try and test RL agents. Like NetHack, it requires a thorough knowledge of its mechanics to plan precise sequences of actions in a procedurally-generated world. In this article,\n\nWe learned how to use MineRL;\nWe saw two approaches (script and behavior cloning) and how to combine them;\nWe visualized the agent‚Äôs actions with short videos.\n\nThe main drawback of the environment is its slow processing time. Minecraft is not a lightweight game like NetHack or Pong, which is why the agents take a long time to be trained. If this is a problem for you, I would recommend lighter environments like Gym Retro.\nThank you for your attention! Feel free to follow me on Twitter if you‚Äôre interested in AI applied to video games."
  },
  {
    "objectID": "posts/2022_02_20_Graph_Convolution_Network.html",
    "href": "posts/2022_02_20_Graph_Convolution_Network.html",
    "title": "Graph Convolutional Networks: Introduction to GNNs",
    "section": "",
    "text": "Graph Neural Networks (GNNs) represent one of the most captivating and rapidly evolving architectures within the deep learning landscape. As deep learning models designed to process data structured as graphs, GNNs bring remarkable versatility and powerful learning capabilities.\nAmong the various types of GNNs, the Graph Convolutional Networks (GCNs) have emerged as the most prevalent and broadly applied model. GCNs are innovative due to their ability to leverage both the features of a node and its locality to make predictions, providing an effective way to handle graph-structured data.\nIn this article, we will delve into the mechanics of the GCN layer and explain its inner workings. Furthermore, we will explore its practical application for node classification tasks, using PyTorch Geometric as our tool of choice.\nPyTorch Geometric is a specialized extension of PyTorch that has been created specifically for the development and implementation of GNNs. It is an advanced, yet user-friendly library that provides a comprehensive suite of tools to facilitate graph-based machine learning. To commence our journey, the PyTorch Geometric installation will be required. If you are using Google Colab, PyTorch should already be in place, so all we need to do is execute a few additional commands.\nAll the code is available on Google Colab and GitHub.\n!pip install torch_geometric\n\nimport torch\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nRequirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\nRequirement already satisfied: install in /usr/local/lib/python3.10/dist-packages (1.3.5)\nRequirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.3.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.22.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\nRequirement already satisfied: psutil&gt;=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch_geometric) (2.1.3)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torch_geometric) (1.26.16)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torch_geometric) (2023.5.7)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torch_geometric) (2.0.12)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torch_geometric) (3.4)\nRequirement already satisfied: joblib&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;torch_geometric) (1.3.1)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;torch_geometric) (3.2.0)\nNow that PyTorch Geometric is installed, let‚Äôs explore the dataset we will use in this tutorial."
  },
  {
    "objectID": "posts/2022_02_20_Graph_Convolution_Network.html#i.-graph-data",
    "href": "posts/2022_02_20_Graph_Convolution_Network.html#i.-graph-data",
    "title": "Graph Convolutional Networks: Introduction to GNNs",
    "section": "üåê I. Graph data",
    "text": "üåê I. Graph data\nGraphs are an essential structure for representing relationships between objects. You can encounter graph data in a multitude of real-world scenarios, such as social and computer networks, chemical structures of molecules, natural language processing, and image recognition, to name a few.\nIn this article, we will study the infamous and much-used Zachary‚Äôs karate club dataset.\n\n\n\nThe Zachary‚Äôs karate club dataset embodies the relationships formed within a karate club as observed by Wayne W. Zachary during the 1970s. It is a kind of social network, where each node represents a club member, and edges between nodes represent interactions that occurred outside the club environment.\nIn this particular scenario, the members of the club are split into four distinct groups. Our task is to assign the correct group to each member (node classification), based on the pattern of their interactions.\nLet‚Äôs import the dataset with PyG‚Äôs built-in function and try to understand the Datasets object it uses.\n\nfrom torch_geometric.datasets import KarateClub\n\n# Import dataset from PyTorch Geometric\ndataset = KarateClub()\n\n# Print information\nprint(dataset)\nprint('------------')\nprint(f'Number of graphs: {len(dataset)}')\nprint(f'Number of features: {dataset.num_features}')\nprint(f'Number of classes: {dataset.num_classes}')\n\nKarateClub()\n------------\nNumber of graphs: 1\nNumber of features: 34\nNumber of classes: 4\n\n\nThis dataset only has 1 graph, where each node has a feature vector of 34 dimensions and is part of one out of four classes (our four groups). Actually, the Datasets object can be seen as a collection of Data (graph) objects.\nWe can further inspect our unique graph to know more about it.\n\n# Print first element\nprint(f'Graph: {dataset[0]}')\n\nGraph: Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34])\n\n\nThe Data object is particularly interesting. Printing it offers a good summary of the graph we‚Äôre studying:\n\nx=[34, 34] is the node feature matrix with shape (number of nodes, number of features). In our case, it means that we have 34 nodes (our 34 members), each node being associated to a 34-dim feature vector.\nedge_index=[2, 156] represents the graph connectivity (how the nodes are connected) with shape (2, number of directed edges).\ny=[34] is the node ground-truth labels. In this problem, every node is assigned to one class (group), so we have one value for each node.\ntrain_mask=[34] is an optional attribute that tells which nodes should be used for training with a list of True or False statements.\n\nLet‚Äôs print each of these tensors to understand what they store. Let‚Äôs start with the node features.\n\ndata = dataset[0]\n\nprint(f'x = {data.x.shape}')\nprint(data.x)\n\nx = torch.Size([34, 34])\ntensor([[1., 0., 0.,  ..., 0., 0., 0.],\n        [0., 1., 0.,  ..., 0., 0., 0.],\n        [0., 0., 1.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 1., 0., 0.],\n        [0., 0., 0.,  ..., 0., 1., 0.],\n        [0., 0., 0.,  ..., 0., 0., 1.]])\n\n\nHere, the node feature matrix x is an identity matrix: it doesn‚Äôt contain any relevant information about the nodes. It could contain information like age, skill level, etc. but this is not the case in this dataset. It means we‚Äôll have to classify our nodes just by looking at their connections.\nNow, let‚Äôs print the edge index.\n\nprint(f'edge_index = {data.edge_index.shape}')\nprint(data.edge_index)\n\nedge_index = torch.Size([2, 156])\ntensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n          1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,\n          3,  3,  3,  3,  3,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  6,  7,  7,\n          7,  7,  8,  8,  8,  8,  8,  9,  9, 10, 10, 10, 11, 12, 12, 13, 13, 13,\n         13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 19, 20, 20, 21,\n         21, 22, 22, 23, 23, 23, 23, 23, 24, 24, 24, 25, 25, 25, 26, 26, 27, 27,\n         27, 27, 28, 28, 28, 29, 29, 29, 29, 30, 30, 30, 30, 31, 31, 31, 31, 31,\n         31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33,\n         33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33],\n        [ 1,  2,  3,  4,  5,  6,  7,  8, 10, 11, 12, 13, 17, 19, 21, 31,  0,  2,\n          3,  7, 13, 17, 19, 21, 30,  0,  1,  3,  7,  8,  9, 13, 27, 28, 32,  0,\n          1,  2,  7, 12, 13,  0,  6, 10,  0,  6, 10, 16,  0,  4,  5, 16,  0,  1,\n          2,  3,  0,  2, 30, 32, 33,  2, 33,  0,  4,  5,  0,  0,  3,  0,  1,  2,\n          3, 33, 32, 33, 32, 33,  5,  6,  0,  1, 32, 33,  0,  1, 33, 32, 33,  0,\n          1, 32, 33, 25, 27, 29, 32, 33, 25, 27, 31, 23, 24, 31, 29, 33,  2, 23,\n         24, 33,  2, 31, 33, 23, 26, 32, 33,  1,  8, 32, 33,  0, 24, 25, 28, 32,\n         33,  2,  8, 14, 15, 18, 20, 22, 23, 29, 30, 31, 33,  8,  9, 13, 14, 15,\n         18, 19, 20, 22, 23, 26, 27, 28, 29, 30, 31, 32]])\n\n\nIn graph theory and network analysis, connectivity between nodes is stored using a variety of data structures. The edge_index is one such data structure, where the graph‚Äôs connections are stored in two lists (156 directed edges, which equate to 78 bidirectional edges). The reason for these two lists is that one list stores the source nodes, while the second one identifies the destination nodes.\nThis method is known as a coordinate list (COO) format, which is essentially a means to efficiently store a sparse matrix. Sparse matrices are data structures that efficiently store matrices with a majority of zero elements. In the COO format, only non-zero elements are stored, saving memory and computational resources.\nContrarily, a more intuitive and straightforward way to represent graph connectivity is through an adjacency matrix \\(A\\). This is a square matrix where each element \\(A_{ij}\\) specifies the presence or absence of an edge from node \\(i\\) to node \\(j\\) in the graph. In other words, a non-zero element \\(A_{ij}\\) implies a connection from node \\(i\\) to node \\(j\\), and a zero indicates no direct connection.\n\n\n\nAn adjacency matrix, however, is not as space-efficient as the COO format for sparse matrices or graphs with fewer edges. However, for clarity and easy interpretation, the adjacency matrix remains a popular choice for representing graph connectivity.\nThe adjacency matrix can be inferred from the edge_index with a utility function to_dense_adj().\n\nfrom torch_geometric.utils import to_dense_adj\n\nA = to_dense_adj(data.edge_index)[0].numpy().astype(int)\nprint(f'A = {A.shape}')\nprint(A)\n\nA = (34, 34)\n[[0 1 1 ... 1 0 0]\n [1 0 1 ... 0 0 0]\n [1 1 0 ... 0 1 0]\n ...\n [1 0 0 ... 0 1 1]\n [0 0 1 ... 1 0 1]\n [0 0 0 ... 1 1 0]]\n\n\nWith graph data, it is relatively uncommon for nodes to be densely interconnected. As you can see, our adjacency matrix \\(A\\) is sparse (filled with zeros).\nIn many real-world graphs, most nodes are connected to only a few other nodes, resulting in a large number of zeros in the adjacency matrix. Storing so many zeros is not efficient at all, which is why the COO format is adopted by PyG.\nOn the contrary, ground-truth labels are easy to understand.\n\nprint(f'y = {data.y.shape}')\nprint(data.y)\n\ny = torch.Size([34])\ntensor([1, 1, 1, 1, 3, 3, 3, 1, 0, 1, 3, 1, 1, 1, 0, 0, 3, 1, 0, 1, 0, 1, 0, 0,\n        2, 2, 0, 0, 2, 0, 0, 2, 0, 0])\n\n\nOur node ground-truth labels stored in y simply encode the group number (0, 1, 2, 3) for each node, which is why we have 34 values.\nFinally, let‚Äôs print the train mask.\n\nprint(f'train_mask = {data.train_mask.shape}')\nprint(data.train_mask)\n\ntrain_mask = torch.Size([34])\ntensor([ True, False, False, False,  True, False, False, False,  True, False,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False,  True, False, False, False, False, False,\n        False, False, False, False])\n\n\nThe train mask shows which nodes are supposed to be used for training with True statements. These nodes represent the training set, while the others can be considered as the test set. This division helps in model evaluation by providing unseen data for testing.\nBut we‚Äôre not done yet! The Data object has a lot more to offer. It provides various utility functions that enable the investigation of several properties of the graph. For instance:\n\nis_directed() tells you if the graph is directed. A directed graph signifies that the adjacency matrix is not symmetric, i.e., the direction of edges matters in the connections between nodes.\nisolated_nodes() checks if some nodes are not connected to the rest of the graph. These nodes are likely to pose challenges in tasks like classification due to their lack of connections.\nhas_self_loops() indicates if at least one node is connected to itself. This is distinct from the concept of []loops: a loop implies a path that starts and ends at the same node, traversing other nodes in between.\n\nIn the context of the Zachary‚Äôs karate club dataset, all these properties return False. This implies that the graph is not directed, does not have any isolated nodes, and none of its nodes are connected to themselves.\n\nprint(f'Edges are directed: {data.is_directed()}')\nprint(f'Graph has isolated nodes: {data.has_isolated_nodes()}')\nprint(f'Graph has loops: {data.has_self_loops()}')\n\nEdges are directed: False\nGraph has isolated nodes: False\nGraph has loops: False\n\n\nFinally, we can convert a graph from PyTorch Geometric to the popular graph library NetworkX using to_networkx. This is particularly useful to visualize a small graph with networkx and matplotlib.\nLet‚Äôs plot our dataset with a different color for each group.\n\nfrom torch_geometric.utils import to_networkx\n\nG = to_networkx(data, to_undirected=True)\nplt.figure(figsize=(12,12))\nplt.axis('off')\nnx.draw_networkx(G,\n                pos=nx.spring_layout(G, seed=0),\n                with_labels=True,\n                node_size=800,\n                node_color=data.y,\n                cmap=\"hsv\",\n                vmin=-2,\n                vmax=3,\n                width=0.8,\n                edge_color=\"grey\",\n                font_size=14\n                )\nplt.show()\n\n\n\n\nThis plot of Zachary‚Äôs karate club displays our 34 nodes, 78 (bidirectional) edges, and 4 labels with 4 different colors. Now that we‚Äôve seen the essentials of loading and handling a dataset with PyTorch Geometric, we can introduce the Graph Convolutional Network architecture."
  },
  {
    "objectID": "posts/2022_02_20_Graph_Convolution_Network.html#ii.-graph-convolutional-network",
    "href": "posts/2022_02_20_Graph_Convolution_Network.html#ii.-graph-convolutional-network",
    "title": "Graph Convolutional Networks: Introduction to GNNs",
    "section": "‚úâÔ∏è II. Graph Convolutional Network",
    "text": "‚úâÔ∏è II. Graph Convolutional Network\nThis section aims to introduce and build the graph convolutional layer from the ground up.\nIn traditional neural networks, linear layers apply a linear transformation to the incoming data. This transformation converts input features \\(x\\) into hidden vectors \\(h\\) through the use of a weight matrix \\(\\mathbf{W}\\). Ignoring biases for the time being, this can be expressed as:\n\\[h = \\mathbf{W} x\\]\nWith graph data, an additional layer of complexity is added through the connections between nodes. These connections matter because, typically, in networks, it‚Äôs assumed that similar nodes are more likely to be linked to each other than dissimilar ones, a phenomenon known as network homophily.\nWe can enrich our node representation by merging its features with those of its neighbors. This operation is called convolution, or neighborhood aggregation. Let‚Äôs represent the neighborhood of node \\(i\\) including itself as \\(\\tilde{\\mathcal{N}}_i\\).\n\\[h_i = \\sum_{j \\in \\tilde{\\mathcal{N}}_i} \\mathbf{W} x_j\\]\nUnlike filters in Convolutional Neural Networks (CNNs), our weight matrix \\(\\mathbf{W}\\) is unique and shared among every node. But there is another issue: nodes do not have a fixed number of neighbors like pixels do.\nHow do we address cases where one node has only 1 neighbor, and another has 500? If we simply sum the feature vectors, the resulting embedding \\(h\\) would be much larger for the node with 500 neighbors. To ensure a similar range of values for all nodes and comparability between them, we can normalize the result based on the degree of nodes, where degree refers to the number of connections a node has.\n\\[h_i = \\dfrac{1}{\\deg(i)} \\sum_{j \\in \\tilde{\\mathcal{N}}_i} \\mathbf{W} x_j\\]\nWe‚Äôre almost there! Introduced by Kipf et al.¬†(2016), the graph convolutional layer has one final improvement.\nThe authors observed that features from nodes with numerous neighbors propagate much more easily than those from more isolated nodes. To offset this effect, they suggested assigning bigger weights to features from nodes with fewer neighbors, thus balancing the influence across all nodes. This operation is written as:\n\\[h_i = \\sum_{j \\in \\tilde{\\mathcal{N}}_i} \\dfrac{1}{\\sqrt{\\deg(i)}\\sqrt{\\deg(j)}} \\mathbf{W} x_j\\]\nNote that when \\(i\\) and \\(j\\) have the same number of neighbors, it is equivalent to our own layer. Now, let‚Äôs see how to implement it in Python with PyTorch Geometric."
  },
  {
    "objectID": "posts/2022_02_20_Graph_Convolution_Network.html#iii.-implementing-a-gcn",
    "href": "posts/2022_02_20_Graph_Convolution_Network.html#iii.-implementing-a-gcn",
    "title": "Graph Convolutional Networks: Introduction to GNNs",
    "section": "üß† III. Implementing a GCN",
    "text": "üß† III. Implementing a GCN\nPyTorch Geometric provides the GCNConv function, which directly implements the graph convolutional layer.\nIn this example, we‚Äôll create a basic Graph Convolutional Network with a single GCN layer, a ReLU activation function, and a linear output layer. This output layer will yield four values corresponding to our four categories, with the highest value determining the class of each node.\nIn the following code block, we define the GCN layer with a 3-dimensional hidden layer.\n\nfrom torch.nn import Linear\nfrom torch_geometric.nn import GCNConv\n\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.gcn = GCNConv(dataset.num_features, 3)\n        self.out = Linear(3, dataset.num_classes)\n\n    def forward(self, x, edge_index):\n        h = self.gcn(x, edge_index).relu()\n        z = self.out(h)\n        return h, z\n\nmodel = GCN()\nprint(model)\n\nGCN(\n  (gcn): GCNConv(34, 3)\n  (out): Linear(in_features=3, out_features=4, bias=True)\n)\n\n\nIf we added a second GCN layer, our model would not only aggregate feature vectors from the neighbors of each node, but also from the neighbors of these neighbors.\nWe can stack several graph layers to aggregate more and more distant values, but there‚Äôs a catch: if we add too many layers, the aggregation becomes so intense that all the embeddings end up looking the same. This phenomenon is called over-smoothing and can be a real problem when you have too many layers.\nNow that we‚Äôve defined our GNN, let‚Äôs write a simple training loop with PyTorch. I chose a regular cross-entropy loss since it‚Äôs a multi-class classification task, with Adam as optimizer. In this article, we won‚Äôt implement a train/test split to keep things simple and focus on how GNNs learn instead.\nThe training loop is standard: we try to predict the correct labels, and we compare the GCN‚Äôs results to the values stored in data.y. The error is calculated by the cross-entropy loss and backpropagated with Adam to fine-tune our GNN‚Äôs weights and biases. Finally, we print metrics every 10 epochs.\n\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n\n# Calculate accuracy\ndef accuracy(pred_y, y):\n    return (pred_y == y).sum() / len(y)\n\n# Data for animations\nembeddings = []\nlosses = []\naccuracies = []\noutputs = []\n\n# Training loop\nfor epoch in range(201):\n    # Clear gradients\n    optimizer.zero_grad()\n\n    # Forward pass\n    h, z = model(data.x, data.edge_index)\n\n    # Calculate loss function\n    loss = criterion(z, data.y)\n\n    # Calculate accuracy\n    acc = accuracy(z.argmax(dim=1), data.y)\n\n    # Compute gradients\n    loss.backward()\n\n    # Tune parameters\n    optimizer.step()\n\n    # Store data for animations\n    embeddings.append(h)\n    losses.append(loss)\n    accuracies.append(acc)\n    outputs.append(z.argmax(dim=1))\n\n    # Print metrics every 10 epochs\n    if epoch % 10 == 0:\n        print(f'Epoch {epoch:&gt;3} | Loss: {loss:.2f} | Acc: {acc*100:.2f}%')\n\nEpoch   0 | Loss: 1.40 | Acc: 41.18%\nEpoch  10 | Loss: 1.21 | Acc: 47.06%\nEpoch  20 | Loss: 1.02 | Acc: 67.65%\nEpoch  30 | Loss: 0.80 | Acc: 73.53%\nEpoch  40 | Loss: 0.59 | Acc: 73.53%\nEpoch  50 | Loss: 0.39 | Acc: 94.12%\nEpoch  60 | Loss: 0.23 | Acc: 97.06%\nEpoch  70 | Loss: 0.13 | Acc: 100.00%\nEpoch  80 | Loss: 0.07 | Acc: 100.00%\nEpoch  90 | Loss: 0.05 | Acc: 100.00%\nEpoch 100 | Loss: 0.03 | Acc: 100.00%\nEpoch 110 | Loss: 0.02 | Acc: 100.00%\nEpoch 120 | Loss: 0.02 | Acc: 100.00%\nEpoch 130 | Loss: 0.02 | Acc: 100.00%\nEpoch 140 | Loss: 0.01 | Acc: 100.00%\nEpoch 150 | Loss: 0.01 | Acc: 100.00%\nEpoch 160 | Loss: 0.01 | Acc: 100.00%\nEpoch 170 | Loss: 0.01 | Acc: 100.00%\nEpoch 180 | Loss: 0.01 | Acc: 100.00%\nEpoch 190 | Loss: 0.01 | Acc: 100.00%\nEpoch 200 | Loss: 0.01 | Acc: 100.00%\n\n\nGreat! Without much surprise, we reach 100% accuracy on the training set (full dataset). It means that our model learned to correctly assign every member of the karate club to its correct group.\nWe can produce a neat visualization by animating the graph and see the evolution of the GNN‚Äôs predictions during the training process.\n\n%%capture\nfrom IPython.display import HTML\nfrom matplotlib import animation\nplt.rcParams[\"animation.bitrate\"] = 3000\n\ndef animate(i):\n    G = to_networkx(data, to_undirected=True)\n    nx.draw_networkx(G,\n                    pos=nx.spring_layout(G, seed=0),\n                    with_labels=True,\n                    node_size=800,\n                    node_color=outputs[i],\n                    cmap=\"hsv\",\n                    vmin=-2,\n                    vmax=3,\n                    width=0.8,\n                    edge_color=\"grey\",\n                    font_size=14\n                    )\n    plt.title(f'Epoch {i} | Loss: {losses[i]:.2f} | Acc: {accuracies[i]*100:.2f}%',\n              fontsize=18, pad=20)\n\nfig = plt.figure(figsize=(12, 12))\nplt.axis('off')\n\nanim = animation.FuncAnimation(fig, animate, \\\n            np.arange(0, 200, 10), interval=500, repeat=True)\nhtml = HTML(anim.to_html5_video())\n\n\ndisplay(html)\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nThe first predictions are random, but the GCN perfectly labels every node after a while. Indeed, the final graph is the same as the one we plotted at the end of the first section. But what does the GCN really learn?\nBy aggregating features from neighboring nodes, the GNN learns a vector representation (or embedding) of every node in the network. In our model, the final layer just learns how to use these representations to produce the best classifications. However, embeddings are the real products of GNNs.\nLet‚Äôs print the embeddings learned by our model.\n\n# Print embeddings\nprint(f'Final embeddings = {h.shape}')\nprint(h)\n\nFinal embeddings = torch.Size([34, 3])\ntensor([[1.9099e+00, 2.3584e+00, 7.4027e-01],\n        [2.6203e+00, 2.7997e+00, 0.0000e+00],\n        [2.2567e+00, 2.2962e+00, 6.4663e-01],\n        [2.0802e+00, 2.8785e+00, 0.0000e+00],\n        [0.0000e+00, 0.0000e+00, 2.9694e+00],\n        [0.0000e+00, 0.0000e+00, 3.3817e+00],\n        [0.0000e+00, 1.5008e-04, 3.4246e+00],\n        [1.7593e+00, 2.4292e+00, 2.4551e-01],\n        [1.9757e+00, 6.1032e-01, 1.8986e+00],\n        [1.7770e+00, 1.9950e+00, 6.7018e-01],\n        [0.0000e+00, 1.1683e-04, 2.9738e+00],\n        [1.8988e+00, 2.0512e+00, 2.6225e-01],\n        [1.7081e+00, 2.3618e+00, 1.9609e-01],\n        [1.8303e+00, 2.1591e+00, 3.5906e-01],\n        [2.0755e+00, 2.7468e-01, 1.9804e+00],\n        [1.9676e+00, 3.7185e-01, 2.0011e+00],\n        [0.0000e+00, 0.0000e+00, 3.4787e+00],\n        [1.6945e+00, 2.0350e+00, 1.9789e-01],\n        [1.9808e+00, 3.2633e-01, 2.1349e+00],\n        [1.7846e+00, 1.9585e+00, 4.8021e-01],\n        [2.0420e+00, 2.7512e-01, 1.9810e+00],\n        [1.7665e+00, 2.1357e+00, 4.0325e-01],\n        [1.9870e+00, 3.3886e-01, 2.0421e+00],\n        [2.0614e+00, 5.1042e-01, 2.4872e+00],\n        [1.8381e-01, 2.1094e+00, 2.2035e+00],\n        [1.8858e-01, 2.0701e+00, 2.1601e+00],\n        [2.2553e+00, 4.1764e-01, 2.0231e+00],\n        [1.6532e+00, 8.6745e-01, 2.2131e+00],\n        [2.4265e-01, 2.1862e+00, 1.6104e+00],\n        [2.5709e+00, 4.6342e-02, 2.3627e+00],\n        [2.1778e+00, 4.4730e-01, 2.0077e+00],\n        [3.8906e-02, 2.3443e+00, 1.9195e+00],\n        [3.0748e+00, 0.0000e+00, 3.0789e+00],\n        [3.4316e+00, 1.9716e-01, 2.5231e+00]], grad_fn=&lt;ReluBackward0&gt;)\n\n\nAs you can see, embeddings do not need to have the same dimensions as feature vectors. Here, I chose to reduce the number of dimensions from 34 (dataset.num_features) to three to get a nice visualization in 3D.\nLet‚Äôs plot these embeddings before any training happens, at epoch 0.\n\n# Get first embedding at epoch = 0\nembed = h.detach().cpu().numpy()\n\nfig = plt.figure(figsize=(12, 12))\nax = fig.add_subplot(projection='3d')\nax.patch.set_alpha(0)\nplt.tick_params(left=False,\n                bottom=False,\n                labelleft=False,\n                labelbottom=False)\nax.scatter(embed[:, 0], embed[:, 1], embed[:, 2],\n           s=200, c=data.y, cmap=\"hsv\", vmin=-2, vmax=3)\n\nplt.show()\n\n\n\n\nWe see every node from Zachary‚Äôs karate club with their true labels (and not the model‚Äôs predictions). For now, they‚Äôre all over the place since the GNN is not trained yet. But if we plot these embeddings at each step of the training loop, we‚Äôd be able to visualize what the GNN truly learns.\nLet‚Äôs see how they evolve over time, as the GCN gets better and better at classifying nodes.\n\n%%capture\n\ndef animate(i):\n    embed = embeddings[i].detach().cpu().numpy()\n    ax.clear()\n    ax.scatter(embed[:, 0], embed[:, 1], embed[:, 2],\n           s=200, c=data.y, cmap=\"hsv\", vmin=-2, vmax=3)\n    plt.title(f'Epoch {i} | Loss: {losses[i]:.2f} | Acc: {accuracies[i]*100:.2f}%',\n              fontsize=18, pad=40)\n\nfig = plt.figure(figsize=(12, 12))\nplt.axis('off')\nax = fig.add_subplot(projection='3d')\nplt.tick_params(left=False,\n                bottom=False,\n                labelleft=False,\n                labelbottom=False)\n\nanim = animation.FuncAnimation(fig, animate, \\\n              np.arange(0, 200, 10), interval=800, repeat=True)\nhtml = HTML(anim.to_html5_video())\n\n\ndisplay(html)\n\n\n  \n  Your browser does not support the video tag.\n\n\n\nOur Graph Convolutional Network (GCN) has effectively learned embeddings that group similar nodes into distinct clusters. This enables the final linear layer to distinguish them into separate classes with ease.\nEmbeddings are not unique to GNNs: they can be found everywhere in deep learning. They don‚Äôt have to be 3D either: actually, they rarely are. For instance, language models like BERT produce embeddings with 768 or even 1024 dimensions.\nAdditional dimensions store more information about nodes, text, images, etc. but they also create bigger models that are more difficult to train. This is why keeping low-dimensional embeddings as long as possible is advantageous."
  },
  {
    "objectID": "posts/2022_02_20_Graph_Convolution_Network.html#conclusion",
    "href": "posts/2022_02_20_Graph_Convolution_Network.html#conclusion",
    "title": "Graph Convolutional Networks: Introduction to GNNs",
    "section": "Conclusion",
    "text": "Conclusion\nGraph Convolutional Networks are an incredibly versatile architecture that can be applied in many contexts. In this article, we familiarized ourselves with the PyTorch Geometric library and objects like Datasets and Data. Then, we successfully reconstructed a graph convolutional layer from the ground up. Next, we put theory into practice by implementing a GCN, which gave us an understanding of practical aspects and how individual components interact. Finally, we visualized the training process and obtained a clear perspective of what it involves for such a network.\nZachary‚Äôs karate club is a simplistic dataset, but it is good enough to understand the most important concepts in graph data and GNNs. Although we only talked about node classification in this article, there are other tasks GNNs can accomplish: link prediction (e.g., to recommend a friend), graph classification (e.g., to label molecules), graph generation (e.g., to create new molecules), and so on.\nBeyond GCN, numerous GNN layers and architectures have been proposed by researchers. In the next article, we‚Äôll introduce the Graph Attention Network (GAT) architecture, which dynamically computes the GCN‚Äôs normalization factor and the importance of each connection with an attention mechanism.\nIf you enjoyed this article, feel free to follow me on Twitter for more GNN content. Thank you!"
  },
  {
    "objectID": "posts/2022_02_20_Graph_Convolution_Network.html#graph-neural-network-course",
    "href": "posts/2022_02_20_Graph_Convolution_Network.html#graph-neural-network-course",
    "title": "Graph Convolutional Networks: Introduction to GNNs",
    "section": "üåê Graph Neural Network Course",
    "text": "üåê Graph Neural Network Course\nüìï Hands-On Graph Neural Networks\nüîé Course overview\nüìù Chapter 1: Introduction to Graph Neural Networks\nüìù Chapter 2: Graph Attention Network\nüìù Chapter 3: GraphSAGE\nüìù Chapter 4: Graph Isomorphism Network"
  },
  {
    "objectID": "posts/2023-05-21-Nonlinear_optimization.html",
    "href": "posts/2023-05-21-Nonlinear_optimization.html",
    "title": "Optimize Your Marketing Budget with Nonlinear Programming",
    "section": "",
    "text": "In the age of digital marketing, businesses face the challenge of allocating their marketing budget across multiple channels to maximize sales.\nHowever, as they broaden their reach, these firms inevitably face the issue of diminishing returns - the phenomenon where additional investment in a marketing channel yields progressively smaller increases in conversions. This is where the concept of marketing budget allocation steps in, adding another layer of complexity to the whole process.\nIn this article, we‚Äôre going to explore the potential of nonlinear programming, specifically conic optimization (or cone programming), as a tool for marketing budget allocation. With the use of this advanced mathematical technique, we aim to optimize the distribution of marketing budget across various platforms to extract the maximum value and the highest possible ROI.\nThe code is available on GitHub and Google Colab."
  },
  {
    "objectID": "posts/2023-05-21-Nonlinear_optimization.html#marketing-budget-allocation",
    "href": "posts/2023-05-21-Nonlinear_optimization.html#marketing-budget-allocation",
    "title": "Optimize Your Marketing Budget with Nonlinear Programming",
    "section": "üí∞ Marketing budget allocation",
    "text": "üí∞ Marketing budget allocation\nMarketing budget allocation is a critical aspect of any advertising campaign, requiring businesses to strategically distribute their resources across different channels. The goal is to maximize the effectiveness of their marketing efforts and achieve the highest possible return on investment (ROI). To tackle this challenge, we need to consider three key components:\n\nAttribution: How can we connect conversion events to specific campaigns?\nPerformance Estimation: How can we predict the performance of a campaign based on its allocated budget?\nOptimization: How can we allocate budgets across various campaigns to maximize ROI?"
  },
  {
    "objectID": "posts/2023-05-21-Nonlinear_optimization.html#attribution-connecting-conversions-to-campaigns",
    "href": "posts/2023-05-21-Nonlinear_optimization.html#attribution-connecting-conversions-to-campaigns",
    "title": "Optimize Your Marketing Budget with Nonlinear Programming",
    "section": "üîó 1. Attribution: Connecting Conversions to Campaigns",
    "text": "üîó 1. Attribution: Connecting Conversions to Campaigns\nAttribution is the process of determining which campaigns are responsible for converting customers. Some channels, like Facebook or AdWords, can directly claim conversions. However, there are various attribution models to consider, including:\n\nFirst touch\nLast touch\nMulti-touch\nTime decay\nPosition-based\n\nAttribution systems are not without their issues, with two main challenges:\n\nLag: The time it takes to measure the performance of ads and attribute conversions accurately\nAttribution Window: The trade-off between using a short versus a long window to attribute conversions\n\nFor example, DoorDash used a several-day last-touch attribution system. The problem they faced was the need to wait for several days to measure the performance of their ads, which proved too lengthy given the rapid changes in their market."
  },
  {
    "objectID": "posts/2023-05-21-Nonlinear_optimization.html#performance-estimation-predicting-campaign-success",
    "href": "posts/2023-05-21-Nonlinear_optimization.html#performance-estimation-predicting-campaign-success",
    "title": "Optimize Your Marketing Budget with Nonlinear Programming",
    "section": "üîÆ 2. Performance Estimation: Predicting Campaign Success",
    "text": "üîÆ 2. Performance Estimation: Predicting Campaign Success\nPerformance estimation involves creating a model that can predict the success of a marketing campaign based on its budget allocation. Here, success can be defined in terms of various Key Performance Indicators (KPIs), such as:\n\nLeads\nCost per Lead (CPL)\nCustomer Lifetime Value (CLV)\nCustomer Acquisition Cost (CAC)\n\nTraditionally, linear models have been used for performance estimation. However, they assume that marketing channels don‚Äôt exhibit diminishing returns, which is often not the case. To obtain nontrivial solutions, linear models typically incorporate multiple constraints and are solved using Linear Programming (LP).\nIn reality, response curves in marketing mix modeling often display different shapes, such as:\n\nLinear (rare)\nConcave (common, indicating diminishing returns)\nConvex (rare)\nS-shaped (rare)\n\n\n\n\nThese shapes reflect the diminishing returns of marketing spending or the varying effectiveness of different channels at different budget levels. For example, investing more money into a channel might initially yield higher returns (convex), but after a certain point, each additional dollar may generate less and less incremental outcome (becoming concave), creating an S-shaped curve overall.\nTo capture the intrinsic nonlinearity of the marketing budget allocation problem, a more sophisticated approach is needed. This is where nonlinear programming, specifically conic optimization, comes into play."
  },
  {
    "objectID": "posts/2023-05-21-Nonlinear_optimization.html#optimization-nonlinear-optimization-with-cvxpy",
    "href": "posts/2023-05-21-Nonlinear_optimization.html#optimization-nonlinear-optimization-with-cvxpy",
    "title": "Optimize Your Marketing Budget with Nonlinear Programming",
    "section": "üîÑ 3. Optimization: Nonlinear Optimization with¬†CVXPY",
    "text": "üîÑ 3. Optimization: Nonlinear Optimization with¬†CVXPY\nNonlinear programming, also known as nonlinear optimization, is a method used to solve optimization problems where the objective function, constraints, or both, are nonlinear. In simple terms, it‚Äôs the process of finding the optimal solution (either maximizing or minimizing) for a system that‚Äôs governed by a set of nonlinear equations.\nIn this example, we will model the returns for each marketing channel (response curve) using the natural logarithm as follows:\n\\text{returns}_i(\\text{budget}_i) = \\alpha_i + \\beta_i \\ln(\\text{budget}_i)\nThe two previous steps of attribution and performance estimation approximate the values of \\alpha_i and \\beta_i for every channel i. Let‚Äôs take a simple example with three channels:\n\\begin{align}\n\\text{Google} &= -9453.72 + 8256.21\\ \\ln(\\text{budget}_1) \\\\\n\\text{Facebook} &= -8312.84 + 7764.20\\ \\ln(\\text{budget}_2) \\\\\n\\text{YouTube} &= -7371.33 + 7953.36\\ \\ln(\\text{budget}_3) \\\\\n\\end{align}\nThe noise observed in these values is typical in marketing budget allocation problems. Note that the alpha values are negative; this can be interpreted as the initial cost of engaging with a new marketing channel.\nWe can plot the response curves of each marketing channel using matplotlib.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nnp.random.seed(0)\n\nTOTAL_BUDGET = 100_000\n\n# Alpha and beta constants\nalphas = np.array([-9453.72, -8312.84, -7371.33])\nbetas = np.array([8256.21, 7764.20, 7953.36])\n\n# Linearly spaced numbers\nx = np.linspace(1, TOTAL_BUDGET, TOTAL_BUDGET)\n\n# Plot the response curves\nfig = plt.figure(figsize=(10, 5), dpi=300)\nplt.plot(x, alphas[0] + betas[0] * np.log(x), color='red', label='Google Ads')\nplt.plot(x, alphas[1] + betas[1] * np.log(x), color='blue', label='Facebook Ads')\nplt.plot(x, alphas[2] + betas[2] * np.log(x), color='green', label='Twitter Ads')\nplt.xlabel('Budget ($)')\nplt.ylabel('Returns ($)') \nplt.legend()\nplt.show()\n\n\n\n\nHow to find the best values for each response curve? The easiest solution consists of a greedy algorithm that randomly samples values and evaluates the result. Our optimization problem can be described as follows:\n\\begin{align}\n&\\text{maximize}\\quad \\sum^N_{i=1} \\alpha_i + \\beta_i \\ln(\\text{budget}_i) \\\\\n&\\begin{aligned}\\text{s.t.}\\quad &\\sum^N_{i=1} \\text{budget}_i \\leq \\text{total budget}\\\\\n&\\text{budget}_i \\geq 0\\quad \\forall i \\in \\mathbb{Z}, 1 \\leq i \\leq N\n\\end{aligned}\n\\end{align}\nThe following function has a budget of 1,000 iterations to find the best allocation.\n\ndef greedy_optimization(TOTAL_BUDGET, alphas, betas, num_iterations=1_000):\n    # Initialize the budget allocation and the best objective value\n    google_budget = facebook_budget = twitter_budget = TOTAL_BUDGET / 3\n    obj = alphas[0] + betas[0] * np.log(google_budget) + alphas[1] + betas[1] * np.log(facebook_budget) + alphas[2] + betas[2] * np.log(twitter_budget)\n\n    for _ in range(num_iterations):\n        # Generate a new random allocation\n        random_allocation = np.random.dirichlet(np.ones(3)) * TOTAL_BUDGET\n        google_budget_new, facebook_budget_new, twitter_budget_new = random_allocation\n\n        # Calculate the new objective value\n        new_obj = alphas[0] + betas[0] * np.log(google_budget_new) + alphas[1] + betas[1] * np.log(facebook_budget_new) + alphas[2] + betas[2] * np.log(twitter_budget_new)\n\n        # If the new allocation improves the objective value, keep it\n        if new_obj &gt; obj:\n            google_budget, facebook_budget, twitter_budget = google_budget_new, facebook_budget_new, twitter_budget_new\n            obj = new_obj\n\n    # Return the best allocation and the corresponding objective value\n    return (google_budget, facebook_budget, twitter_budget), obj\n\nLet‚Äôs run it and see the approximated solution it found:\n\n%%time\n\n# Run the greedy optimization\n(best_google, best_facebook, best_twitter), obj = greedy_optimization(TOTAL_BUDGET, alphas, betas)\n\n# Print the result\nprint('='*59 + '\\n' + ' '*24 + 'Solution' + ' '*24 + '\\n' + '='*59)\nprint(f'Returns = ${round(obj):,}\\n')\nprint('Marketing allocation:')\nprint(f' - Google Ads   = ${round(best_google):,}')\nprint(f' - Facebook Ads = ${round(best_facebook):,}')\nprint(f' - Twitter Ads  = ${round(best_twitter):,}')\n\n===========================================================\n                        Solution                        \n===========================================================\nReturns = $224,534\n\nMarketing allocation:\n - Google Ads   = $35,476\n - Facebook Ads = $31,722\n - Twitter Ads  = $32,802\nCPU times: total: 0 ns\nWall time: 21.6 ms\n\n\nAfter running our calculations, we find that our total return is $224,533. You might wonder if we can improve it by tweaking our model more or running more iterations.\nThis kind of guarantee is exactly where nonlinear programming comes to the rescue: it can output the best solution possible, also called the optimal solution. On top of this overwhelming advantage, it is also faster to run.\nTo solve the marketing budget allocation problem using nonlinear programming, we‚Äôll use the CVXPY library, which supports conic optimization thanks to specialized solvers like ECOS, MOSEK (interior point method), and SCS (first-order method). In this example, we‚Äôll use the open-source ECOS solver to find the optimal solution.\nLet‚Äôs set up the optimization problem:\n\nOur decision variables are the (positive) budgets for each channel\nOur constraint is that the sum of all budgets must not exceed the total budget\nOur objective is to maximize the total return, which is the sum of the returns for each channel\n\n\nimport cvxpy as cp\n\n# Variables\ngoogle   = cp.Variable(pos=True)\nfacebook = cp.Variable(pos=True)\ntwitter  = cp.Variable(pos=True)\n\n# Constraint\nconstraint = [google + facebook + twitter &lt;= TOTAL_BUDGET]\n\n# Objective\nobj = cp.Maximize(alphas[0] + betas[0] * cp.log(google)\n                + alphas[1] + betas[1] * cp.log(facebook)\n                + alphas[2] + betas[2] * cp.log(twitter))\n\nFinally, we call the ECOS solver to find the optimal budget allocations and display the results.\n\n%%time\n\n# Solve\nprob = cp.Problem(obj, constraint)\nprob.solve(solver='ECOS', verbose=False)\n\n# Print solution\nprint('='*59 + '\\n' + ' '*24 + 'Solution' + ' '*24 + '\\n' + '='*59)\nprint(f'Status = {prob.status}')\nprint(f'Returns = ${round(prob.value):,}\\n')\nprint('Marketing allocation:')\nprint(f' - Google Ads   = ${round(google.value):,}')\nprint(f' - Facebook Ads = ${round(facebook.value):,}')\nprint(f' - Twitter Ads  = ${round(twitter.value):,}')\n\n===========================================================\n                        Solution                        \n===========================================================\nStatus = optimal\nReturns = $224,540\n\nMarketing allocation:\n - Google Ads   = $34,439\n - Facebook Ads = $32,386\n - Twitter Ads  = $33,175\nCPU times: total: 0 ns\nWall time: 13 ms\n\n\nThe optimal allocation found by the solver is $34,439 for Google Ads, $32,386 for Facebook Ads, and $33,175 for YouTube, for a total return of $224,540! This is $7 higher than what the greedy algorithm returned ($224,533).\nKeep in mind that this allocation maximizes the returns based on our response curves: correctly modeling these curves is crucial for optimizing the budget effectively.\nLet‚Äôs visualize this optimal allocation on top of the previous response curves.\n\n# Plot the functions and the results\nfig = plt.figure(figsize=(10, 5), dpi=300)\n\nplt.plot(x, alphas[0] + betas[0] * np.log(x), color='red', label='Google Ads')\nplt.plot(x, alphas[1] + betas[1] * np.log(x), color='blue', label='Facebook Ads')\nplt.plot(x, alphas[2] + betas[2] * np.log(x), color='green', label='Twitter Ads')\n\n# Plot optimal points\nplt.scatter([google.value, facebook.value, twitter.value],\n            [alphas[0] + betas[0] * np.log(google.value),\n             alphas[1] + betas[1] * np.log(facebook.value),\n             alphas[2] + betas[2] * np.log(twitter.value)],\n            marker=\"+\", color='black', zorder=10)\n\nplt.xlabel('Budget ($)')\nplt.ylabel('Returns ($)') \nplt.legend()\nplt.show()\n\n\n\n\nBut is it really optimal? We can do a quick sanity check by running the greedy algorithm for different numbers of iterations. This will show us the difference between these two approaches.\nLet‚Äôs run it for 20 different numbers of iterations between 1 and 1,000,000.\n\n%%time\n\n# List to store the best objective value for each number of iterations\nbest_obj_list = []\n\n# Range of number of iterations to test\nnum_iterations_range = np.logspace(0, 6, 20).astype(int)\n\n# Run the greedy algorithm for each number of iterations and store the best objective value\nfor num_iterations in num_iterations_range:\n    _, best_obj = greedy_optimization(TOTAL_BUDGET, alphas, betas, num_iterations)\n    best_obj_list.append(best_obj)\n\nCPU times: total: 19.9 s\nWall time: 24.7 s\n\n\nWe can now plot the resulting list using matplotlib and compare it to the optimal solution:\n\n# Plot the results\nplt.figure(figsize=(10, 5), dpi=300)\nplt.ticklabel_format(useOffset=False)\nplt.plot(num_iterations_range, best_obj_list, label='Greedy algorithm')\nplt.axhline(y=prob.value, color='r', linestyle='--', label='Optimal solution (CVXPY)')\nplt.xlabel('Number of iterations')\nplt.xticks(num_iterations_range)\nplt.xscale(\"log\")\nplt.ylabel('Best returns ($)')\nplt.title('Best returns found by the greedy algorithm for different numbers of iterations')\nplt.legend()\nplt.show()\n\n\n\n\nWe observe that the greedy algorithm performs relatively well when given a large number of iterations. However, despite one million attempts, it falls just short of finding the optimal allocation, which yields a return of $224,540.1500. The best non-rounded value it could reach is $224,540.1489.\nTo add to this, there‚Äôs a significant difference in terms of computational speed between the two approaches. The nonlinear programming model identified the optimal solution in a swift 22.3 milliseconds. In stark contrast, the greedy algorithm took a considerable 30 seconds to run its 1 million iterations and find a nearly optimal solution.\nThis disparity becomes even more crucial when we extend our problem to numerous marketing channels. Nonlinear programming with CVXPY maintains its speed and precision, making it a highly efficient tool for complex, high-dimensional marketing budget allocation problems."
  },
  {
    "objectID": "posts/2023-05-21-Nonlinear_optimization.html#conclusion",
    "href": "posts/2023-05-21-Nonlinear_optimization.html#conclusion",
    "title": "Optimize Your Marketing Budget with Nonlinear Programming",
    "section": "Conclusion",
    "text": "Conclusion\nNonlinear programming offers a powerful approach to tackling the marketing budget allocation problem. By modeling the diminishing returns of each marketing channel with nonlinear functions and leveraging the CVXPY library, we can find the optimal allocation of resources that maximizes sales.\nAs the marketing landscape evolves and the number of channels increases, optimization techniques like nonlinear programming can help businesses make better, data-driven decisions about their marketing investments. While this article provides a starting point, there are many more advanced techniques and models to explore. Keep learning and experimenting to find the best approach for your business.\nIf you‚Äôre interested to know more about it, feel free to follow me on Twitter @maximelabonne. Happy optimizing!"
  },
  {
    "objectID": "posts/2023-05-21-Nonlinear_optimization.html#references",
    "href": "posts/2023-05-21-Nonlinear_optimization.html#references",
    "title": "Optimize Your Marketing Budget with Nonlinear Programming",
    "section": "References",
    "text": "References\nIf you want to learn more about marketing budget allocation, I recommend the following resources:\n\nA Nonlinear Optimization Model of Advertising Budget Allocation across Multiple Digital Media Channels (Park et al., 2022): an excellent approach based on diminishing returns, which inspired this article.\nA Unified Framework for Marketing Budget Allocation (Zhao et al., 2019): fascinating architecture currently in production at Alibaba, based on a logit response curve.\nCross-channel marketing spend optimization using deep learning (Katsov, 2019): blog post about an intriguing LSTM-based approach, without convex optimization.\nOptimizing the marketing budget allocation problem using Lagrangean based techniques: master thesis about a Lagrangean relaxation optimization approach.\nDeriving a Marketing Budget Allocation Model Under Uncertain History Data: another master thesis focused on introducing uncertainty."
  },
  {
    "objectID": "posts/2023-05-21-Nonlinear_optimization.html#linear-programming-course",
    "href": "posts/2023-05-21-Nonlinear_optimization.html#linear-programming-course",
    "title": "Optimize Your Marketing Budget with Nonlinear Programming",
    "section": "ü•á Linear Programming Course",
    "text": "ü•á Linear Programming Course\nüîé Course overview\nüìù Chapter 1: Introduction to Linear Programming\nüìù Chapter 2: Integer vs.¬†Linear Programming\nüìù Chapter 3: Constraint Programming\nüìù Chapter 4: Nonlinear Programming for Marketing Budget Allocation"
  },
  {
    "objectID": "posts/2023-06-07-Decoding_strategies.html",
    "href": "posts/2023-06-07-Decoding_strategies.html",
    "title": "Decoding Strategies in Large Language Models",
    "section": "",
    "text": "In the fascinating world of large language models (LLMs), much attention is given to model architectures, data processing, and optimization. However, decoding strategies like beam search, which play a crucial role in text generation, are often overlooked. In this article, we will explore how LLMs generate text by delving into the mechanics of greedy search and beam search, as well as sampling techniques with top-k and nucleus sampling.\nBy the conclusion of this article, you‚Äôll not only understand these decoding strategies thoroughly but also be familiar with how to handle important hyperparameters like temperature, num_beams, top_k, and top_p.\nThe code for this article can be found on GitHub and Google Colab for reference and further exploration."
  },
  {
    "objectID": "posts/2023-06-07-Decoding_strategies.html#background",
    "href": "posts/2023-06-07-Decoding_strategies.html#background",
    "title": "Decoding Strategies in Large Language Models",
    "section": "üìö Background",
    "text": "üìö Background\nTo kick things off, let‚Äôs start with an example. We‚Äôll feed the text ‚ÄúI have a dream‚Äù to a GPT-2 model and ask it to generate the next five tokens (words or subwords).\n\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel.eval()\n\ntext = \"I have a dream\"\ninput_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n\noutputs = model.generate(input_ids, max_length=len(input_ids.squeeze())+5)\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(f\"Generated text: {generated_text}\")\n\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nGenerated text: I have a dream of being a doctor.\n\n\nThe sentence ‚ÄúI have a dream of being a doctor‚Äù appears to have been generated by GPT-2. However, GPT-2 didn‚Äôt exactly produce this sentence.\nThere‚Äôs a common misconception that LLMs like GPT-2 directly produce text. This isn‚Äôt the case. Instead, LLMs calculate logits, which are scores assigned to every possible token in their vocabulary. To simplify, here‚Äôs an illustrative breakdown of the process:\n\n\n\n\n\nThe tokenizer, Byte-Pair Encoding in this instance, translates each token in the input text into a corresponding token ID. Then, GPT-2 uses these token IDs as input and tries to predict the next most likely token. Finally, the model generates logits, which are converted into probabilities using a softmax function.\nFor example, the model assigns a probability of 17% to the token for ‚Äúof‚Äù being the next token after ‚ÄúI have a dream‚Äù. This output essentially represents a ranked list of potential next tokens in the sequence. More formally, we denote this probability as P(\\text{of } | \\text{ I have a dream}) = 17%.\nAutoregressive models like GPT predict the next token in a sequence based on the preceding tokens. Consider a sequence of tokens w = (w_1, w_2, \\ldots, w_t). The joint probability of this sequence P(w) can be broken down as:\n\\begin{align}\nP(w) &= P(w_1, w_2, \\ldots, w_t) \\\\\n     &= P(w_1) P(w_2 | w_1) P(w_3 | w_2, w_1) \\ldots P(w_t | w_1, \\ldots, w_{t-1}) \\\\\n     &= \\prod_{i=1}^t P(w_i | w_1, \\dots, w_{i-1}).\n\\end{align}\nFor each token w_i in the sequence, P(w_i | w_1, \\ldots, w_{i-1}) represents the conditional probability of w_i given all the preceding tokens (w_1, \\ldots, w_{i-1}). GPT-2 calculates this conditional probability for each of the 50,257 tokens in its vocabulary.\nThis leads to the question: how do we use these probabilities to generate text? This is where decoding strategies, such as greedy search and beam search, come into play."
  },
  {
    "objectID": "posts/2023-06-07-Decoding_strategies.html#greedy-search",
    "href": "posts/2023-06-07-Decoding_strategies.html#greedy-search",
    "title": "Decoding Strategies in Large Language Models",
    "section": "üèÉ‚Äç‚ôÇÔ∏è Greedy Search",
    "text": "üèÉ‚Äç‚ôÇÔ∏è Greedy Search\nGreedy search is a decoding method that takes the most probable token at each step as the next token in the sequence. To put it simply, it only retains the most likely token at each stage, discarding all other potential options. Using our example:\n\nStep 1: Input: ‚ÄúI have a dream‚Äù ‚Üí Most likely token: ‚Äù of‚Äù\nStep 2: Input: ‚ÄúI have a dream of‚Äù ‚Üí Most likely token: ‚Äù being‚Äù\nStep 3: Input: ‚ÄúI have a dream of being‚Äù ‚Üí Most likely token: ‚Äù a‚Äù\nStep 4: Input: ‚ÄúI have a dream of being a‚Äù ‚Üí Most likely token: ‚Äù doctor‚Äù\nStep 5: Input: ‚ÄúI have a dream of being a doctor‚Äù ‚Üí Most likely token: ‚Äú.‚Äù\n\nWhile this approach might sound intuitive, it‚Äôs important to note that the greedy search is short-sighted: it only considers the most probable token at each step without considering the overall effect on the sequence. This property makes it fast and efficient as it doesn‚Äôt need to keep track of multiple sequences, but it also means that it can miss out on better sequences that might have appeared with slightly less probable next tokens.\nNext, let‚Äôs illustrate the greedy search implementation using graphviz and networkx. We select the ID with the highest score, compute its log probability (we take the log to simplify calculations), and add it to the tree. We‚Äôll repeat this process for five tokens.\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport time\n\ndef get_log_prob(logits, token_id):\n    # Compute the softmax of the logits\n    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n    log_probabilities = torch.log(probabilities)\n    \n    # Get the log probability of the token\n    token_log_probability = log_probabilities[token_id].item()\n    return token_log_probability\n\ndef greedy_search(input_ids, node, length=5):\n    if length == 0:\n        return input_ids\n\n    outputs = model(input_ids)\n    predictions = outputs.logits\n\n    # Get the predicted next sub-word (here we use top-k search)\n    logits = predictions[0, -1, :]\n    token_id = torch.argmax(logits).unsqueeze(0)\n\n    # Compute the score of the predicted token\n    token_score = get_log_prob(logits, token_id)\n\n    # Add the predicted token to the list of input ids\n    new_input_ids = torch.cat([input_ids, token_id.unsqueeze(0)], dim=-1)\n\n    # Add node and edge to graph\n    next_token = tokenizer.decode(token_id, skip_special_tokens=True)\n    current_node = list(graph.successors(node))[0]\n    graph.nodes[current_node]['tokenscore'] = np.exp(token_score) * 100\n    graph.nodes[current_node]['token'] = next_token + f\"_{length}\"\n\n    # Recursive call\n    input_ids = greedy_search(new_input_ids, current_node, length-1)\n    \n    return input_ids\n\n# Parameters\nlength = 5\nbeams = 1\n\n# Create a balanced tree with height 'length'\ngraph = nx.balanced_tree(1, length, create_using=nx.DiGraph())\n\n# Add 'tokenscore', 'cumscore', and 'token' attributes to each node\nfor node in graph.nodes:\n    graph.nodes[node]['tokenscore'] = 100\n    graph.nodes[node]['token'] = text\n\n# Start generating text\noutput_ids = greedy_search(input_ids, 0, length=length)\noutput = tokenizer.decode(output_ids.squeeze().tolist(), skip_special_tokens=True)\nprint(f\"Generated text: {output}\")\n\nGenerated text: I have a dream of being a doctor.\n\n\nOur greedy search generates the same text as the one from the transformers library: ‚ÄúI have a dream of being a doctor.‚Äù Let‚Äôs visualize the tree we created.\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport matplotlib.colors as mcolors\nfrom matplotlib.colors import LinearSegmentedColormap\n\ndef plot_graph(graph, length, beams, score):\n    fig, ax = plt.subplots(figsize=(3+1.2*beams**length, max(5, 2+length)), dpi=300, facecolor='white')\n\n    # Create positions for each node\n    pos = nx.nx_agraph.graphviz_layout(graph, prog=\"dot\")\n\n    # Normalize the colors along the range of token scores\n    if score == 'token':\n        scores = [data['tokenscore'] for _, data in graph.nodes(data=True) if data['token'] is not None]\n    elif score == 'sequence':\n        scores = [data['sequencescore'] for _, data in graph.nodes(data=True) if data['token'] is not None]\n    vmin = min(scores)\n    vmax = max(scores)\n    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n    cmap = LinearSegmentedColormap.from_list('rg', [\"r\", \"y\", \"g\"], N=256) \n\n    # Draw the nodes\n    nx.draw_networkx_nodes(graph, pos, node_size=2000, node_shape='o', alpha=1, linewidths=4, \n                          node_color=scores, cmap=cmap)\n\n    # Draw the edges\n    nx.draw_networkx_edges(graph, pos)\n\n    # Draw the labels\n    if score == 'token':\n        labels = {node: data['token'].split('_')[0] + f\"\\n{data['tokenscore']:.2f}%\" for node, data in graph.nodes(data=True) if data['token'] is not None}\n    elif score == 'sequence':\n        labels = {node: data['token'].split('_')[0] + f\"\\n{data['sequencescore']:.2f}\" for node, data in graph.nodes(data=True) if data['token'] is not None}\n    nx.draw_networkx_labels(graph, pos, labels=labels, font_size=10)\n    plt.box(False)\n\n    # Add a colorbar\n    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n    sm.set_array([])\n    if score == 'token':\n        fig.colorbar(sm, ax=ax, orientation='vertical', pad=0, label='Token probability (%)')\n    elif score == 'sequence':\n        fig.colorbar(sm, ax=ax, orientation='vertical', pad=0, label='Sequence score')\n    plt.show()\n\n# Plot graph\nplot_graph(graph, length, 1.5, 'token')\n\n\n\n\nIn this graph, the top node stores the input token (thus with a 100% probability), while all other nodes represent generated tokens. Although each token in this sequence was the most likely at the time of prediction, ‚Äúbeing‚Äù and ‚Äúdoctor‚Äù were assigned relatively low probabilities of 9.68% and 2.86%, respectively. This suggests that ‚Äúof‚Äù, our first predicted token, may not have been the most suitable choice as it led to ‚Äúbeing‚Äù, which is quite unlikely.\nIn the following section, we‚Äôll explore how beam search can address this problem."
  },
  {
    "objectID": "posts/2023-06-07-Decoding_strategies.html#beam-search",
    "href": "posts/2023-06-07-Decoding_strategies.html#beam-search",
    "title": "Decoding Strategies in Large Language Models",
    "section": "‚öñÔ∏è Beam Search",
    "text": "‚öñÔ∏è Beam Search\nUnlike greedy search, which only considers the next most probable token, beam search takes into account the n most likely tokens, where n represents the number of beams. This procedure is repeated until a predefined maximum length is reached or an end-of-sequence token appears. At this point, the sequence (or ‚Äúbeam‚Äù) with the highest overall score is chosen as the output.\nWe can adapt the previous function to consider the n most probable tokens instead of just one. Here, we‚Äôll maintain the sequence score \\log P(w), which is the cumulative sum of the log probability of every token in the beam. We normalize this score by the sequence length to prevent bias towards longer sequences (this factor can be adjusted). Once again, we‚Äôll generate five additional tokens to complete the sentence ‚ÄúI have a dream.‚Äù\n\nfrom tqdm.notebook import tqdm\n\ndef greedy_sampling(logits, beams):\n    return torch.topk(logits, beams).indices\n    \ndef beam_search(input_ids, node, bar, length, beams, sampling, temperature=0.1):\n    if length == 0:\n        return None\n\n    outputs = model(input_ids)\n    predictions = outputs.logits\n\n    # Get the predicted next sub-word (here we use top-k search)\n    logits = predictions[0, -1, :]\n\n    if sampling == 'greedy':\n        top_token_ids = greedy_sampling(logits, beams)\n    elif sampling == 'top_k':\n        top_token_ids = top_k_sampling(logits, temperature, 20, beams)\n    elif sampling == 'nucleus':\n        top_token_ids = nucleus_sampling(logits, temperature, 0.5, beams)\n\n    for j, token_id in enumerate(top_token_ids):\n        bar.update(1)\n\n        # Compute the score of the predicted token\n        token_score = get_log_prob(logits, token_id)\n        cumulative_score = graph.nodes[node]['cumscore'] + token_score\n\n        # Add the predicted token to the list of input ids\n        new_input_ids = torch.cat([input_ids, token_id.unsqueeze(0).unsqueeze(0)], dim=-1)\n\n        # Add node and edge to graph\n        token = tokenizer.decode(token_id, skip_special_tokens=True)\n        current_node = list(graph.successors(node))[j]\n        graph.nodes[current_node]['tokenscore'] = np.exp(token_score) * 100\n        graph.nodes[current_node]['cumscore'] = cumulative_score\n        graph.nodes[current_node]['sequencescore'] = 1/(len(new_input_ids.squeeze())) * cumulative_score\n        graph.nodes[current_node]['token'] = token + f\"_{length}_{j}\"\n\n        # Recursive call\n        beam_search(new_input_ids, current_node, bar, length-1, beams, sampling, 1)\n\n# Parameters\nlength = 5\nbeams = 2\n\n# Create a balanced tree with height 'length' and branching factor 'k'\ngraph = nx.balanced_tree(beams, length, create_using=nx.DiGraph())\nbar = tqdm(total=len(graph.nodes))\n\n# Add 'tokenscore', 'cumscore', and 'token' attributes to each node\nfor node in graph.nodes:\n    graph.nodes[node]['tokenscore'] = 100\n    graph.nodes[node]['cumscore'] = 0\n    graph.nodes[node]['sequencescore'] = 0\n    graph.nodes[node]['token'] = text\n\n# Start generating text\nbeam_search(input_ids, 0, bar, length, beams, 'greedy', 1)\n\nThe function computes the scores for 63 tokens and beams^length = 5¬≤ = 25 possible sequences. In our implementation, all the information is stored in the graph. Our next step is to extract the best sequence.\nFirst, we identify the leaf node with the highest sequence score. Next, we find the shortest path from the root to this leaf. Every node along this path contains a token from the optimal sequence. Here‚Äôs how we can implement it:\n\ndef get_best_sequence(G):\n    # Create a list of leaf nodes\n    leaf_nodes = [node for node in G.nodes() if G.out_degree(node)==0]\n\n    # Get the leaf node with the highest cumscore\n    max_score_node = None\n    max_score = float('-inf')\n    for node in leaf_nodes:\n        if G.nodes[node]['sequencescore'] &gt; max_score:\n            max_score = G.nodes[node]['sequencescore']\n            max_score_node = node\n\n    # Retrieve the sequence of nodes from this leaf node to the root node in a list\n    path = nx.shortest_path(G, source=0, target=max_score_node)\n\n    # Return the string of token attributes of this sequence\n    sequence = \"\".join([G.nodes[node]['token'].split('_')[0] for node in path])\n    \n    return sequence, max_score\n\nsequence, max_score = get_best_sequence(graph)\nprint(f\"Generated text: {sequence}\")\n\nGenerated text: I have a dream. I have a dream\n\n\nThe best sequence seems to be ‚ÄúI have a dream. I have a dream,‚Äù which is a common response from GPT-2, even though it may be surprising. To verify this, let‚Äôs plot the graph.\nIn this visualization, we‚Äôll display the sequence score for each node, which represents the score of the sequence up to that point. If the function get_best_sequence() is correct, the ‚Äúdream‚Äù node in the sequence ‚ÄúI have a dream. I have a dream‚Äù should have the highest score among all the leaf nodes.\n\n# Plot graph\nplot_graph(graph, length, beams, 'sequence')\n\n\n\n\n\n\nIndeed, the ‚Äúdream‚Äù token has the highest sequence score with a value of -0.69. Interestingly, we can see the score of the greedy sequence ‚ÄúI have a dream of being a doctor.‚Äù on the left with a value of -1.16.\nAs expected, the greedy search leads to suboptimal results. But, to be honest, our new outcome is not particularly compelling either. To generate more varied sequences, we‚Äôll implement two sampling algorithms: top-k and nucleus."
  },
  {
    "objectID": "posts/2023-06-07-Decoding_strategies.html#top-k-sampling",
    "href": "posts/2023-06-07-Decoding_strategies.html#top-k-sampling",
    "title": "Decoding Strategies in Large Language Models",
    "section": "üé≤ Top-k sampling",
    "text": "üé≤ Top-k sampling\nTop-k sampling is a technique that leverages the probability distribution generated by the language model to select a token randomly from the k most likely options.\nTo illustrate, suppose we have k = 3 and four tokens: A, B, C, and D, with respective probabilities: P(A) = 30%, P(B) = 15%, P(C) = 5%, and P(D) = 1%. In top-k sampling, token D is disregarded, and the algorithm will output A 60% of the time, B 30% of the time, and C 10% of the time. This approach ensures that we prioritize the most probable tokens while introducing an element of randomness in the selection process.\nAnother way of introducing randomness is the concept of temperature. The temperature T is a parameter that ranges from 0 to 1, which affects the probabilities generated by the softmax function, making the most likely tokens more influential. In practice, it simply consists of dividing the input logits by a value we call temperature:\n\\text{softmax}(x_i) = \\frac{e^{x_i / T}}{\\sum_{j} e^{x_j / T}}\nHere is a chart that demonstrates the impact of temperature on the probabilities generated for a given set of input logits [1.5, -1.8, 0.9, -3.2]. We‚Äôve plotted three different temperature values to observe the differences.\n\n\n\n\n\nA temperature of 1.0 is equivalent to a default softmax with no temperature at all. On the other hand, a low temperature setting (0.1) significantly alters the probability distribution. This is commonly used in text generation to control the level of ‚Äúcreativity‚Äù in the generated output. By adjusting the temperature, we can influence the extent to which the model produces more diverse or predictable responses.\nLet‚Äôs now implement the top k sampling algorithm. We‚Äôll use it in the beam_search() function by providing the ‚Äútop_k‚Äù argument. To illustrate how the algorithm works, we will also plot the probability distributions for top_k=20.\n\ndef plot_prob_distribution(probabilities, next_tokens, sampling, potential_nb, total_nb=50):\n    # Get top k tokens\n    top_k_prob, top_k_indices = torch.topk(probabilities, total_nb)\n    top_k_tokens = [tokenizer.decode([idx]) for idx in top_k_indices.tolist()]\n\n    # Get next tokens and their probabilities\n    next_tokens_list = [tokenizer.decode([idx]) for idx in next_tokens.tolist()]\n    next_token_prob = probabilities[next_tokens].tolist()\n\n    # Create figure\n    plt.figure(figsize=(0.4*total_nb, 5), dpi=300, facecolor='white')\n    plt.rc('axes', axisbelow=True)\n    plt.grid(axis='y', linestyle='-', alpha=0.5)\n    if potential_nb &lt; total_nb:\n        plt.axvline(x=potential_nb-0.5, ls=':', color='grey', label='Sampled tokens')\n    plt.bar(top_k_tokens, top_k_prob.tolist(), color='blue')\n    plt.bar(next_tokens_list, next_token_prob, color='red', label='Selected tokens')\n    plt.xticks(rotation=45, ha='right', va='top')\n    plt.gca().spines['top'].set_visible(False)\n    plt.gca().spines['right'].set_visible(False)\n    if sampling == 'top_k':\n        plt.title('Probability distribution of predicted tokens with top-k sampling')\n    elif sampling == 'nucleus':\n        plt.title('Probability distribution of predicted tokens with nucleus sampling')\n    plt.legend()\n    plt.savefig(f'{sampling}_{time.time()}.png', dpi=300)\n    plt.close()\n\ndef top_k_sampling(logits, temperature, top_k, beams, plot=True):\n    assert top_k &gt;= 1\n    assert beams &lt;= top_k\n\n    indices_to_remove = logits &lt; torch.topk(logits, top_k)[0][..., -1, None]\n    new_logits = torch.clone(logits)\n    new_logits[indices_to_remove] = float('-inf')\n\n    # Convert logits to probabilities\n    probabilities = torch.nn.functional.softmax(new_logits / temperature, dim=-1)\n\n    # Sample n tokens from the resulting distribution\n    next_tokens = torch.multinomial(probabilities, beams)\n\n    # Plot distribution\n    if plot:\n        total_prob = torch.nn.functional.softmax(logits / temperature, dim=-1)\n        plot_prob_distribution(total_prob, next_tokens, 'top_k', top_k)\n\n    return next_tokens\n\n# Start generating text\nbeam_search(input_ids, 0, bar, length, beams, 'top_k', 1)\n\n\n\n\n\n\nThese plots give a good intuition of how top-k sampling works, with all the potentially selected tokens on the left of the horizontal bar. While the most probable tokens are selected (in red) most of the time, it also allows less likely tokens to be chosen. This offers an interesting tradeoff that can steer a sequence towards a less predictable but more natural-sounding sentence. Now let‚Äôs print the text it generated.\n\nsequence, max_score = get_best_sequence(graph)\nprint(f\"Generated text: {sequence}\")\n\nGenerated text: I have a dream job and I want to\n\n\nThe top-k sampling found a new sequence: ‚ÄúI have a dream job and I want to‚Äù, which feels significantly more natural than ‚ÄúI have a dream. I have a dream‚Äù. We‚Äôre making progress!\nLet‚Äôs see how this decision tree differs from the previous one.\n\n# Plot graph\nplot_graph(graph, length, beams, 'sequence')\n\n\n\n\n\n\nYou can see how the nodes differ significantly from the previous iteration, making more diverse choices. Although the sequence score of this new outcome might not be the highest (-1.01 instead of -0.69 previously), it‚Äôs important to remember that higher scores do not always lead to more realistic or meaningful sequences.\nNow that we‚Äôve introduced top-k sampling, we have to present the other most popular sampling technique: nucleus sampling."
  },
  {
    "objectID": "posts/2023-06-07-Decoding_strategies.html#nucleus-sampling",
    "href": "posts/2023-06-07-Decoding_strategies.html#nucleus-sampling",
    "title": "Decoding Strategies in Large Language Models",
    "section": "üî¨ Nucleus sampling",
    "text": "üî¨ Nucleus sampling\nNucleus sampling, also known as top-p sampling, takes a different approach from top-k sampling. Rather than selecting the top k most probable tokens, nucleus sampling chooses a cutoff value p such that the sum of the probabilities of the selected tokens exceeds p. This forms a ‚Äúnucleus‚Äù of tokens from which to randomly choose the next token.\nIn other words, the model examines its top probable tokens in descending order and keeps adding them to the list until the total probability surpasses the threshold p. Unlike top-k sampling, the number of tokens included in the nucleus can vary from step to step. This variability often results in a more diverse and creative output, making nucleus sampling popular for tasks such as text generation.\nTo implement the nucleus sampling method, we can use the ‚Äúnucleus‚Äù parameter in the beam_search() function. In this example, we‚Äôll set the value of p to 0.5. To make it easier, we‚Äôll include a minimum number of tokens equal to the number of beams. We‚Äôll also consider tokens with cumulative probabilities lower than p, rather than higher. It‚Äôs worth noting that while the details may differ, the core idea of nucleus sampling remains the same.\n\ndef nucleus_sampling(logits, temperature, p, beams, plot=True):\n    assert p &gt; 0\n    assert p &lt;= 1\n\n    # Sort the probabilities in descending order and compute cumulative probabilities\n    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n    probabilities = torch.nn.functional.softmax(sorted_logits / temperature, dim=-1)\n    cumulative_probabilities = torch.cumsum(probabilities, dim=-1)\n\n    # Create a mask for probabilities that are in the top-p\n    mask = cumulative_probabilities &lt; p\n\n    # If there's not n index where cumulative_probabilities &lt; p, we use the top n tokens instead\n    if mask.sum() &gt; beams:\n        top_p_index_to_keep = torch.where(mask)[0][-1].detach().cpu().tolist()\n    else:\n        top_p_index_to_keep = beams\n\n    # Only keep top-p indices\n    indices_to_remove = sorted_indices[top_p_index_to_keep:]\n    sorted_logits[indices_to_remove] = float('-inf')\n\n    # Sample n tokens from the resulting distribution\n    probabilities = torch.nn.functional.softmax(sorted_logits / temperature, dim=-1)\n    next_tokens = torch.multinomial(probabilities, beams)\n\n    # Plot distribution\n    if plot:\n        total_prob = torch.nn.functional.softmax(logits / temperature, dim=-1)\n        plot_prob_distribution(total_prob, next_tokens, 'nucleus', top_p_index_to_keep)\n\n    return next_tokens\n\n# Start generating text\nbeam_search(input_ids, 0, bar, length, beams, 'nucleus', 1)\n\n\n\n\n\n\nIn this plot, you can see that the number of tokens included in the nucleus fluctuates a lot. The generated probability distributions vary considerably, leading to the selection of tokens that are not always among the most probable ones. This opens the door to the generation of unique and varied sequences. Now, let‚Äôs observe the text it generated.\n\nsequence, max_score = get_best_sequence(graph)\nprint(f\"Generated text: {sequence}\")\n\nGenerated text: I have a dream. I'm going to\n\n\nThe nucleus sampling algorithm produces the sequence: ‚ÄúI have a dream. I‚Äôm going to‚Äù, which shows a notable enhancement in semantic coherence compared to greedy sampling.\nTo compare the decision paths, let‚Äôs visualize the new tree nucleus sampling generated.\n\n# Plot graph\nplot_graph(graph, length, beams, 'sequence')\n\n\n\n\n\n\nAs with top-k sampling, this tree is very different from the one generated with greedy sampling, displaying more variety. Both top-k and nucleus sampling offer unique advantages when generating text, enhancing diversity, and introducing creativity into the output. Your choice between the two methods (or even greedy search) will depend on the specific requirements and constraints of your project."
  },
  {
    "objectID": "posts/2023-06-07-Decoding_strategies.html#conclusion",
    "href": "posts/2023-06-07-Decoding_strategies.html#conclusion",
    "title": "Decoding Strategies in Large Language Models",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we have delved deep into various decoding methods used by LLMs, specifically GPT-2. We started with a simply greedy search and its immediate (yet often suboptimal) selection of the most probable next token. Next, we introduced the beam search technique, which considers several of the most likely tokens at each step. Although it offers more nuanced results, beam search can sometimes fall short in generating diverse and creative sequences.\nTo bring more variability into the process, we then moved on to top-k sampling and nucleus sampling. Top-k sampling diversifies the text generation by randomly selecting among the k most probable tokens, while nucleus sampling takes a different path by dynamically forming a nucleus of tokens based on cumulative probability. Each of these methods brings unique strengths and potential drawbacks to the table, and the specific requirements of your project will largely dictate the choice among them.\nUltimately, understanding these techniques and their trade-offs will equip you to better guide the LLMs towards producing increasingly realistic, nuanced, and compelling textual output.\nIf you‚Äôre interested in more technical content around LLMs, you can follow me on Twitter @maximelabonne."
  },
  {
    "objectID": "posts/4_bit_Quantization_with_GPTQ.html",
    "href": "posts/4_bit_Quantization_with_GPTQ.html",
    "title": "4-bit LLM Quantization with GPTQ",
    "section": "",
    "text": "Recent advancements in weight quantization allow us to run massive large language models on consumer hardware, like a LLaMA-30B model on an RTX 3090 GPU. This is possible thanks to novel 4-bit quantization techniques with minimal performance degradation, like GPTQ, GGML, and NF4.\nIn the previous article, we introduced na√Øve 8-bit quantization techniques and the excellent LLM.int8(). In this article, we will explore the popular GPTQ algorithm to understand how it works and implement it using the AutoGPTQ library.\nYou can find the code on Google Colab and GitHub."
  },
  {
    "objectID": "posts/4_bit_Quantization_with_GPTQ.html#optimal-brain-quantization",
    "href": "posts/4_bit_Quantization_with_GPTQ.html#optimal-brain-quantization",
    "title": "4-bit LLM Quantization with GPTQ",
    "section": "üß† Optimal Brain Quantization",
    "text": "üß† Optimal Brain Quantization\nLet‚Äôs start by introducing the problem we‚Äôre trying to solve. For every layer \\ell in the network, we want to find a quantized version \\widehat{\\mathbf{W}}_\\ell of the original weights \\mathbf{W}_\\ell. This is called the layer-wise compression problem. More specifically, to minimize performance degradation, we want the outputs (\\mathbf{\\widehat{W}_\\ell X_\\ell}) of these new weights to be as close as possible to the original ones (\\mathbf{W_\\ell X_\\ell}). In other words, we want to find:\n\\arg \\min_{\\mathbf{\\widehat{W}}_\\ell} \\parallel\\mathbf{W_\\ell X_\\ell} - \\mathbf{\\widehat{W}_\\ell X_\\ell}\\parallel_2^2.\nDifferent approaches have been proposed to solve this problem, but we‚Äôre interested in the Optimal Brain Quantizer (OBQ) framework here.\nThis method is inspired by a pruning technique to carefully remove weights from a fully trained dense neural network (Optimal Brain Surgeon). It uses an approximation technique and provides explicit formulas for the best single weight w_q to remove and optimal update \\delta_F to adjust the set of remaining non-quantized weights F to make up for the removal:\n\\begin{align*}\nw_q &= \\arg\\min_{w_q} \\frac{(\\text{quant}(w_q) - w_q)^2}{[\\mathbf{H}_F^{-1}]_{qq}},\\\\ \\quad \\delta_F &= -\\frac{w_q - \\text{quant}(w_q)}{[\\mathbf{H}_F^{-1}]_{qq}} \\cdot (\\mathbf{H}_F^{-1})_{:,q}.\n\\end{align*}\nwhere \\text{quant}(w) is the weight rounding given by the quantization and \\mathbf{H}_F is the Hessian.\nUsing OBQ, we can quantize the easiest weight first and then adjust all remaining non-quantized weights to compensate for this precision loss. Then we pick the next weight to quantize, and so on.\nA potential issue with this approach is when there are outlier weights, which can result in high quantization error. Usually, these outliers would be quantized last, when there are few non-quantized weights left that could be adjusted to compensate for the large error. This effect can worsen when some weights are pushed further outside the grid by intermediate updates. A simple heuristic is applied to prevent this: outliers are quantized as soon as they appear.\nThis process could be computationally heavy, especially for LLMs. To deal with this, the OBQ method uses a trick that avoids redoing the entire computation each time a weight is simplified. After quantizing a weight, it adjusts the matrix used in calculations (the Hessian) by removing the row and column associated with that weight (using Gaussian elimination):\n\n\\mathbf{H}^{-1}_{-q} = \\left( \\mathbf{H}^{-1} - \\frac{1}{[\\mathbf{H}^{-1}]_{qq}} \\mathbf{H}^{-1}_{:,q} \\mathbf{H}^{-1}_{q,:} \\right)_{-p}.\n\nThe method also employs vectorization to process multiple rows of the weight matrix at once. Despite its efficiency, the OBQ‚Äôs computation time increases significantly as the size of the weight matrix increases. This cubic growth makes it difficult to use OBQ on very large models with billions of parameters."
  },
  {
    "objectID": "posts/4_bit_Quantization_with_GPTQ.html#the-gptq-algorithm",
    "href": "posts/4_bit_Quantization_with_GPTQ.html#the-gptq-algorithm",
    "title": "4-bit LLM Quantization with GPTQ",
    "section": "üßÆ The GPTQ Algorithm",
    "text": "üßÆ The GPTQ Algorithm\nIntroduced by Frantar et al.¬†(2023), the GPTQ algorithm takes inspiration from the OBQ method, but with significant improvements to scale it for (very) large language models.\n\nStep 1: Arbitrary Order Insight\nThe OBQ method selects weights (parameters in a model) for quantization in a certain order, determined by which will add the least additional error. However, GPTQ observes that for large models, quantizing weights in any fixed order can perform just as well. This is because even though some weights might introduce more error individually, they are quantized later in the process when there are few other weights left that could increase the error. So the order doesn‚Äôt matter as much as we thought.\nBased on this insight, GPTQ aims to quantize all weights in the same order for all rows of a matrix. This makes the process faster because certain computations have to be done only once for each column, rather than once for each weight.\n\n\n\n\n\nStep 2: Lazy Batch-Updates\nThis scheme won‚Äôt be fast because it requires updating a huge matrix with very few computations for each entry. This type of operation can‚Äôt utilize the full compute capabilities of GPUs and will be slowed down by memory limitations (memory throughput bottleneck).\nTo resolve this, GPTQ introduces ‚Äúlazy batch‚Äù updates. It turns out that the final rounding decisions for a given column are only affected by updates performed on that column, not on later columns. Therefore, GPTQ can apply the algorithm to a batch of columns at a time (like 128 columns), updating only those columns and a corresponding block of the matrix. After a block is fully processed, the algorithm performs global updates on the entire matrix.\n\\begin{align*}\n\\mathbf{\\delta}_F &= -(\\mathbf{w}_Q - \\text{quant}(\\mathbf{w}_Q))([\\mathbf{H}_F^{-1}]_{QQ})^{-1} (\\mathbf{H}_F^{-1})_{:,Q}, \\\\\n\\mathbf{H}^{-1}_{-Q} &= \\left(\\mathbf{H}^{-1} - \\mathbf{H}^{-1}_{:,Q}([\\mathbf{H}_F^{-1}]_{QQ})^{-1}\\mathbf{H}^{-1}_{Q,:}\\right)_{-Q}.\n\\end{align*}\n\n\nStep 3: Cholesky Reformulation\nHowever, there‚Äôs one more issue to address. When the algorithm scales up to very large models, numerical inaccuracies can become a problem. Specifically, repeated applications of a certain operation can accumulate numerical errors.\nTo tackle this, GPTQ uses a Cholesky decomposition, a numerically stable method for solving certain mathematical problems. It involves precomputing some required information from the matrix using the Cholesky method. This approach, combined with a slight ‚Äúdampening‚Äù (adding a small constant to diagonal elements of the matrix), helps the algorithm to avoid numerical issues.\nThe full algorithm can be summarized in a few steps:\n\nThe GPTQ algorithm begins with a Cholesky decomposition of the Hessian inverse (a matrix that helps decide how to adjust the weights)\nIt then runs in loops, handling batches of columns at a time.\nFor each column in a batch, it quantizes the weights, calculates the error, and updates the weights in the block accordingly.\nAfter processing the batch, it updates all remaining weights based on the block‚Äôs errors.\n\nThe GPTQ algorithm was tested on various language generation tasks. It was compared with other quantization methods, like rounding all weights to the nearest quantized value (RTN). GPTQ was used with the BLOOM (176B parameters) and OPT (175B parameters) model families, and models were quantized using a single NVIDIA A100 GPU."
  },
  {
    "objectID": "posts/4_bit_Quantization_with_GPTQ.html#quantize-an-llm-with-autogptq",
    "href": "posts/4_bit_Quantization_with_GPTQ.html#quantize-an-llm-with-autogptq",
    "title": "4-bit LLM Quantization with GPTQ",
    "section": "üíª Quantize an LLM with AutoGPTQ",
    "text": "üíª Quantize an LLM with AutoGPTQ\nGPTQ has been very popular to create models in 4-bit precision that can efficiently run on GPUs. You can find many examples on the Hugging Face Hub, especially from TheBloke. If you‚Äôre looking for an approach that is more CPU-friendly, GGML is currently your best option. Finally, the transformers library with bitsandbytes allows you to quantize a model when it‚Äôs loaded using the load_in_4bit=true argument, which requires downloading full models and storing them in your RAM.\nLet‚Äôs implement the GPTQ algorithm using the AutoGPTQ library and quantize a GPT-2 model. This requires a GPU, but a free T4 on Google Colab will do. We start by loading the libraries and defining the model we want to quantize (in this case, GPT-2).\n\n!BUILD_CUDA_EXT=0 pip install -q auto-gptq transformers\n\n\nimport random\n\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nfrom datasets import load_dataset\nimport torch\nfrom transformers import AutoTokenizer\n\n\n# Define base model and output directory\nmodel_id = \"gpt2\"\nout_dir = model_id + \"-GPTQ\"\n\nWe now want to load the model and the tokenizer. The tokenizer is loaded using the classic AutoTokenizer class from the transformers library. On the other hand, we need to pass a specific configuration (BaseQuantizeConfig) to load the model.\nIn this configuration, we can specify the number of bits to quantize (here, bits=4) and the group size (size of the lazy batch). Note that this group size is optional: we could also use one set of parameters for the entire weight matrix. In practice, these groups generally improve the quality of the quantization at a very low cost (especially with group_size=1024). The damp_percent value is here to help the Cholesky reformulation and should not be changed.\nFinally, the desc_act (also called act order) is a tricky parameter. It allows you to process rows based on decreasing activation, meaning the most important or impactful rows (determined by sampled inputs and outputs) are processed first. This method aims to place most of the quantization error (inevitably introduced during quantization) on less significant weights. This approach improves the overall accuracy of the quantization process by ensuring the most significant weights are processed with greater precision. However, when used alongside group size, desc_act can lead to performance slowdowns due to the need to frequently reload quantization parameters. For this reason, we won‚Äôt use it here (it will probably be fixed in the future, however).\n\n# Load quantize config, model and tokenizer\nquantize_config = BaseQuantizeConfig(\n    bits=4,\n    group_size=128,\n    damp_percent=0.01,\n    desc_act=False,\n)\nmodel = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nThe quantization process relies heavily on samples to evaluate and enhance the quality of the quantization. They provide a means of comparison between the outputs produced by the origina and the newly quantized model. The larger the number of samples provided, the greater the potential for more accurate and effective comparisons, leading to improved quantization quality.\nIn the context of this article, we utilize the C4 (Colossal Clean Crawled Corpus) dataset to generate our samples. The C4 dataset is a large-scale, multilingual collection of web text gathered from the Common Crawl project. This expansive dataset has been cleaned and prepared specifically for training large-scale language models, making it a great resource for tasks such as this. The WikiText dataset is another popular option.\nIn the following code block, we load 1024 samples from the C4 dataset, tokenize them, and format them.\n\n# Load data and tokenize examples\nn_samples = 1024\ndata = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.00001-of-01024.json.gz\", split=f\"train[:{n_samples*5}]\")\ntokenized_data = tokenizer(\"\\n\\n\".join(data['text']), return_tensors='pt')\n\n# Format tokenized examples\nexamples_ids = []\nfor _ in range(n_samples):\n    i = random.randint(0, tokenized_data.input_ids.shape[1] - tokenizer.model_max_length - 1)\n    j = i + tokenizer.model_max_length\n    input_ids = tokenized_data.input_ids[:, i:j]\n    attention_mask = torch.ones_like(input_ids)\n    examples_ids.append({'input_ids': input_ids, 'attention_mask': attention_mask})\n\nWARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/allenai___json/allenai--c4-6e494e9c0ee1404e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\nToken indices sequence length is longer than the specified maximum sequence length for this model (2441065 &gt; 1024). Running this sequence through the model will result in indexing errors\n\n\nNow that dataset is ready, we can start the quantization process with a batch size of 1. Optionally, we also use OpenAI Triton, a CUDA alternative, to communicate with the GPU. Once this is done, we save the tokenizer and the model in a safetensors format.\n\n%%time\n\n# Quantize with GPTQ\nmodel.quantize(\n    examples_ids,\n    batch_size=1,\n    use_triton=True,\n)\n\n# Save model and tokenizer\nmodel.save_quantized(out_dir, use_safetensors=True)\ntokenizer.save_pretrained(out_dir)\n\nCPU times: user 4min 35s, sys: 3.49 s, total: 4min 39s\nWall time: 5min 8s\n\n\n('gpt2-GPTQ/tokenizer_config.json',\n 'gpt2-GPTQ/special_tokens_map.json',\n 'gpt2-GPTQ/vocab.json',\n 'gpt2-GPTQ/merges.txt',\n 'gpt2-GPTQ/added_tokens.json',\n 'gpt2-GPTQ/tokenizer.json')\n\n\nAs per usual, the model and tokenizer can then be loaded from the output directory using the AutoGPTQForCausalLM and AutoTokenizer classes.\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n# Reload model and tokenizer\nmodel = AutoGPTQForCausalLM.from_quantized(\n    out_dir,\n    device=device,\n    use_triton=True,\n    use_safetensors=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(out_dir)\n\nWARNING:accelerate.utils.modeling:The safetensors archive passed at gpt2-GPTQ/gptq_model-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\nWARNING:auto_gptq.modeling._base:GPT2GPTQForCausalLM hasn't fused attention module yet, will skip inject fused attention.\nWARNING:auto_gptq.modeling._base:GPT2GPTQForCausalLM hasn't fused mlp module yet, will skip inject fused mlp.\n\n\nLet‚Äôs check that the model is working correctly. The AutoGPTQ model (mostly) works as a normal transformers model, which makes it compatible with inference pipelines, as shown in the following example:\n\nfrom transformers import pipeline\n\ngenerator = pipeline('text-generation', model=model, tokenizer=tokenizer)\nresult = generator(\"I have a dream\", do_sample=True, max_length=50)[0]['generated_text']\nprint(result)\n\nThe model 'GPT2GPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nI have a dream,\" she told CNN last week. \"I have this dream of helping my mother find her own. But, to tell that for the first time, now that I'm seeing my mother now, just knowing how wonderful it is that\n\n\nWe managed to get a convincing completion from our quantized GPT-2 model. A more in-depth evaluation would require measuring the perplexity of the quantized model versus the original one. However, we will leave it out of the scope of this article."
  },
  {
    "objectID": "posts/4_bit_Quantization_with_GPTQ.html#conclusion",
    "href": "posts/4_bit_Quantization_with_GPTQ.html#conclusion",
    "title": "4-bit LLM Quantization with GPTQ",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we introduced the GPTQ algorithm, a state-of-the-art quantization technique to run LLMs on consumer-grade hardware. We showed how it addresses the layer-wise compression problem, based on an improved OBS technique with arbitrary order insight, lazy batch updates, and Cholesky reformulation. This novel approach significantly reduces memory and computation requirements, making LLMs accessible to a broader audience.\nIn addition, we quantized our own LLM model on a free T4 GPU and ran it to generate text. You can push your own version of a GPTQ 4-bit quantized model on the Hugging Face Hub. As mentioned in the introduction, GPTQ is not the only 4-bit quantization algorithm: GGML and NF4 are excellent alternatives with slightly different scopes. I encourage you to learn more about them and give them a shot!\nIf you‚Äôre interested in more technical content around LLMs, follow me on Twitter @maximelabonne."
  },
  {
    "objectID": "posts/4_bit_Quantization_with_GPTQ.html#references",
    "href": "posts/4_bit_Quantization_with_GPTQ.html#references",
    "title": "4-bit LLM Quantization with GPTQ",
    "section": "References",
    "text": "References\n\nB. Hassibi, D. G. Stork and G. J. Wolff, ‚ÄúOptimal Brain Surgeon and general network pruning,‚Äù IEEE International Conference on Neural Networks, San Francisco, CA, USA, 1993, pp.¬†293-299 vol.1, doi: 10.1109/ICNN.1993.298572.\nElias Frantar, Sidak Pal Singh, & Dan Alistarh. (2023). Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, & Dan Alistarh. (2023). GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, & Peter J. Liu. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer."
  },
  {
    "objectID": "posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html",
    "href": "posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html",
    "title": "Improve ChatGPT with Knowledge Graphs",
    "section": "",
    "text": "ChatGPT has shown impressive capabilities in processing and generating human-like text. However, it is not without its imperfections. A primary concern is the model‚Äôs propensity to produce either inaccurate or obsolete answers, often called ‚Äúhallucinations.‚Äù\nThe New York Times recently highlighted this issue in their article, ‚ÄúHere‚Äôs What Happens When Your Lawyer Uses ChatGPT.‚Äù It presents a lawsuit where a lawyer leaned heavily on ChatGPT to assist in preparing a court filing for a client suing an airline. The model generated fictional court decisions to back its arguments, which didn‚Äôt go unnoticed. This incident underscores the need for solutions to ground AI models like ChatGPT and improve their performance.\nTo address this, we propose an approach that focuses on augmenting ChatGPT using a knowledge graph. This method aims to provide a structured context, ensuring the model outputs are accurate but also relevant and up-to-date. By bridging the gap between the unstructured textual world of ChatGPT and the structured clarity of knowledge graphs, we strive to enhance the effectiveness and reliability of AI language models.\nAll the code used in this article is available on Google Colab and on GitHub."
  },
  {
    "objectID": "posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html#what-is-a-knowledge-graph",
    "href": "posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html#what-is-a-knowledge-graph",
    "title": "Improve ChatGPT with Knowledge Graphs",
    "section": "What is a knowledge graph?",
    "text": "What is a knowledge graph?\nA knowledge graph is a structured format of knowledge representation, usually composed of entities and relationships. In a typical knowledge graph, entities are the nodes, and the relationships between them are the edges. The graph-based representation allows complex relationships to be modeled in a way that‚Äôs intuitive and closer to human understanding. Here is a simple illustration of a knowledge graph:\n\n\n\n\nSource: Wikipedia. CC BY-SA 4.0\n\n\nGoogle has been using knowledge graphs since 2012 to provide additional contextual information and sources. The structured representation of data offers a new dimension of context to the AI model, grounding it in validated knowledge."
  },
  {
    "objectID": "posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html#applying-knowledge-graphs-to-improve-chatgpt",
    "href": "posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html#applying-knowledge-graphs-to-improve-chatgpt",
    "title": "Improve ChatGPT with Knowledge Graphs",
    "section": "Applying Knowledge Graphs to Improve ChatGPT",
    "text": "Applying Knowledge Graphs to Improve ChatGPT\nA crucial limitation of ChatGPT is its lack of real-time information updates. Since the model was last trained using data up until 2021, it doesn‚Äôt have access to events, data, or context after that year. This leads to ChatGPT having outdated or incomplete information about events, technological advancements, or other critical happenings post-2021.\nLet‚Äôs illustrate this limitation by asking ChatGPT about a recent event, ‚ÄúWhen did Apple announce the Vision Pro?‚Äù. Given the model‚Äôs knowledge cutoff in 2021, we would expect it to be unaware of this announcement, which happened in 2023.\n\n!pip install -q openai langchain\n\n\nimport os\nimport openai\n\nos.environ['OPENAI_API_KEY'] = \"your-OpenAI-API-key\"\nopenai.api_key = os.environ['OPENAI_API_KEY']\n\nquestion = \"When did apple announced the Vision Pro?\"\ncompletion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n                                          temperature=0,\n                                          messages=[{\"role\": \"user\",\n                                                     \"content\": question}])\nprint(completion[\"choices\"][0][\"message\"][\"content\"])\n\nAs an AI language model, I do not have access to current events or real-time information. However, as of my last training data, Apple has not announced any product called \"Vision Pro.\" It is possible that this product does not exist or has not been announced yet.\n\n\nAs expected, ChatGPT is unable to provide the correct answer due to its training data limitations. This clearly highlights the need for constant updates to the model‚Äôs knowledge base, which can be addressed by integrating it with a continuously updated knowledge graph.\nBy implementing such a knowledge graph, we can ensure that ChatGPT can provide accurate, current, and reliable information, effectively addressing the ‚Äúhallucination‚Äù issues as well as the knowledge cutoff limitations.\n\nExperiment 1: Sentence-Level Knowledge Graphs\nTo demonstrate this, we‚Äôll use the LangChain library, a powerful tool designed for building frameworks around large language models. The library includes a component called GraphIndexCreator, which can parse a sentence and create a knowledge graph. This component is currently limited and cannot process long corpus of text, but it serves as a perfect starting point for our experiment.\nLet‚Äôs start with a straightforward sentence: ‚ÄúApple announced the Vision Pro in 2023.‚Äù\n\nfrom langchain.llms import OpenAI\nfrom langchain.indexes import GraphIndexCreator\nfrom langchain.chains import GraphQAChain\nfrom langchain.prompts import PromptTemplate\n\ntext = \"Apple announced the Vision Pro in 2023.\"\n\nindex_creator = GraphIndexCreator(llm=OpenAI(temperature=0))\ngraph = index_creator.from_text(text)\ngraph.get_triples()\n\n[('Apple', 'Vision Pro', 'announced'),\n ('Vision Pro', '2023', 'was announced in')]\n\n\nBy feeding this sentence into the GraphIndexCreator, it creates a knowledge graph by identifying the sentence‚Äôs entities and relationships, forming triplets of information in the format of (source node, relation, target node). However, the GraphIndexCreator might get confused with the relations and target nodes due to the inherent complexity of natural language.\nEven though it‚Äôs a tiny graph based on a single sentence, we can represent it visually using popular Python libraries such as matplotlib and networkx.\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create graph\nG = nx.DiGraph()\nG.add_edges_from((source, target, {'relation': relation}) for source, relation, target in graph.get_triples())\n\n# Plot the graph\nplt.figure(figsize=(8,5), dpi=300)\npos = nx.spring_layout(G, k=3, seed=0)\n\nnx.draw_networkx_nodes(G, pos, node_size=2000)\nnx.draw_networkx_edges(G, pos, edge_color='gray')\nnx.draw_networkx_labels(G, pos, font_size=12)\nedge_labels = nx.get_edge_attributes(G, 'relation')\nnx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=10)\n\n# Display the plot\nplt.axis('off')\nplt.show()\n\n\n\n\nNow, let‚Äôs enhance ChatGPT using the knowledge graph. We will use another component of the LangChain library, GraphQAChain, to this end.\nInitializing the GraphQAChain, we input the same question we asked earlier, ‚ÄúWhen did Apple announce the Vision Pro?‚Äù. This time, ChatGPT leverages the knowledge graph we‚Äôve just built.\n\nchain = GraphQAChain.from_llm(OpenAI(temperature=0), graph=graph, verbose=True)\nchain.run(question)\n\n\n\n&gt; Entering new GraphQAChain chain...\nEntities Extracted:\n Apple, Vision Pro\nFull Context:\nApple announced Vision ProVision Pro was announced in 2023\n\n&gt; Finished chain.\n\n\n' Apple announced Vision Pro in 2023.'\n\n\nThis time, ChatGPT was able to output the correct information! The good thing is that we don‚Äôt need any parser to build our knowledge graphs and can use existing ones. In the next experiment, let‚Äôs try to use a bigger graph and see if it‚Äôs still as performant.\n\n\nExperiment 2: Bigger Knowledge Graphs\nIn this experiment, we manually create this more complex graph by supplying a list of triplets to the GraphIndexCreator object using the add_triple() method. Each triplet represents a distinct piece of knowledge related to Apple, such as products it has created or where it is located.\n\nfrom langchain.graphs.networkx_graph import KnowledgeTriple\n\n# Knowledge graph\nkg = [\n    ('Apple', 'is', 'Company'),\n    ('Apple', 'created', 'iMac'),\n    ('Apple', 'created', 'iPhone'),\n    ('Apple', 'created', 'Apple Watch'),\n    ('Apple', 'created', 'Vision Pro'),\n\n    ('Apple', 'developed', 'macOS'),\n    ('Apple', 'developed', 'iOS'),\n    ('Apple', 'developed', 'watchOS'),\n\n    ('Apple', 'is located in', 'USA'),\n    ('Steve Jobs', 'co-founded', 'Apple'),\n    ('Steve Wozniak', 'co-founded', 'Apple'),\n    ('Tim Cook', 'is the CEO of', 'Apple'),\n\n    ('iOS', 'runs on', 'iPhone'),\n    ('macOS', 'runs on', 'iMac'),\n    ('watchOS', 'runs on', 'Apple Watch'),\n\n    ('Apple', 'was founded in', '1976'),\n    ('Apple', 'owns', 'App Store'),\n    ('App Store', 'sells', 'iOS apps'),\n\n    ('iPhone', 'announced in', '2007'),\n    ('iMac', 'announced in', '1998'),\n    ('Apple Watch', 'announced in', '2014'),\n    ('Vision Pro', 'announced in', '2023'),\n]\n\ngraph = index_creator.from_text('')\nfor (node1, relation, node2) in kg:\n    graph.add_triple(KnowledgeTriple(node1, relation, node2))\n\nAlthough we could include many more triplets (real-world knowledge graphs often encompass millions of nodes), the size of our graph for this demonstration is sufficient. When visualized, this more extensive knowledge graph exhibits greater complexity and a richer depiction of information.\n\n# Create directed graph\nG = nx.DiGraph()\nfor node1, relation, node2 in kg:\n    G.add_edge(node1, node2, label=relation)\n\n# Plot the graph\nplt.figure(figsize=(25, 25), dpi=300)\npos = nx.spring_layout(G, k=2, iterations=50, seed=0)\n\nnx.draw_networkx_nodes(G, pos, node_size=5000)\nnx.draw_networkx_edges(G, pos, edge_color='gray', edgelist=G.edges(), width=2)\nnx.draw_networkx_labels(G, pos, font_size=12)\nedge_labels = nx.get_edge_attributes(G, 'label')\nnx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=12)\n\n# Display the plot\nplt.axis('off')\nplt.show()\n\n\n\n\nWith this larger graph, we once again ask ChatGPT the question: ‚ÄúWhen did Apple announce the Vision Pro?‚Äù Leveraging the GraphQAChain object, ChatGPT processes the information embedded in the knowledge graph.\n\nchain = GraphQAChain.from_llm(OpenAI(temperature=0), graph=graph, verbose=True)\nchain.run(question)\n\n\n\n&gt; Entering new GraphQAChain chain...\nEntities Extracted:\n Apple, Vision Pro\nFull Context:\nApple is Company\nApple created iMac\nApple created iPhone\nApple created Apple Watch\nApple created Vision Pro\nApple developed macOS\nApple developed iOS\nApple developed watchOS\nApple is located in USA\nApple was founded in 1976\nApple owns App StoreVision Pro announced in 2023\n\n&gt; Finished chain.\n\n\n' Apple announced the Vision Pro in 2023.'\n\n\nChatGPT successfully extracts the correct information from the more expansive knowledge graph. This result demonstrates that our model can not only scale to larger graphs but can also efficiently navigate a more extensive knowledge base.\nThe possibilities of implementing larger and more diverse knowledge graphs are practically endless. They can be populated with data from various sources, such as legal documents, code documentation, scientific literature, and more, enhancing the AI‚Äôs understanding and response accuracy across multiple domains. The integration of ChatGPT and knowledge graphs thus holds immense promise for future AI development."
  },
  {
    "objectID": "posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html#conclusion",
    "href": "posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html#conclusion",
    "title": "Improve ChatGPT with Knowledge Graphs",
    "section": "Conclusion",
    "text": "Conclusion\nAs seen in our experiments, knowledge graphs can significantly aid in grounding and improving ChatGPT‚Äôs outputs. A key challenge with large knowledge graphs is finding connections between distant nodes, a problem often referred to as graph completion. Successfully addressing this issue would allow ChatGPT to make insightful connections and propose new ideas based on the information available in the knowledge graph.\nHowever, the process of integrating knowledge graphs into language models like ChatGPT is still an evolving field. To further explore the various applications and delve into the details of implementing knowledge graphs, consider the book Hands-On Graph Neural Networks Using Python, which provides a comprehensive guide on this subject. Through this type of research and experimentation, we can continuously improve AI‚Äôs ability to understand and generate text, moving us closer to more reliable and grounded AI models.\nIf you found it helpful, follow me on Twitter @maximelabonne for more graph-related content!"
  },
  {
    "objectID": "posts/A_Beginners_Guide_to_LLM_Finetuning.html",
    "href": "posts/A_Beginners_Guide_to_LLM_Finetuning.html",
    "title": "A Beginner‚Äôs Guide to LLM Fine-Tuning",
    "section": "",
    "text": "The growing interest in Large Language Models (LLMs) has led to a surge in tools and wrappers designed to streamline their training process.\nPopular options include FastChat from LMSYS (used to train Vicuna) and Hugging Face‚Äôs transformers/trl libraries (used in my previous article). In addition, each big LLM project, like WizardLM, tends to have its own training script, inspired by the original Alpaca implementation.\nIn this article, we will use Axolotl, a tool created by the OpenAccess AI Collective. We will use it to fine-tune a Code Llama 7b model on an evol-instruct dataset comprised of 1,000 samples of Python code."
  },
  {
    "objectID": "posts/A_Beginners_Guide_to_LLM_Finetuning.html#why-axolotl",
    "href": "posts/A_Beginners_Guide_to_LLM_Finetuning.html#why-axolotl",
    "title": "A Beginner‚Äôs Guide to LLM Fine-Tuning",
    "section": "ü§î Why Axolotl?",
    "text": "ü§î Why Axolotl?\nThe main appeal of Axolotl is that it provides a one-stop solution, which includes numerous features, model architectures, and an active community. Here‚Äôs a quick list of my favorite things about it:\n\nConfiguration: All parameters used to train an LLM are neatly stored in a yaml config file. This makes it convenient for sharing and reproducing models. You can see an example for Llama 2 here.\nDataset Flexibility: Axolotl allows the specification of multiple datasets with varied prompt formats such as alpaca ({\"instruction\": \"...\", \"input\": \"...\", \"output\": \"...\"}), sharegpt:chat ({\"conversations\": [{\"from\": \"...\", \"value\": \"...\"}]}), and raw completion ({\"text\": \"...\"}). Combining datasets is seamless, and the hassle of unifying the prompt format is eliminated.\nFeatures: Axolotl is packed with SOTA techniques such as FSDP, deepspeed, LoRA, QLoRA, ReLoRA, sample packing, GPTQ, FlashAttention, xformers, and rope scaling.\nUtilities: There are numerous user-friendly utilities integrated, including the addition or alteration of special tokens, or a custom wandb configuration.\n\nSome well-known models trained using this tool are Manticore-13b from the OpenAccess AI Collective and Samantha-1.11-70b from Eric Hartford. Like other wrappers, it is built on top of the transformers library and uses many of its features."
  },
  {
    "objectID": "posts/A_Beginners_Guide_to_LLM_Finetuning.html#create-your-own-config-file",
    "href": "posts/A_Beginners_Guide_to_LLM_Finetuning.html#create-your-own-config-file",
    "title": "A Beginner‚Äôs Guide to LLM Fine-Tuning",
    "section": "‚öôÔ∏è Create your own config file",
    "text": "‚öôÔ∏è Create your own config file\nBefore anything, we need a configuration file. You can reuse an existing configuration from the examples folder. In our case, we will tweak the QLoRA config for Llama 2 to create our own Code Llama model. The model will be trained on a subset of 1,000 Python samples from the nickrosh/Evol-Instruct-Code-80k-v1 dataset.\nFirst, we must change the base_model and base_model_config fields to ‚Äúcodellama/CodeLlama-7b-hf‚Äù. To push our trained adapter to the Hugging Face Hub, let‚Äôs add a new field hub_model_id, which corresponds to the name of our model, ‚ÄúEvolCodeLlama-7b‚Äù. Now, we have to update the dataset to mlabonne/Evol-Instruct-Python-1k and set type to ‚Äúalpaca‚Äù.\nThere‚Äôs no sample bigger than 2048 tokens in this dataset, so we can reduce the sequence_len to ‚Äú2048‚Äù and save some VRAM. Talking about VRAM, we‚Äôre going to use a micro_batch_size of 10 and a gradient_accumulation_steps of 1 to maximize its use. In practice, you try different values until you use &gt;95% of the available VRAM.\nFor convenience, I‚Äôm going to add the name ‚Äúaxolotl‚Äù to the wandb_project field so it‚Äôs easier to track on my account. I‚Äôm also setting the warmup_steps to ‚Äú100‚Äù (personal preference) and the eval_steps to 0.01 so we‚Äôll end up with 100 evaluations.\nHere‚Äôs how the final config file should look:\nbase_model: codellama/CodeLlama-7b-hf\nbase_model_config: codellama/CodeLlama-7b-hf\nmodel_type: LlamaForCausalLM\ntokenizer_type: LlamaTokenizer\nis_llama_derived_model: true\nhub_model_id: EvolCodeLlama-7b\n\nload_in_8bit: false\nload_in_4bit: true\nstrict: false\n\ndatasets:\n    - path: mlabonne/Evol-Instruct-Python-1k\n    type: alpaca\ndataset_prepared_path: last_run_prepared\nval_set_size: 0.02\noutput_dir: ./qlora-out\n\nadapter: qlora\nlora_model_dir:\n\nsequence_len: 2048\nsample_packing: true\n\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_modules:\nlora_target_linear: true\nlora_fan_in_fan_out:\n\nwandb_project: axolotl\nwandb_entity:\nwandb_watch:\nwandb_run_id:\nwandb_log_model:\n\ngradient_accumulation_steps: 1\nmicro_batch_size: 10\nnum_epochs: 3\noptimizer: paged_adamw_32bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: true\nfp16: false\ntf32: false\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\nwarmup_steps: 100\neval_steps: 0.01\nsave_strategy: epoch\nsave_steps:\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\n    bos_token: \"&lt;s&gt;\"\n    eos_token: \"&lt;/s&gt;\"\n    unk_token: \"&lt;unk&gt;\"\nYou can also find this config file here as a GitHub gist.\nBefore we start training our model, I want to introduce a few parameters that are important to understand:\n\nQLoRA: We‚Äôre using QLoRA for fine-tuning, which is why we‚Äôre loading the base model in 4-bit precision (NF4 format). You can check this article from Benjamin Marie to know more about QLoRA.\nGradient checkpointing: It lowers the VRAM requirements by removing some activations that are re-computed on demand during the backward pass. It also slows down training by about 20%, according to Hugging Face‚Äôs documentation.\nFlashAttention: This implements the FlashAttentionmechanism, which improves the speed and memory efficiency of our model thanks to a clever fusion of GPU operations (learn more about it in this article from Aleksa Gordi√ß).\nSample packing: Smart way of creating batches with as little padding as possible, by reorganizing the order of the samples (bin packing problem). As a result, we need fewer batches to train the model on the same dataset. It was inspired by the Multipack Sampler (see my note) and Krell et al.\n\nYou can find FlashAttention in some other tools, but sample packing is relatively new. As far as I know, OpenChatwas the first project to use sample packing during fine-tuning. Thanks to Axolotl, we‚Äôll use these techniques for free."
  },
  {
    "objectID": "posts/A_Beginners_Guide_to_LLM_Finetuning.html#fine-tune-code-llama",
    "href": "posts/A_Beginners_Guide_to_LLM_Finetuning.html#fine-tune-code-llama",
    "title": "A Beginner‚Äôs Guide to LLM Fine-Tuning",
    "section": "ü¶ô Fine-tune Code Llama",
    "text": "ü¶ô Fine-tune Code Llama\nHaving the config file ready, it‚Äôs time to get our hands dirty with the actual fine-tuning. You might consider running the training on a Colab notebook. However, for those without access to a high-performance GPU, a more cost-effective solution consists of renting cloud-based GPU services, like AWS, Lambda Labs, Vast.ai, Banana, or RunPod.\nPersonally, I use RunPod, which is a popular option in the fine-tuning community. It‚Äôs not the cheapest service but it hits a good tradeoff with a clean UI. You can easily replicate the following steps using your favorite service.\nWhen your RunPod account is set up, go to Manage &gt; Templates and click on ‚ÄúNew Template‚Äù. Here is a simple template:\n\n\n\nLet‚Äôs review the different fields and their corresponding values:\n\nTemplate Name: Axolotl (you can choose whatever you want)\nContainer Image: winglian/axolotl-runpod:main-py3.10-cu118-2.0.1\nContainer Disk: 100 GB\nVolume Disk: 0 GB\nVolume Mount Path: /workspace\n\nIn addition, there are two handy environment variables can include:\n\nHUGGING_FACE_HUB_TOKEN: you can find your token on this page (requires an account)\nWANDB_API_KEY: you can find your key on this page (requires an account)\n\nAlternatively, you can simply log in the terminal later (using huggingface-cli login and wandb login). Once you‚Äôre set-up, go to Community Cloud and deploy an RTX 3090. Here you can search for the name of your template and select it as follows:\n\n\n\nYou can click on ‚ÄúContinue‚Äù and RunPod will deploy your template. You can see the installation in your pod‚Äôs logs (Manage &gt; Pods). When the option becomes available, click on ‚ÄúConnect‚Äù. Here, click on ‚Äútart Web Terminal‚Äù and then ‚ÄúConnect to Web Terminal‚Äù. You are now connected to your pod!\nThe following steps are the same no matter what service you choose:\n\nWe install Axolotl and the PEFT library as follows:\n\ngit clone https://github.com/OpenAccess-AI-Collective/axolotl\ncd axolotl\n\npip3 install -e .[flash-attn]\npip3 install -U git+https://github.com/huggingface/peft.git\n\nDownload the config file we created:\n\nwget https://gist.githubusercontent.com/mlabonne/8055f6335e2b85f082c8c75561321a66/raw/93915a9563fcfff8df9a81fc0cdbf63894465922/EvolCodeLlama-7b.yaml\n\nYou can now start fine-tuning the model with the following command:\n\naccelerate launch scripts/finetune.py EvolCodeLlama-7b.yaml\nIf everything is configured correctly, you should be able to train the model in a little more than one hour (it took me 1h 11m 44s). If you check the GPU memory used, you‚Äôll see almost 100% with this config, which means we‚Äôre optimizing it pretty nicely. If you‚Äôre using a GPU with more VRAM (like an A100), you can increase the micro-batch size to make sure you‚Äôre fully using it.\nIn the meantime, feel free to close the web terminal and check your loss on Weights & Biases. We‚Äôre using tmux so the training won‚Äôt stop if you close the terminal. Here are my loss curves:\n\n\n\nWe see a steady improvement in the eval loss, which is a good sign. However, you can also spot drops in the eval loss that are not correlated with a decrease in the quality of the outputs. The best way to evaluate your model is simply by using it: you can run it in the terminal with the command accelerate launch scripts/finetune.py EvolCodeLlama-7b.yaml ‚Äìinference ‚Äìlora_model_dir=‚Äú./qlora-out‚Äù.\nThe QLoRA adapter should already be uploaded to the Hugging Face Hub. However, you can also merge the base Code Llama model with this adapter and push the merged model there by following these steps:\n\nDownload this script:\n\nwget https://gist.githubusercontent.com/mlabonne/a3542b0519708b8871d0703c938bba9f/raw/60abc5afc07f9d843bc23d56f4e0b7ab072c4a62/merge_peft.py\n\nExecute it with this command:\n\npython merge_peft.py --base_model=codellama/CodeLlama-7b-hf --peft_model=./qlora-out --hub_id=EvolCodeLlama-7b\nCongratulations, you should have your own EvolCodeLlama-7b on the Hugging Face Hub at this point! For reference, you can access my own model trained with this process here: mlabonne/EvolCodeLlama-7b\nConsidering that our EvolCodeLlama-7b is a code LLM, it would be interesting to compare its performance with other models on standard benchmarks, such as HumanEval and MBPP. For reference, you can find a leaderboard at the following address: Multilingual Code Evals.\nIf you‚Äôre happy with this model, you can quantize it with GGML for local inference with this free Google Colab notebook. You can also fine-tune bigger models (e.g., 70b parameters) thanks to deepspeed, which only requires an additional config file."
  },
  {
    "objectID": "posts/A_Beginners_Guide_to_LLM_Finetuning.html#conclusion",
    "href": "posts/A_Beginners_Guide_to_LLM_Finetuning.html#conclusion",
    "title": "A Beginner‚Äôs Guide to LLM Fine-Tuning",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we‚Äôve covered the essentials of how to efficiently fine-tune LLMs. We customized parameters to train on our Code Llama model on a small Python dataset. Finally, we merged the weights and uploaded the result on Hugging Face.\nI hope you found this guide useful. I recommend using Axolotl with a cloud-based GPU service to get some experience and upload a few models on Hugging Face. Build your own datasets, play with the parameters, and break stuff along the way. Like with every wrapper, don‚Äôt hesitate to check the source code to get a good intuition of what it‚Äôs actually doing. It will massively help in the long run.\nThanks to the OpenAccess AI Collective and all the contributors!"
  },
  {
    "objectID": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html",
    "href": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html",
    "title": "Fine-Tune Your Own Llama 2 Model in a Colab Notebook",
    "section": "",
    "text": "With the release of LLaMA v1, we saw a Cambrian explosion of fine-tuned models, including Alpaca, Vicuna, WizardLM, among others. This trend encouraged different businesses to launch their own base models with licenses suitable for commercial use, such as OpenLLaMA, Falcon, XGen, etc. The release of Llama 2 now combines the best elements from both sides: it offers a highly efficient base model along with a more permissive license.\nDuring the first half of 2023, the software landscape was significantly shaped by the widespread use of APIs (like OpenAI API) to create infrastructures based on Large Language Models (LLMs). Libraries such as LangChain and LlamaIndex played a critical role in this trend. Moving into the latter half of the year, the process of fine-tuning (or instruction tuning) these models is set to become a standard procedure in the LLMOps workflow. This trend is driven by various factors: the potential for cost savings, the ability to process confidential data, and even the potential to develop models that exceed the performance of prominent models like ChatGPT and GPT-4 in certain specific tasks.\nIn this article, we will see why instruction tuning works and how to implement it in a Google Colab notebook to create your own Llama 2 model. As usual, the code is available on Colab and GitHub."
  },
  {
    "objectID": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#background-on-fine-tuning-llms",
    "href": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#background-on-fine-tuning-llms",
    "title": "Fine-Tune Your Own Llama 2 Model in a Colab Notebook",
    "section": "üîß Background on fine-tuning LLMs",
    "text": "üîß Background on fine-tuning LLMs\n\n\n\n\nLLMs are pretrained on an extensive corpus of text. In the case of Llama 2, we know very little about the composition of the training set, besides its length of 2 trillion tokens. In comparison, BERT (2018) was ‚Äúonly‚Äù trained on the BookCorpus (800M words) and English Wikipedia (2,500M words). From experience, this is a very costly and long process with a lot of hardware issues. If you want to know more about it, I recommend reading Meta‚Äôs logbook about the pretraining of the OPT-175B model.\nWhen the pretraining is complete, auto-regressive models like Llama 2 can predict the next token in a sequence. However, this does not make them particularly useful assistants since they don‚Äôt reply to instructions. This is why we employ instruction tuning to align their answers with what humans expect. There are two main fine-tuning techniques:\n\nSupervised Fine-Tuning (SFT): Models are trained on a dataset of instructions and responses. It adjusts the weights in the LLM to minimize the difference between the generated answers and ground-truth responses, acting as labels.\nReinforcement Learning from Human Feedback (RLHF): Models learn by interacting with their environment and receiving feedback. They are trained to maximize a reward signal (using PPO), which is often derived from human evaluations of model outputs.\n\nIn general, RLHF is shown to capture more complex and nuanced human preferences, but is also more challenging to implement effectively. Indeed, it requires careful design of the reward system and can be sensitive to the quality and consistency of human feedback. A possible alternative in the future is the Direct Preference Optimization (DPO) algorithm, which directly runs preference learning on the SFT model.\nIn our case, we will perform SFT, but this raises a question: why does fine-tuning work in the first place? As highlighted in the Orca paper, our understanding is that fine-tuning leverages knowledge learned during the pretraining process. In other words, fine-tuning will be of little help if the model has never seen the kind of data you‚Äôre interested in. However, if that‚Äôs the case, SFT can be extremely performant.\nFor example, the LIMA paper showed how you could outperform GPT-3 (DaVinci003) by fine-tuning a LLaMA (v1) model with 65 billion parameters on only 1,000 high-quality samples. The quality of the instruction dataset is essential to reach this level of performance, which is why a lot of work is focused on this issue (like evol-instruct, Orca, or phi-1). Note that the size of the LLM (65b, not 13b or 7b) is also fundamental to leverage pre-existing knowledge efficiently.\nAnother important point related to the data quality is the prompt template. Prompts are comprised of similar elements: system prompt (optional) to guide the model, user prompt (required) to give the instruction, additional inputs (optional) to take into consideration, and the model‚Äôs answer (required). In the case of Llama 2, the authors used the following template for the chat models:\n&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nSystem prompt\n&lt;&lt;/SYS&gt;&gt;\n\nUser prompt [/INST] Model answer &lt;/s&gt;\nThere are other templates, like the ones from Alpaca and Vicuna, and their impact is not very clear. In this example, we will reformat our instruction dataset to follow Llama 2‚Äôs template. For the purpose of this tutorial, I‚Äôve already done it using the excellent timdettmers/openassistant-guanaco dataset. You can find it on Hugging Face under the name mlabonne/guanaco-llama2-1k. In the following, we will use a base model instead of a chat model, so this step is optional. Note that you don‚Äôt need to follow a specific prompt template if you‚Äôre using the base Llama 2 model instead of the chat version."
  },
  {
    "objectID": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#how-to-fine-tune-llama-2",
    "href": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#how-to-fine-tune-llama-2",
    "title": "Fine-Tune Your Own Llama 2 Model in a Colab Notebook",
    "section": "ü¶ô How to fine-tune Llama 2",
    "text": "ü¶ô How to fine-tune Llama 2\nIn this section, we will fine-tune a Llama 2 model with 7 billion parameters on a T4 GPU with high RAM using Google Colab (2.21 credits/hour). Note that a T4 only has 16 GB of VRAM, which is barely enough to store Llama 2-7b‚Äôs weights (7b √ó 2 bytes = 14 GB in FP16). In addition, we need to consider the overhead due to optimizer states, gradients, and forward activations (see this excellent article for more information). This means that a full fine-tuning is not possible here: we need parameter-efficient fine-tuning (PEFT) techniques like LoRA or QLoRA.\nTo drastically reduce the VRAM usage, we must fine-tune the model in 4-bit precision, which is why we‚Äôll use QLoRA here. The good thing is that we can leverage the Hugging Face ecosystem with the transformers, accelerate, peft, trl, and bitsandbytes libraries. This is what we‚Äôll do in the following code, based on Younes Belkada‚Äôs GitHub Gist. First, we install and load these libraries.\n\n!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n\n\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer\n\nLet‚Äôs talk a bit about the parameters we can tune here. First, we want to load a llama-2-7b-chat-hf model and train it on the mlabonne/guanaco-llama2-1k (1,000 samples), which will produce our fine-tuned model llama-2-7b-miniguanaco. If you‚Äôre interested in how this dataset was created, you can check this notebook. Feel free to change it: there are many good datasets on the Hugging Face Hub, like databricks/databricks-dolly-15k.\nQLoRA will use a rank of 64 with a scaling parameter of 16 (see this article for more information about LoRA parameters). We‚Äôll load the Llama 2 model directly in 4-bit precision using the NF4 type and train it for 1 epoch. To get more information about the other parameters, check the TrainingArguments, PeftModel, and SFTTrainer documentation.\n\n# The model that you want to train from the Hugging Face hub\nmodel_name = \"NousResearch/llama-2-7b-chat-hf\"\n\n# The instruction dataset to use\ndataset_name = \"mlabonne/guanaco-llama2-1k\"\n\n# Fine-tuned model name\nnew_model = \"llama-2-7b-miniguanaco\"\n\n################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Number of training epochs\nnum_train_epochs = 1\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule (constant a bit better than cosine)\nlr_scheduler_type = \"constant\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 25\n\n# Log every X updates steps\nlogging_steps = 25\n\n################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}\n\nWe can now load everything and start the fine-tuning process. We‚Äôre relying on multiple wrappers, so bear with me.\n\nFirst of all, we want to load the dataset we defined. Here, our dataset is already preprocessed but, usually, this is where you would reformat the prompt, filter out bad text, combine multiple datasets, etc.\nThen, we‚Äôre configuring bitsandbytes for 4-bit quantization.\nNext, we‚Äôre loading the Llama 2 model in 4-bit precision on a GPU with the corresponding tokenizer.\nFinally, we‚Äôre loading configurations for QLoRA, regular training parameters, and passing everything to the SFTTrainer. The training can finally start!\n\n\n# Load dataset (you can process it here)\ndataset = load_dataset(dataset_name, split=\"train\")\n\n# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major &gt;= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n\n# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# Train model\ntrainer.train()\n\n# Save trained model\ntrainer.model.save_pretrained(new_model)\n\n\n\n\nThe training can be very long, depending on the size of your dataset. Here, it took less than an hour on a T4 GPU. We can check the plots on tensorboard, as follows:\n\n%load_ext tensorboard\n%tensorboard --logdir results/runs\n\n\n\n\n\n\n\nLet‚Äôs make sure that the model is behaving correctly. It would require a more exhaustive evaluation, but we can use the text generation pipeline to ask questions like ‚ÄúWhat is a large language model?‚Äù Note that I‚Äôm formatting the input to match Llama 2‚Äôs prompt template.\n\n# Ignore warnings\nlogging.set_verbosity(logging.CRITICAL)\n\n# Run text generation pipeline with our next model\nprompt = \"What is a large language model?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"&lt;s&gt;[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])\n\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n\n\n&lt;s&gt;[INST] What is a large language model? [/INST] A large language model is a type of artificial intelligence that is trained on a large dataset of text to generate human-like language. It is typically trained on a dataset of text that is much larger than the dataset used for smaller language models. The large dataset allows the model to learn more complex patterns in language, which can result in more accurate and natural-sounding language generation.\n\nLarge language models are often used for tasks such as text summarization, language translation, and chatbots. They are also used for more complex tasks such as writing articles, generating creative content, and even composing music.\n\nLarge language models are trained using a technique called deep learning, which involves using many layers of artificial neural networks to learn complex patterns in the data. The model is trained on a large dataset of text, and the neural networks are trained to predict the next word in a sequence of text given\n\n\nThe model outputs the following response:\nA large language model is a type of artificial intelligence that is trained on a large dataset of text to generate human-like language. It is typically trained on a dataset of text that is much larger than the dataset used for smaller language models. The large dataset allows the model to learn more complex patterns in language, which can result in more accurate and natural-sounding language generation.\n\nLarge language models are often used for tasks such as text summarization, language translation, and chatbots. They are also used for more complex tasks such as writing articles, generating creative content, and even composing music.\n\nLarge language models are trained using a technique called deep learning, which involves using many layers of artificial neural networks to learn complex patterns in the data. The model is trained on a large dataset of text, and the neural networks are trained to predict the next word in a sequence of text given\nFrom experience, it is very coherent for a model with only 7 billion parameters. You can play with it and ask harder questions from evaluation datasets like BigBench-Hard. Guanaco is an excellent dataset that has produced high-quality models in the past. You can train a Llama 2 model on the entire dataset using mlabonne/guanaco-llama2.\nHow can we store our new llama-2-7b-miniguanaco model now? We need to merge the weights from LoRA with the base model. Unfortunately, as far as I know, there is no straightforward way to do it: we need to reload the base model in FP16 precision and use the peft library to merge everything. Alas, it also creates a problem with the VRAM (despite emptying it), so I recommend restarting the notebook, re-executing the three first cells, and then executing the next one. Please contact me if you know a fix!\n\n# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\n\n# Reload tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n\n\n\nOur weights are merged and we reloaded the tokenizer. We can now push everything to the Hugging Face Hub to save our model.\n\n!huggingface-cli login\n\nmodel.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)\n\n\n\n\n\n\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/mlabonne/llama-2-7b-guanaco/commit/0f5ed9581b805b659aec68484820edb5e3e6c3f5', commit_message='Upload tokenizer', commit_description='', oid='0f5ed9581b805b659aec68484820edb5e3e6c3f5', pr_url=None, pr_revision=None, pr_num=None)\n\n\nYou can now use this model for inference by loading it like any other Llama 2 model from the Hub. It is also possible to reload it for more fine-tuning ‚Äì perhaps with another dataset?\nIf you‚Äôre serious about fine-tuning models, using a script instead of a notebook is recommended. You can easily rent GPUs on Lambda Labs, Runpod, Vast.ai, for less than 0.3$/h. Once you‚Äôre connected, you can install libraries, import your script, log in to Hugging Face and other tools (like Weights & Biases for logging your experiments), and start your fine-tuning.\nThe trl script is currently very limited, so I made my own based on the previous notebook. You can find it here on GitHub Gist. If you‚Äôre looking for a comprehensive solution, check out Axolotl from the OpenAccess AI Collective, which natively handles multiple datasets, Deepspeed, Flash Attention, etc."
  },
  {
    "objectID": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#conclusion",
    "href": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#conclusion",
    "title": "Fine-Tune Your Own Llama 2 Model in a Colab Notebook",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we saw how to fine-tune a Llama 2 7b model using a Colab notebook. We introduced some necessary background on LLM training and fine-tuning, as well as important considerations related to instruction datasets. In the second section, we successfully fine-tuned the Llama 2 model with its native prompt template and custom parameters.\nThese fine-tuned models can then be integrated into LangChain and other architectures as advantageous alternatives to the OpenAI API. Remember, in this new paradigm, instruction datasets are the new gold, and the quality of your model heavily depends on the data on which it‚Äôs been fine-tuned. So, good luck with building high-quality datasets!\nIf you‚Äôre interested in more content about LLMs, follow me on Twitter @maximelabonne."
  },
  {
    "objectID": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#references",
    "href": "posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html#references",
    "title": "Fine-Tune Your Own Llama 2 Model in a Colab Notebook",
    "section": "References",
    "text": "References\n\nHugo Touvron, Thomas Scialom, et al.¬†(2023). Llama 2: Open Foundation and Fine-Tuned Chat Models.\nPhilipp Schmid, Omar Sanseviero, Pedro Cuenca, & Lewis Tunstall. Llama 2 is here - get it on Hugging Face. https://huggingface.co/blog/llama2\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, & Tatsunori B. Hashimoto. (2023). Stanford Alpaca: An Instruction-following LLaMA model.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, & Kristina Toutanova. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, & Luke Zettlemoyer. (2023). QLoRA: Efficient Finetuning of Quantized LLMs."
  },
  {
    "objectID": "posts/Introduction_to_Weight_Quantization.html",
    "href": "posts/Introduction_to_Weight_Quantization.html",
    "title": "Introduction to Weight Quantization",
    "section": "",
    "text": "Large Language Models (LLMs) are known for their extensive computational requirements. Typically, the size of a model is calculated by multiplying the number of parameters (size) by the precision of these values (data type). However, to save memory, weights can be stored using lower-precision data types through a process known as quantization.\nWe distinguish two main families of weight quantization techniques in the literature:\nIn this article, we focus on PTQ to reduce the precision of our parameters. To get a good intuition, we will apply both na√Øve and more sophisticated techniques to a toy example using a GPT-2 model.\nThe entire code is freely available on Google Colab and GitHub."
  },
  {
    "objectID": "posts/Introduction_to_Weight_Quantization.html#background-on-floating-point-representation",
    "href": "posts/Introduction_to_Weight_Quantization.html#background-on-floating-point-representation",
    "title": "Introduction to Weight Quantization",
    "section": "üìö Background on Floating Point Representation",
    "text": "üìö Background on Floating Point Representation\nThe choice of data type dictates the quantity of computational resources required, affecting the speed and efficiency of the model. In deep learning applications, balancing precision and computational performance becomes a vital exercise as higher precision often implies greater computational demands.\nAmong various data types, floating point numbers are predominantly employed in deep learning due to their ability to represent a wide range of values with high precision. Typically, a floating point number uses \\(n\\) bits to store a numerical value. These \\(n\\) bits are further partitioned into three distinct components:\n\nSign: The sign bit indicates the positive or negative nature of the number. It uses one bit where 0 indicates a positive number and 1 signals a negative number.\nExponent: The exponent is a segment of bits that represents the power to which the base (usually 2 in binary representation) is raised. The exponent can also be positive or negative, allowing the number to represent very large or very small values.\nSignificand/Mantissa: The remaining bits are used to store the significand, also referred to as the mantissa. This represents the significant digits of the number. The precision of the number heavily depends on the length of the significand.\n\nThis design allows floating point numbers to cover a wide range of values with varying levels of precision. The formula used for this representation is:\n\\[(-1)^{\\text{sign}} \\times \\text{base}^{\\text{exponent}} \\times \\text{significand}\\]\nTo understand this better, let‚Äôs delve into some of the most commonly used data types in deep learning: float32 (FP32), float16 (FP16), and bfloat16 (BF16):\n\nFP32 uses 32 bits to represent a number: one bit for the sign, eight for the exponent, and the remaining 23 for the significand. While it provides a high degree of precision, the downside of FP32 is its high computational and memory footprint.\nFP16 uses 16 bits to store a number: one is used for the sign, five for the exponent, and ten for the significand. Although this makes it more memory-efficient and accelerates computations, the reduced range and precision can introduce numerical instability, potentially impacting model accuracy.\nBF16 is also a 16-bit format but with one bit for the sign, eight for the exponent, and seven for the significand. BF16 expands the representable range compared to FP16, thus decreasing underflow and overflow risks. Despite a reduction in precision due to fewer significand bits, BF16 typically does not significantly impact model performance and is a useful compromise for deep learning tasks.\n\n\n\n\n\n\nIn ML jargon, FP32 is often termed ‚Äúfull precision‚Äù (4 bytes), while BF16 and FP16 are ‚Äúhalf-precision‚Äù (2 bytes). But could we do even better and store weights using a single byte? The answer is the INT8 data type, which consists of an 8-bit representation capable of storing \\(2^8 = 256\\) different values. In the next section, we‚Äôll see how to convert FP32 weights into an INT8 format."
  },
  {
    "objectID": "posts/Introduction_to_Weight_Quantization.html#na√Øve-8-bit-quantization",
    "href": "posts/Introduction_to_Weight_Quantization.html#na√Øve-8-bit-quantization",
    "title": "Introduction to Weight Quantization",
    "section": "üî∞ Na√Øve 8-bit Quantization",
    "text": "üî∞ Na√Øve 8-bit Quantization\nIn this section, we will implement two quantization techniques: a symmetric one with absolute maximum (absmax) quantization and an asymmetric one with zero-point quantization. In both cases, the goal is to map an FP32 tensor \\(\\mathbf{X}\\) (original weights) to an INT8 tensor \\(\\mathbf{X}_{\\text{quant}}\\) (quantized weights).\nWith absmax quantization, the original number is divided by the absolute maximum value of the tensor and multiplied by a scaling factor (127) to map inputs into the range [-127, 127]. To retrieve the original FP16 values, the INT8 number is divided by the quantization factor, acknowledging some loss of precision due to rounding.\n\\[\\begin{align*}\n\\mathbf{X}_{\\text{quant}} &= \\text{round}\\Biggl ( \\frac{127}{\\max|\\mathbf{X}|} \\cdot \\mathbf{X} \\Biggr ) \\\\\n\\mathbf{X}_{\\text{dequant}} &= \\frac{\\max|\\mathbf{X}|}{127} \\cdot \\mathbf{X}_{\\text{quant}}\n\\end{align*}\\]\nFor instance, let‚Äôs say we have an absolution maximum value of 3.2. A weight of 0.1 would be quantized to \\(\\text{round}(0.1 \\times \\frac{127}{3.2}) = 4\\). If we want to dequantize it, we would get \\(4 / \\frac{127}{3.2} = 0.1008\\), which implies an error of 0.008. Here‚Äôs the corresponding Python implementation:\n\nimport torch\n\ndef absmax_quantize(X):\n    # Calculate scale\n    scale = 127 / torch.max(torch.abs(X))\n\n    # Quantize\n    X_quant = (scale * X).round()\n\n    # Dequantize\n    X_dequant = X_quant / scale\n\n    return X_quant.to(torch.int8), X_dequant\n\nWith zero-point quantization, we can consider asymmetric input distributions, which is useful when you consider the output of a ReLU function (only positive values) for example. The input values are first scaled by the total range of values (255) divided by the difference between the maximum and minimum values. This distribution is then shifted by the zero-point to map it into the range [-128, 127] (notice the extra value compared to absmax). First, we calculate the scale factor and the zero-point value:\n\\[\\begin{align*}\n\\text{scale} &= \\frac{255}{\\max(\\mathbf{X}) - \\min(\\mathbf{X})} \\\\\n\\text{zeropoint} &= - \\text{round}(\\text{scale} \\cdot \\min(\\mathbf{X})) - 128\n\\end{align*}\\]\nThen, we can use these variables to quantize or dequantize our weights:\n\\[\\begin{align*}\n\\mathbf{X}_{\\text{quant}} &= \\text{round}\\bigg(\\text{scale} \\cdot \\mathbf{X} + \\text{zeropoint} \\bigg) \\\\\n\\mathbf{X}_{\\text{dequant}} &= \\frac{\\mathbf{X}_{\\text{quant}} - \\text{zeropoint}}{\\text{scale}}\n\\end{align*}\\]\nLet‚Äôs take an example: we have a maximum value of 3.2 and a minimum value of -3.0. We can calculate the scale is \\(\\frac{255}{3.2 + 3.0} = 41.13\\) and the zero-point \\(-\\text{round}(41.13 \\cdot -3.0) - 128 = 123 - 128 = -5\\), so our previous weight of 0.1 would be quantized to \\(\\text{round}(41.13 \\cdot 0.1 - 5) = -1\\). This is very different from the previous value obtained using absmax (4 vs.¬†-1).\n\n\n\n\n\nThe Python implementation is quite straightforward:\n\ndef zeropoint_quantize(X):\n    # Calculate value range (denominator)\n    x_range = torch.max(X) - torch.min(X)\n    x_range = 1 if x_range == 0 else x_range\n\n    # Calculate scale\n    scale = 255 / x_range\n\n    # Shift by zero-point\n    zeropoint = (-scale * torch.min(X) - 128).round()\n\n    # Scale and round the inputs\n    X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)\n\n    # Dequantize\n    X_dequant = (X_quant - zeropoint) / scale\n\n    return X_quant.to(torch.int8), X_dequant\n\nInstead of relying on complete toy examples, we can use these two functions on a real model thanks to the transformers library.\nWe start by loading the model and tokenizer for GPT-2. This is a very small model we probably don‚Äôt want to quantize, but it will be good enough for this tutorial. First, we want to observe the model‚Äôs size so we can compare it later and evaluate the memory savings due to 8-bit quantization.\n\n!pip install -q bitsandbytes&gt;=0.39.0\n!pip install -q git+https://github.com/huggingface/accelerate.git\n!pip install -q git+https://github.com/huggingface/transformers.git\n\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\ntorch.manual_seed(0)\n\n# Set device to CPU for now\ndevice = 'cpu'\n\n# Load model and tokenizer\nmodel_id = 'gpt2'\nmodel = AutoModelForCausalLM.from_pretrained(model_id).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# Print model size\nprint(f\"Model size: {model.get_memory_footprint():,} bytes\")\n\n\n\n\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\nCUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\nModel size: 510,342,192 bytes\n\n\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n  warn(msg)\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n  warn(msg)\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//172.28.0.1'), PosixPath('8013')}\n  warn(msg)\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-20b5bv2xvtu9a --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n  warn(msg)\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n  warn(msg)\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n  warn(msg)\n/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\nEither way, this might cause trouble in the future:\nIf you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n  warn(msg)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe size of the GPT-2 model is approximately 487MB in FP32. The next step consists of quantizing the weights using zero-point and absmax quantization. In the following example, we apply these techniques to the first attention layer of GPT-2 to see the results.\n\n# Extract weights of the first layer\nweights = model.transformer.h[0].attn.c_attn.weight.data\nprint(\"Original weights:\")\nprint(weights)\n\n# Quantize layer using absmax quantization\nweights_abs_quant, _ = absmax_quantize(weights)\nprint(\"\\nAbsmax quantized weights:\")\nprint(weights_abs_quant)\n\n# Quantize layer using absmax quantization\nweights_zp_quant, _ = zeropoint_quantize(weights)\nprint(\"\\nZero-point quantized weights:\")\nprint(weights_zp_quant)\n\nOriginal weights:\ntensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],\n        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],\n        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],\n        ...,\n        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],\n        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],\n        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]])\n\nAbsmax quantized weights:\ntensor([[-21, -12,  -4,  ...,   2,  -3,   1],\n        [  4,   7,  11,  ...,  -2,  -1,  -1],\n        [  0,   3,  16,  ...,   5,   2,  -1],\n        ...,\n        [-12,  -1,   9,  ...,   0,  -2,   1],\n        [  7,  10,   5,  ...,   1,  -2,  -2],\n        [-18,  -9, -11,  ...,   0,   0,   1]], dtype=torch.int8)\n\nZero-point quantized weights:\ntensor([[-20, -11,  -3,  ...,   3,  -2,   2],\n        [  5,   8,  12,  ...,  -1,   0,   0],\n        [  1,   4,  18,  ...,   6,   3,   0],\n        ...,\n        [-11,   0,  10,  ...,   1,  -1,   2],\n        [  8,  11,   6,  ...,   2,  -1,  -1],\n        [-18,  -8, -10,  ...,   1,   1,   2]], dtype=torch.int8)\n\n\nThe difference between the original (FP32) and quantized values (INT8) is clear, but the difference between absmax and zero-point weights is more subtle. In this case, the inputs look shifted by a value of -1. This suggests that the weight distribution in this layer is quite symmetric.\nWe can compare these techniques by quantizing every layer in GPT-2 (linear layers, attention layers, etc.) and create two new models: model_abs and model_zp. To be precise, we will actually replace the original weights with de-quantized ones. This has two benefits: it allows us to 1/ compare the distribution of our weights (same scale) and 2/ actually run the models.\nIndeed, PyTorch doesn‚Äôt allow INT8 matrix multiplication by default. In a real scenario, we would dequantize them to run the model (in FP16 for example) but store them as INT8. In the next section, we will use the bitsandbytes library to solve this issue.\n\nimport numpy as np\nfrom copy import deepcopy\n\n# Store original weights\nweights = [param.data.clone() for param in model.parameters()]\n\n# Create model to quantize\nmodel_abs = deepcopy(model)\n\n# Quantize all model weights\nweights_abs = []\nfor param in model_abs.parameters():\n    _, dequantized = absmax_quantize(param.data)\n    param.data = dequantized\n    weights_abs.append(dequantized)\n\n# Create model to quantize\nmodel_zp = deepcopy(model)\n\n# Quantize all model weights\nweights_zp = []\nfor param in model_zp.parameters():\n    _, dequantized = zeropoint_quantize(param.data)\n    param.data = dequantized\n    weights_zp.append(dequantized)\n\nNow that our models have been quantized, we want to check the impact of this process. Intuitively, we want to make sure that the quantized weights are close to the original ones. A visual way to check it is to plot the distribution of the dequantized and original weights. If the quantization is lossy, it would drastically change the weight distribution.\nThe following figure shows this comparison, where the blue histogram represents the original (FP32) weights, and the red one represents the dequantized (from INT8) weights. Note that we only display this plot between -2 and 2 because of outliers with very high absolute values (more on that later).\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\n# Flatten weight tensors\nweights = np.concatenate([t.cpu().numpy().flatten() for t in weights])\nweights_abs = np.concatenate([t.cpu().numpy().flatten() for t in weights_abs])\nweights_zp = np.concatenate([t.cpu().numpy().flatten() for t in weights_zp])\n\n# Set background style\nplt.style.use('ggplot')\n\n# Create figure and axes\nfig, axs = plt.subplots(2, figsize=(10,10), dpi=300, sharex=True)\n\n# Plot the histograms for original and zero-point weights\naxs[0].hist(weights, bins=150, alpha=0.5, label='Original weights', color='blue', range=(-2, 2))\naxs[0].hist(weights_abs, bins=150, alpha=0.5, label='Absmax weights', color='red', range=(-2, 2))\n\n# Plot the histograms for original and absmax weights\naxs[1].hist(weights, bins=150, alpha=0.5, label='Original weights', color='blue', range=(-2, 2))\naxs[1].hist(weights_zp, bins=150, alpha=0.5, label='Zero-point weights', color='green', range=(-2, 2))\n\n# Add grid\nfor ax in axs:\n    ax.grid(True, linestyle='--', alpha=0.6)\n\n# Add legend\naxs[0].legend()\naxs[1].legend()\n\n# Add title and labels\naxs[0].set_title('Comparison of Original and Absmax Quantized Weights', fontsize=16)\naxs[1].set_title('Comparison of Original and Zeropoint Quantized Weights', fontsize=16)\n\nfor ax in axs:\n    ax.set_xlabel('Weights', fontsize=14)\n    ax.set_ylabel('Count', fontsize=14)\n    ax.yaxis.set_major_formatter(ticker.EngFormatter()) # Make y-ticks more human readable\n\n# Improve font\nplt.rc('font', size=12)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nBoth plots are quite similar, with a surprising spike around 0. This spike shows that our quantization is quite lossy since reversing the process doesn‚Äôt output the original values. This is particularly true for the absmax model, which displays both a lower valley and a higher spike around 0.\nLet‚Äôs compare the performance of the original and quantized models. For this purpose, we define a generate_text() function to generate 50 tokens with top-k sampling.\n\ndef generate_text(model, input_text, max_length=50):\n    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n    output = model.generate(inputs=input_ids,\n                            max_length=max_length,\n                            do_sample=True,\n                            top_k=30,\n                            pad_token_id=tokenizer.eos_token_id,\n                            attention_mask=input_ids.new_ones(input_ids.shape))\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\n# Generate text with original and quantized models\noriginal_text = generate_text(model, \"I have a dream\")\nabsmax_text   = generate_text(model_abs, \"I have a dream\")\nzp_text       = generate_text(model_zp, \"I have a dream\")\n\nprint(f\"Original model:\\n{original_text}\")\nprint(\"-\" * 50)\nprint(f\"Absmax model:\\n{absmax_text}\")\nprint(\"-\" * 50)\nprint(f\"Zeropoint model:\\n{zp_text}\")\n\nOriginal model:\nI have a dream, and it is a dream I believe I would get to live in my future. I love my mother, and there was that one time I had been told that my family wasn't even that strong. And then I got the\n--------------------------------------------------\nAbsmax model:\nI have a dream to find out the origin of her hair. She loves it. But there's no way you could be honest about how her hair is made. She must be crazy.\n\nWe found a photo of the hairstyle posted on\n--------------------------------------------------\nZeropoint model:\nI have a dream of creating two full-time jobs in America‚Äîone for people with mental health issues, and one for people who do not suffer from mental illness‚Äîor at least have an employment and family history of substance abuse, to work part\n\n\nInstead of trying to see if one output makes more sense than the others, we can quantify it by calculating the perplexity of each output. This is a common metric used to evaluate language models, which measures the uncertainty of a model in predicting the next token in a sequence. In this comparison, we make the common assumption that the lower the score, the better the model is. In practice, a sentence with a high perplexity could also be correct.\nWe implement it using a minimal function since it doesn‚Äôt need to consider details like the length of the context window since our sentences are short.\n\ndef calculate_perplexity(model, text):\n    # Encode the text\n    encodings = tokenizer(text, return_tensors='pt').to(device)\n\n    # Define input_ids and target_ids\n    input_ids = encodings.input_ids\n    target_ids = input_ids.clone()\n\n    with torch.no_grad():\n        outputs = model(input_ids, labels=target_ids)\n\n    # Loss calculation\n    neg_log_likelihood = outputs.loss\n\n    # Perplexity calculation\n    ppl = torch.exp(neg_log_likelihood)\n\n    return ppl\n\nppl     = calculate_perplexity(model, original_text)\nppl_abs = calculate_perplexity(model_abs, absmax_text)\nppl_zp  = calculate_perplexity(model_zp, absmax_text)\n\nprint(f\"Original perplexity:  {ppl.item():.2f}\")\nprint(f\"Absmax perplexity:    {ppl_abs.item():.2f}\")\nprint(f\"Zeropoint perplexity: {ppl_zp.item():.2f}\")\n\nOriginal perplexity: 15.53\nAbsmax perplexity:   17.92\nZeropoint perplexity: 17.97\n\n\nWe see that the perplexity of the original model is slightly lower than the two others. A single experiment is not very reliable, but we could repeat this process multiple times to see the difference between each model. In theory, zero-point quantization should be slightly better than absmax, but is also more costly to compute.\nIn this example, we applied quantization techniques to entire layers (per-tensor basis). However, we could apply it at different granularity levels: from the entire model to individual values. Quantizing the entire model in one pass would seriously degrade the performance, while quantizing individual values would create a big overhead. In practice, we often prefer the vector-wise quantization, which considers the variability of values in rows and columns inside of the same tensor.\nHowever, even vector-wise quantization doesn‚Äôt solve the problem of outlier features. Outlier features are extreme values (negative or positive) that appear in all transformer layers when the model reach a certain scale (&gt;6.7B parameters). This is an issue since a single outlier can reduce the precision for all other values. But discarding these outlier features is not an option since it would greatly degrade the model‚Äôs performance."
  },
  {
    "objectID": "posts/Introduction_to_Weight_Quantization.html#bit-quantization-with-llm.int8",
    "href": "posts/Introduction_to_Weight_Quantization.html#bit-quantization-with-llm.int8",
    "title": "Introduction to Weight Quantization",
    "section": "üî¢ 8-bit Quantization with LLM.int8()",
    "text": "üî¢ 8-bit Quantization with LLM.int8()\nIntroduced by Dettmers et al.¬†(2022), LLM.int8() is a solution to the outlier problem. It relies on a vector-wise (absmax) quantization scheme and introduces mixed-precision quantization. This means that outlier features are processed in a FP16 format to retain their precision, while the other values are processed in an INT8 format. As outliers represent about 0.1% of values, this effectively reduces the memory footprint of the LLM by almost 2x.\n\n\n\nLLM.int8() works by conducting matrix multiplication computation in three key steps:\n\nExtract columns from the input hidden states \\(\\mathbf{X}\\) containing outlier features using a custom threshold.\nPerform the matrix multiplication of the outliers using FP16 and the non-outliers using INT8 with vector-wise quantization (row-wise for the hidden state \\(\\mathbf{X}\\) and column-wise for the weight matrix \\(\\mathbf{W}\\)).\nDequantize the non-outlier results (INT8 to FP16) and add them to the outlier results to get the full result in FP16.\n\n\n\n\nThis approach is necessary because 8-bit precision is limited and can lead to substantial errors when quantizing a vector with large values. These errors also tend to amplify as they propagate through multiple layers.\nWe can easily use this technique thanks to the integration of the bitsandbytes library into the Hugging Face ecosystem. We just need to specify load_in_8bit=True when loading the model (it also requires a GPU).\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel_int8 = AutoModelForCausalLM.from_pretrained(model_id,\n                                             device_map='auto',\n                                             load_in_8bit=True,\n                                             )\nprint(f\"Model size: {model_int8.get_memory_footprint():,} bytes\")\n\nSome weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['lm_head.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nModel size: 176,527,896 bytes\n\n\nWith this extra line of code, the model is now almost three times smaller (168MB vs.¬†487MB). We can even compare the distribution of the original and quantized weights as we did earlier:\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\n# Flatten weight tensors\nweights_int8 = [param.data.clone() for param in model_int8.parameters()]\nweights_int8 = np.concatenate([t.cpu().numpy().flatten() for t in weights_int8])\n\n# Set background style\nplt.style.use('ggplot')\n\n# Create figure and axis\nfig, ax = plt.subplots(figsize=(10,5), dpi=300)\n\n# Plot the histograms\nax.hist(weights, bins=150, alpha=0.5, label='Original weights',\n        color='blue', range=(-2, 2))\nax.hist(weights_int8, bins=150, alpha=0.5, label='LLM.int8() weights',\n        color='red', range=(-2, 2))\n\n# Add grid\nax.grid(True, linestyle='--', alpha=0.6)\n\n# Add legend\nax.legend()\n\n# Add title and labels\nax.set_title('Comparison of Original and Dequantized Weights', fontsize=16)\nax.set_xlabel('Weights', fontsize=14)\nax.set_ylabel('Count', fontsize=14)\nplt.gca().yaxis.set_major_formatter(ticker.EngFormatter())\n\n# Improve font\nplt.rc('font', size=12)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nIn this case, we see spikes around -2, -1, 0, 1, 2, etc. These values correspond to the parameters stored in the INT8 format (non-outliers). You can verify it by printing the model‚Äôs weights using model_int8.parameters().\nWe can also generate text with this quantized model and compare it to the original model.\n\n# Generate text with quantized model\ntext_int8 = generate_text(model_int8, \"I have a dream\")\n\nprint(f\"Original model:\\n{original_text}\")\nprint(\"-\" * 50)\nprint(f\"LLM.int8() model:\\n{text_int8}\")\n\nOriginal model:\nI have a dream, and it is a dream I believe I would get to live in my future. I love my mother, and there was that one time I had been told that my family wasn't even that strong. And then I got the\n--------------------------------------------------\nLLM.int8() model:\nI have a dream. I don't know what will come of it, but I am going to have to look for something that will be right. I haven't thought about it for a long time, but I have to try to get that thing\n\n\nOnce again, it is difficult to judge what is the best output, but we can rely on the perplexity metric to give us an (approximate) answer.\n\nprint(f\"Perplexity (original):   {ppl.item():.2f}\")\n\nppl = calculate_perplexity(model_int8, text_int8)\nprint(f\"Perplexity (LLM.int8()): {ppl.item():.2f}\")\n\nPerplexity (original):   15.53\nPerplexity (LLM.int8()): 7.93\n\n\nIn this case, the perplexity of the quantized model is twice as low as the original one. In general, this is not the case, but it shows that this quantization technique is very competitive. In fact, the authors of LLM.int8() show that the performance degradation is so low it‚Äôs negligible (&lt;1%). However, it has an additional cost in terms of computation: LLM.int8() is roughly about 20% slower for large models."
  },
  {
    "objectID": "posts/Introduction_to_Weight_Quantization.html#conclusion",
    "href": "posts/Introduction_to_Weight_Quantization.html#conclusion",
    "title": "Introduction to Weight Quantization",
    "section": "Conclusion",
    "text": "Conclusion\nThis article provided an overview of the most popular weight quantization techniques. We started by gaining an understanding of floating point representation, before introducing two techniques for 8-bit quantization: absmax and zero-point quantization. However, their limitations, particularly when it comes to handling outliers, led to LLM.int8(), a technique that also preserves the model‚Äôs performance. This approach underlines the progress being made in the field of weight quantization, revealing the importance of properly addressing outliers.\nLooking forward, our next article will explore the GPTQ weight quantization technique in depth. This technique, introduced by Frantar et al., only utilizes 4 bits and represents a significant advancement in the field of weight quantization. We will provide a comprehensive guide on how to implement GPTQ using the AutoGPTQ library.\nIf you‚Äôre interested in more technical content around LLMs, follow me on Twitter @maximelabonne."
  },
  {
    "objectID": "posts/Introduction_to_Weight_Quantization.html#references",
    "href": "posts/Introduction_to_Weight_Quantization.html#references",
    "title": "Introduction to Weight Quantization",
    "section": "References",
    "text": "References\n\nT. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale. 2022.\nY. Beldaka, and T. Dettmers, A Gentle Introduction to 8-bit Matrix Multiplication, Hugging Face Blog (2022).\nA. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, and K. Keutzer, A Survey of Quantization Methods for Efficient Neural Network Inference. 2021.\nH. Wu, P. Judd, X. Zhang, M. Isaev, and P. Micikevicius, Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation. 2020.\nLilian Weng, Large Transformer Model Inference Optimization, Lil‚ÄôLog (2023).\nKamil Czarnogorski, Local Large Language Models, Int8 (2023)."
  },
  {
    "objectID": "posts/Quantize_Llama_2_models_using_ggml.html",
    "href": "posts/Quantize_Llama_2_models_using_ggml.html",
    "title": "Quantize Llama models with GGML and llama.cpp",
    "section": "",
    "text": "Due to the massive size of Large Language Models (LLMs), quantization has become an essential technique to run them efficiently. By reducing the precision of their weights, you can save memory and speed up inference while preserving most of the model‚Äôs performance. Recently, 8-bit and 4-bit quantization unlocked the possibility of running LLMs on consumer hardware. Coupled with the release of Llama models and parameter-efficient techniques to fine-tune them (LoRA, QLoRA), this created a rich ecosystem of local LLMs that are now competing with OpenAI‚Äôs GPT-3.5 and GPT-4.\nCurrently, there are three main quantization techniques: NF4, GPTQ, and GGML. NF4 is a static method used by QLoRA to load a model in 4-bit precision to perform fine-tuning. In a previous article, we explored the GPTQ method and quantized our own model to run it on a consumer GPU. In this article, we will introduce the GGML technique, see how to quantize Llama models, and provide tips and tricks to achieve the best results.\nYou can find the code on Google Colab and GitHub."
  },
  {
    "objectID": "posts/Quantize_Llama_2_models_using_ggml.html#what-is-ggml",
    "href": "posts/Quantize_Llama_2_models_using_ggml.html#what-is-ggml",
    "title": "Quantize Llama models with GGML and llama.cpp",
    "section": "What is GGML?",
    "text": "What is GGML?\nGGML is a C library focused on machine learning. It was created by Georgi Gerganov, which is what the initials ‚ÄúGG‚Äù stand for. This library not only provides foundational elements for machine learning, such as tensors, but also a unique binary format to distribute LLMs.\nThis format recently changed to GGUF. This new format is designed to be extensible, so that new features shouldn‚Äôt break compatibility with existing models. It also centralizes all the metadata in one file, such as special tokens, RoPE scaling parameters, etc. In short, it answers a few historical pain points and should be future-proof. For more information, you can read the specification at this address. In the rest of the article, we will call ‚ÄúGGML models‚Äù all models that either use GGUF or previous formats.\nGGML was designed to be used in conjunction with the llama.cpp library, also created by Georgi Gerganov. The library is written in C/C++ for efficient inference of Llama models. It can load GGML models and run them on a CPU. Originally, this was the main difference with GPTQ models, which are loaded and run on a GPU. However, you can now offload some layers of your LLM to the GPU with llama.cpp. To give you an example, there are 35 layers for a 7b parameter model. This drastically speeds up inference and allows you to run LLMs that don‚Äôt fit in your VRAM.\n\n\n\nIf command-line tools are your thing, llama.cpp and GGUF support have been integrated into many GUIs, like oobabooga‚Äôs text-generation-web-ui, koboldcpp, LM Studio, or ctransformers. You can simply load your GGML models with these tools and interact with them in a ChatGPT-like way. Fortunately, many quantized models are directly available on the Hugging Face Hub. You‚Äôll quickly notice that most of them are quantized by TheBloke, a popular figure in the LLM community.\nIn the next section, we will see how to quantize our own models and run them on a consumer GPU."
  },
  {
    "objectID": "posts/Quantize_Llama_2_models_using_ggml.html#how-to-quantize-llms-with-ggml",
    "href": "posts/Quantize_Llama_2_models_using_ggml.html#how-to-quantize-llms-with-ggml",
    "title": "Quantize Llama models with GGML and llama.cpp",
    "section": "How to quantize LLMs with GGML?",
    "text": "How to quantize LLMs with GGML?\nLet‚Äôs look at the files inside of TheBloke/Llama-2-13B-chat-GGML repo. We can see 14 different GGML models, corresponding to different types of quantization. They follow a particular naming convention: ‚Äúq‚Äù + the number of bits used to store the weights (precision) + a particular variant. Here is a list of all the possible quant methods and their corresponding use cases, based on model cards made by TheBloke:\n\nq2_k: Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\nq3_k_l: Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\nq3_k_m: Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\nq3_k_s: Uses Q3_K for all tensors\nq4_0: Original quant method, 4-bit.\nq4_1: Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\nq4_k_m: Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\nq4_k_s: Uses Q4_K for all tensors\nq5_0: Higher accuracy, higher resource usage and slower inference.\nq5_1: Even higher accuracy, resource usage and slower inference.\nq5_k_m: Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\nq5_k_s: Uses Q5_K for all tensors\nq6_k: Uses Q8_K for all tensors\nq8_0: Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.\n\nAs a rule of thumb, I recommend using Q5_K_M as it preserves most of the model‚Äôs performance. Alternatively, you can use Q4_K_M if you want to save some memory. In general, K_M versions are better than K_S versions. I cannot recommend Q2_K or Q3_* versions, as they drastically decrease model performance.\nNow that we know more about the quantization types available, let‚Äôs see how to use them on a real model. You can execute the following code on a free T4 GPU on Google Colab. The first step consists of compiling llama.cpp and installing the required libraries in our Python environment.\n\n# Install llama.cpp\n!git clone https://github.com/ggerganov/llama.cpp\n!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n!pip install -r llama.cpp/requirements.txt\n\nNow we can download our model. We will use the model we fine-tuned in this article, mlabonne/EvolCodeLlama-7b.\n\nMODEL_ID = \"mlabonne/EvolCodeLlama-7b\"\n\n# Download model\n!git lfs install\n!git clone https://huggingface.co/{MODEL_ID}\n\nThis step can take a while. Once it‚Äôs done, we need to convert our weight to GGML FP16 format.\n\nMODEL_NAME = MODEL_ID.split('/')[-1]\nGGML_VERSION = \"gguf\"\n\n# Convert to fp16\nfp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{GGML_VERSION}.fp16.bin\"\n!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n\nFinally, we can quantize the model using one or several methods. In this case, we will use the Q4_K_M and Q5_K_M methods I recommended earlier. This is the only step that actually requires a GPU.\n\nQUANTIZATION_METHODS = [\"q4_k_m\", \"q5_k_m\"]\n\nfor method in QUANTIZATION_METHODS:\n    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{GGML_VERSION}.{method}.bin\"\n    !./llama.cpp/quantize {fp16} {qtype} {method}\n\nOur two quantized models are now ready for inference. We can check the size of the bin files to see how much we compressed them. The FP16 model takes up 13.5 GB, while the Q4_K_M model takes up 4.08 GB (3.3 times smaller) and the Q5_K_M model takes up 4.78 GB (2.8 times smaller).\nLet‚Äôs use llama.cpp to efficiently run them. Since we‚Äôre using a GPU with 16 GB of VRAM, we can offload every layer to the GPU. In this case, it represents 35 layers (7b parameter model), so we‚Äôll use the -ngl 35 parameter. In the following code block, we‚Äôll also input a prompt and the quantization method we want to use.\n\nimport os\n\nmodel_list = [file for file in os.listdir(MODEL_NAME) if GGML_VERSION in file]\n\nprompt = input(\"Enter your prompt: \")\nchosen_method = input(\"Please specify the quantization method to run the model (options: \" + \", \".join(model_list) + \"): \")\n\n# Verify the chosen method is in the list\nif chosen_method not in model_list:\n    print(\"Invalid method chosen!\")\nelse:\n    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{GGML_VERSION}.{method}.bin\"\n    !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\"\n\nLet‚Äôs ask the model ‚ÄúWrite a Python function to print the nth Fibonacci numbers‚Äù using the Q5_K_M method. If we look at the logs, we can confirm that we successfully offloaded our layers thanks to the line ‚Äúllm_load_tensors: offloaded 35/35 layers to GPU‚Äù. Here is the code the model generated:\ndef fib(n):\n    if n == 0 or n == 1:\n        return n\n    return fib(n - 2) + fib(n - 1)\n\nfor i in range(1, 10):\n    print(fib(i))\nThis wasn‚Äôt a very complex prompt, but it successfully produced a working piece of code in no time. With this GGML, you can use your local LLM as an assistant in a terminal using the interactive mode (-i flag). Note that this also works on Macbooks with Apple‚Äôs Metal Performance Shaders (MPS), which is an excellent option to run LLMs.\nFinally, we can push our quantized model to a new repo on the Hugging Face Hub with the ‚Äú-GGUF‚Äù suffix. First, let‚Äôs log in and modify the following code block to match your username.\n\n!pip install -q huggingface_hub\n\nusername = \"mlabonne\"\n\nfrom huggingface_hub import notebook_login, create_repo, HfApi\nnotebook_login()\n\nNow we can create the repo and upload our models. We use the allow_patterns parameter to filter which files to upload, so we don‚Äôt push the entirety of the directory.\n\napi = HfApi()\n\n# Create repo\ncreate_repo(\n    repo_id=f\"{username}/{MODEL_NAME}-GGML\",\n    repo_type=\"model\",\n    exist_ok=True\n)\n\n# Upload bin models\napi.upload_folder(\n    folder_path=MODEL_NAME,\n    repo_id=f\"{username}/{MODEL_NAME}-GGML\",\n    allow_patterns=f\"*{GGML_VERSION}*\",\n)\n\nWe have successfully quantized, run, and pushed GGML models to the Hugging Face Hub! In the next section, we will explore how GGML actually quantize these models."
  },
  {
    "objectID": "posts/Quantize_Llama_2_models_using_ggml.html#quantization-with-ggml",
    "href": "posts/Quantize_Llama_2_models_using_ggml.html#quantization-with-ggml",
    "title": "Quantize Llama models with GGML and llama.cpp",
    "section": "Quantization with GGML",
    "text": "Quantization with GGML\nThe way GGML quantizes weights is not as sophisticated as GPTQ‚Äôs. Basically, it groups blocks of values and rounds them to a lower precision. Some techniques, like Q4_K_M and Q5_K_M, implement a higher precision for critical layers. In this case, every weight is stored in 4-bit precision, with the exception of half of the attention.wv and feed_forward.w2 tensors. Experimentally, this mixed precision proves to be a good tradeoff between accuracy and resource usage.\nIf we look into the ggml.c file, we can see how the blocks are defined. For example, the block_q4_0 structure is defined as:\n#define QK4_0 32\ntypedef struct {\n    ggml_fp16_t d;          // delta\n    uint8_t qs[QK4_0 / 2];  // nibbles / quants\n} block_q4_0;\nIn GGML, weights are processed in blocks, each consisting of 32 values. For each block, a scale factor (delta) is derived from the largest weight value. All weights in the block are then scaled, quantized, and packed efficiently for storage (nibbles). This approach significantly reduces the storage requirements while allowing for a relatively simple and deterministic conversion between the original and quantized weights.\nNow that we know more about the quantization process, we can compare the results with NF4 and GPTQ."
  },
  {
    "objectID": "posts/Quantize_Llama_2_models_using_ggml.html#nf4-vs.-ggml-vs.-gptq",
    "href": "posts/Quantize_Llama_2_models_using_ggml.html#nf4-vs.-ggml-vs.-gptq",
    "title": "Quantize Llama models with GGML and llama.cpp",
    "section": "NF4 vs.¬†GGML vs.¬†GPTQ",
    "text": "NF4 vs.¬†GGML vs.¬†GPTQ\nWhich technique is better for 4-bit quantization? To answer this question, we need to introduce the different backends that run these quantized LLMs. For GGML models, llama.cpp with Q4_K_M models is the way to go. For GPTQ models, we have two options: AutoGPTQ or ExLlama. Finally, NF4 models can directly be run in transformers with the --load-in-4bit flag.\nOobabooga ran multiple experiments in an excellent blog post that compare different models in terms of perplexity (lower is better):\n\n\n\nBased on these results, we can say that GGML models have a slight advantage in terms of perplexity. The difference is not particularly significant, which is why it is better to focus on the generation speed in terms of tokens/second. The best technique depends on your GPU: if you have enough VRAM to fit the entire quantized model, GPTQ with ExLlama will be the fastest. If that‚Äôs not the case, you can offload some layers and use GGML models with llama.cpp to run your LLM."
  },
  {
    "objectID": "posts/Quantize_Llama_2_models_using_ggml.html#conclusion",
    "href": "posts/Quantize_Llama_2_models_using_ggml.html#conclusion",
    "title": "Quantize Llama models with GGML and llama.cpp",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we introduced the GGML library and the new GGUF format to efficiently store these quantized models. We used it to quantize our own Llama model in different formats (Q4_K_M and Q5_K_M). We then ran the GGML model and pushed our bin files to the Hugging Face Hub. Finally, we delved deeper into GGML‚Äôs code to understand how it actually quantizes the weights and compared it to NF4 and GPTQ.\nQuantization is a formidable vector to democratize LLMs by lowering the cost of running them. In the future, mixed precision and other techniques will keep improving the performance we can achieve with quantized weights. Until then, I hope you enjoyed reading this article and learned something new."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": " Publications",
    "section": "",
    "text": "üìú Thesis\n\n\nAnomaly-based network intrusion detection using machine learning Maxime Labonne Polytechnic Institute of Paris (Institut Polytechnique de Paris), 2020\n\n\nüìù Proceedings\n\n\nSpam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection Maxime Labonne, Sean Moran Arxiv\nToward Formal Data Set Verification for Building Effective Machine Learning Models Jorge L√≥pez, Maxime Labonne, Claude Poletti KDIR 2021 - 13th International Conference on Knowledge Discovery and Information Retrieval\nShort-Term Flow-Based Bandwidth Forecasting using Machine Learning Maxime Labonne, Jorge L√≥pez, Claude Poletti, Jean-Baptiste Munier 2021 IEEE 22nd International Symposium on a World of Wireless, Mobile and Multimedia Networks (WoWMoM)\nPriority flow admission and routing in sdn: Exact and heuristic approaches Jorge L√≥pez, Maxime Labonne, Claude Poletti, Dallal Belabed 2020 IEEE 19th International Symposium on Network Computing and Applications (NCA)\nPredicting Bandwidth Utilization on Network Links Using Machine Learning Maxime Labonne, Charalampos Chatzinakis, Alexis Olivereau 2020 European Conference on Networks and Communications (EuCNC)\nUnsupervised protocol-based intrusion detection for real-world networks Maxime Labonne, Alexis Olivereau, Baptise Polv√©, Djamal Zeghlache 2020 International Conference on Computing, Networking and Communications (ICNC)\nA Cascade-structured Meta-Specialists Approach for Neural Network-based Intrusion Detection Maxime Labonne, Alexis Olivereau, Baptiste Polv√©, Djamal Zeghlache 2019 16th IEEE Annual Consumer Communications & Networking Conference (CCNC)\nAnomaly detection in vehicle-to-infrastructure communications Michele Russo, Maxime Labonne, Alexis Olivereau, Mohammad Rmayti 2018 IEEE 87th Vehicular Technology Conference (VTC Spring)"
  }
]