{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /minecraft/\n",
    "categories:\n",
    "- reinforcement learning\n",
    "colab: <a href=\"https://colab.research.google.com/drive/1hRxR_UM72JAe32YveLxqoC6eX6Lqd9c2?usp=sharing\"><img src=\"images/colab.png\" alt=\"Open In Colab\"></a>\n",
    "date: '2022-05-25'\n",
    "image: /images/minerl/thumbnail.jpg\n",
    "title: Create a Bot to Find Diamonds in Minecraft\n",
    "subtitle: Reinforcement Learning and Behavior Cloning in Python with MineRL\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Mbta3PiFFRv1"
   },
   "source": [
    "<center><img src=\"/images/minerl/thumbnail.jpg\"></center>\n",
    "\n",
    "Minecraft is an incredible challenge for Reinforcement Learning.\n",
    "\n",
    "It's a huge game, with many mechanics and complex sequences of actions. It takes an [entire wiki](https://minecraft.fandom.com/wiki/Minecraft_Wiki) with **over 8000 pages** just to teach humans how to play Minecraft. So how good can be machine learning?\n",
    "\n",
    "This is the question we'll answer in this article. We'll design a bot and try to achieve one of the most difficult challenges in Minecraft: **finding diamonds from scratch**. To make things even worse, we will take on this challenge in randomly generated worlds so we can't learn a particular seed.\n",
    "\n",
    "<center><img src=\"/images/minerl/sequence.png\">Sequence of actions to find diamonds</center>\n",
    "\n",
    "What we're gonna talk about is not limited to Minecraft. It can be applied to similar **complex environments**. More specifically, we will implement two different techniques that will become the backbone of our intelligent agent.\n",
    "\n",
    "But before we can train an agent, we need to understand **how to interact** with the environment. Let's start with a scripted bot to get familiar with the syntax. We'll use [MineRL](https://minerl.io/), a fantastic library to build AI applications in Minecraft.\n",
    "\n",
    "The code used in this article is available on [Google Colab](https://colab.research.google.com/drive/1hRxR_UM72JAe32YveLxqoC6eX6Lqd9c2?usp=sharing). It is a simplified and finetuned version of the excellent notebooks made by the organizers of the [MineRL 2021 competition](https://github.com/KarolisRam/MineRL2021-Intro-baselines) (MIT License).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SoRhhLx-fHT"
   },
   "outputs": [],
   "source": [
    "# # Install JDK, OpenGL, etc.\n",
    "!sudo add-apt-repository -y ppa:openjdk-r/ppa > /dev/null 2>&1\n",
    "!sudo apt purge openjdk-* > /dev/null 2>&1\n",
    "!sudo apt install openjdk-8-jdk xvfb xserver-xephyr vnc4server python-opengl ffmpeg > /dev/null 2>&1\n",
    "\n",
    "# # Install MineRL, the virtual display, and a video renderer\n",
    "!pip install -q -U minerl pyvirtualdisplay colabgymrender imageio==2.4.1\n",
    "\n",
    "# RL environment\n",
    "import gym\n",
    "import minerl\n",
    "\n",
    "# Visualization\n",
    "from colabgymrender.recorder import Recorder\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Others\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "logging.disable(logging.ERROR)\n",
    "\n",
    "# Create virtual display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uK28ZdPTS7Fi"
   },
   "source": [
    "## üìú I. Scripted bot\n",
    "\n",
    "MineRL allows us to launch Minecraft in Python and interact with the game. This is done through the popular `gym` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "yxXJl2nF-eMC",
    "outputId": "de748d69-c7d0-48fd-87c9-8a2b5cc1f584"
   },
   "outputs": [],
   "source": [
    "env = gym.make('MineRLObtainDiamond-v0')\n",
    "env = Recorder(env, './video', fps=60)\n",
    "env.seed(21)\n",
    "obs = env.reset()\n",
    "env.release()\n",
    "env.play()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "byTHRbUl-grB"
   },
   "source": [
    "<center><img src=\"/images/minerl/start.png\"></center>\n",
    "\n",
    "We are in front of a tree. As you can see, the resolution is **quite low**. A low resolution means fewer pixels, which speeds things up. Fortunately for us, neural networks don't need a 4K resolution to understand what's happening on screen.\n",
    "\n",
    "Now, we would like to **interact** with the game. What can our agent do? Here's the [list of possible actions](https://minerl.io/docs/environments/#id14):\n",
    "\n",
    "<center><img src=\"/images/minerl/actions.png\" width=\"800\"></center>\n",
    "\n",
    "The first step to find diamonds is to **get wood** to make a crafting table and a wooden pickaxe.\n",
    "\n",
    "Let's try to get closer to the tree. It means that we need to hold the \"forward\" button for less than a second. With MineRL, there are **20 actions processed per second**: we don't need a full second so let's process it 5 times, and wait for 40 more ticks.\n",
    "\n",
    "<center><img src=\"/images/minerl/sequence1.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "TAEOYYBOScnL",
    "outputId": "5550ae9b-f82a-4cb5-e083-baa1a6916169"
   },
   "outputs": [],
   "source": [
    "# Define the sequence of actions\n",
    "script = ['forward'] * 5 + [''] * 40\n",
    "\n",
    "env = gym.make('MineRLObtainDiamond-v0')\n",
    "env = Recorder(env, './video', fps=60)\n",
    "env.seed(21)\n",
    "obs = env.reset()\n",
    "\n",
    "for action in script:\n",
    "    # Get the action space (dict of possible actions)\n",
    "    action_space = env.action_space.noop()\n",
    "\n",
    "    # Activate the selected action in the script\n",
    "    action_space[action] = 1\n",
    "\n",
    "    # Update the environment with the new action space\n",
    "    obs, reward, done, _ = env.step(action_space)\n",
    "\n",
    "env.release()\n",
    "env.play()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WeBBNgSoXk6o"
   },
   "source": [
    "<center><img src=\"/images/minerl/start2.gif\"></center>\n",
    "\n",
    "Great, let's chop this tree now. We need four actions in total:\n",
    "\n",
    "* **Forward** to go in front of the tree;\n",
    "* **Attack** to chop the tree;\n",
    "* **Camera** to look up or down;\n",
    "* **Jump** to get the final piece of wood.\n",
    "\n",
    "<center><img src=\"/images/minerl/sequence2.png\"></center>\n",
    "\n",
    "Handling the camera can be a hassle. To simplify the syntax, we're gonna use the `str_to_act` function from [this GitHub repository](https://github.com/KarolisRam/MineRL2021-Intro-baselines) (MIT license). This is what the new script looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucxXIPfg_C5S"
   },
   "outputs": [],
   "source": [
    "script = []\n",
    "script += [''] * 20 \n",
    "script += ['forward'] * 5\n",
    "script += ['attack'] * 61\n",
    "script += ['camera:[-10,0]'] * 7  # Look up\n",
    "script += ['attack'] * 240\n",
    "script += ['jump']\n",
    "script += ['forward'] * 10        # Jump forward\n",
    "script += ['camera:[-10,0]'] * 2  # Look up\n",
    "script += ['attack'] * 150\n",
    "script += ['camera:[10,0]'] * 7   # Look down\n",
    "script += [''] * 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117,
     "referenced_widgets": [
      "4b0051bb39ed49cfaade86a198b96efb",
      "cfbe87f2867d49ff95d344e1dda79985",
      "0c729a8145e04d199ab702f2177d55f6",
      "636c32f65ade41488826702d5f998076",
      "551deaa66f16415ca9e165c2071c6029",
      "c2c27918f8354524a459171d2b8e2210",
      "29b3f496af15428d9e3a01e296688cf3",
      "210b7fbce33d413fb8963ad68ed8b55e",
      "20eb7527dfbd4a5bb5938ae70d2f8dad",
      "5a7d4e87a15e47319488a0f4c1270661",
      "2d0650782a0f4b9881bf5d210240f8fb"
     ]
    },
    "id": "ArETKjU8SfZH",
    "outputId": "0b2babac-dda5-4e77-bae3-fbe141ff431a"
   },
   "outputs": [],
   "source": [
    "# Code from https://github.com/KarolisRam/MineRL2021-Intro-baselines\n",
    "def str_to_act(env, actions):\n",
    "    action_space = env.action_space.noop()\n",
    "    for action in actions.split():\n",
    "        if ':' in action:\n",
    "            k, v = action.split(':')\n",
    "            if k == 'camera':\n",
    "                action_space[k] = eval(v)\n",
    "            else:\n",
    "                action_space[k] = v\n",
    "        else:\n",
    "            action_space[action] = 1\n",
    "    return action_space\n",
    "    \n",
    "env = gym.make('MineRLObtainDiamond-v0')\n",
    "env = Recorder(env, './video', fps=60)\n",
    "env.seed(21)\n",
    "obs = env.reset()\n",
    " \n",
    "for action in tqdm(script):\n",
    "    obs, reward, done, _ = env.step(str_to_act(env, action))\n",
    "\n",
    "env.release()\n",
    "env.play()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aozc93YS41xd"
   },
   "source": [
    "{{< video https://youtu.be/3A2P0lQs2c0 >}}\n",
    "\n",
    "\n",
    "The agent efficiently chopped the **entire tree**. This is a good start, but we would like to do it in a more automated way...\n",
    "\n",
    "## üß† II. Deep Learning\n",
    "\n",
    "Our bot works well in a fixed environment, but what happens if we change the seed or its starting point?\n",
    "\n",
    "Everything is **scripted** so the agent would probably try to chop a non-existent tree.\n",
    "\n",
    "This approach is **too static** for our requirements: we need something that can adapt to new environments. Instead of scripting orders, we want an AI that knows how to chop trees. Naturally, reinforcement learning is a pertinent framework to train this agent. More specifically, deep RL seems to be the solution since we're processing images to select the best actions.\n",
    "\n",
    "There are two ways of implementing it:\n",
    "\n",
    "* **Pure deep RL**: the agent is trained from scratch by interacting with the environment. It is rewarded every time it chops a tree.\n",
    "* **Imitation learning**: the agent learns how to chop trees from a dataset. In this case, it is a sequence of actions to chop trees made by a human.\n",
    "\n",
    "The two approaches have the same outcome, but they're not equivalent. According to the authors of the [MineRL 2021 competition](https://github.com/KarolisRam/MineRL2021-Intro-baselines), it takes **8 hours** for the pure RL solution and **15 minutes** for the imitation learning agent to reach the same level of performance.\n",
    "\n",
    "We don't have that much time to spend, so we're going for the Imitation Learning solution. This technique is also called **Behavior Cloning**, which is the simplest form of imitation.\n",
    "\n",
    "Note that Imitation Learning is not always more efficient than RL. If you want to know more about it, Kumar et al. wrote a great [blog post](https://bair.berkeley.edu/blog/2022/04/25/rl-or-bc/) about this topic.\n",
    "\n",
    "<center><img src=\"/images/minerl/cnn.png\"></center>\n",
    "\n",
    "The problem is reduced to a multi-class classification task. Our dataset consists of mp4 videos, so we'll use a [Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN) to translate these images into relevant actions. Our goal is also to **limit the number of actions** (classes) that can be taken so the CNN has fewer options, which means it'll be trained more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eGSExa_C42Hs"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_shape, output_dim):\n",
    "        super().__init__()\n",
    "        n_input_channels = input_shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, observations):\n",
    "        return self.cnn(observations)\n",
    "\n",
    "def dataset_action_batch_to_actions(dataset_actions, camera_margin=5):\n",
    "    camera_actions = dataset_actions[\"camera\"].squeeze()\n",
    "    attack_actions = dataset_actions[\"attack\"].squeeze()\n",
    "    forward_actions = dataset_actions[\"forward\"].squeeze()\n",
    "    jump_actions = dataset_actions[\"jump\"].squeeze()\n",
    "    batch_size = len(camera_actions)\n",
    "    actions = np.zeros((batch_size,), dtype=int)\n",
    "\n",
    "    for i in range(len(camera_actions)):\n",
    "        if camera_actions[i][0] < -camera_margin:\n",
    "            actions[i] = 3\n",
    "        elif camera_actions[i][0] > camera_margin:\n",
    "            actions[i] = 4\n",
    "        elif camera_actions[i][1] > camera_margin:\n",
    "            actions[i] = 5\n",
    "        elif camera_actions[i][1] < -camera_margin:\n",
    "            actions[i] = 6\n",
    "        elif forward_actions[i] == 1:\n",
    "            if jump_actions[i] == 1:\n",
    "                actions[i] = 2\n",
    "            else:\n",
    "                actions[i] = 1\n",
    "        elif attack_actions[i] == 1:\n",
    "            actions[i] = 0\n",
    "        else:\n",
    "            actions[i] = -1\n",
    "    return actions\n",
    "\n",
    "class ActionShaping(gym.ActionWrapper):\n",
    "    def __init__(self, env, camera_angle=10):\n",
    "        super().__init__(env)\n",
    "        self.camera_angle = camera_angle\n",
    "        self._actions = [\n",
    "            [('attack', 1)],\n",
    "            [('forward', 1)],\n",
    "            [('jump', 1)],\n",
    "            [('camera', [-self.camera_angle, 0])],\n",
    "            [('camera', [self.camera_angle, 0])],\n",
    "            [('camera', [0, self.camera_angle])],\n",
    "            [('camera', [0, -self.camera_angle])],\n",
    "        ]\n",
    "        self.actions = []\n",
    "        for actions in self._actions:\n",
    "            act = self.env.action_space.noop()\n",
    "            for a, v in actions:\n",
    "                act[a] = v\n",
    "                act['attack'] = 1\n",
    "            self.actions.append(act)\n",
    "        self.action_space = gym.spaces.Discrete(len(self.actions))\n",
    "\n",
    "    def action(self, action):\n",
    "        return self.actions[action]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BnN9GSro-DNa"
   },
   "source": [
    "In this example, we manually define **7 relevant actions**: attack, forward, jump, and move the camera (left, right, up, down). Another popular approach is to apply [K-means](https://minerl.readthedocs.io/en/latest/tutorials/k-means.html) in order to automatically retrieve the most relevant actions taken by humans. In any case, the objective is to discard the least useful actions to complete our objective, such as crafting in our example.\n",
    "\n",
    "Let's train our CNN on the `MineRLTreechop-v0` dataset. Other datasets can be found at [this address](https://minerl.io/docs/environments/index.html#basic-environments). We chose a learning rate of 0.0001 and 6 epochs with a batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793,
     "referenced_widgets": [
      "5e7dd4a5e5444ae6bc97e167f603440c",
      "9276e6197c244d8ca0e247b86c187479",
      "8bea3cd46e944cff9b8c49f1e8a36720",
      "5ff5883624c64cde8f781acaf45254bc",
      "41ca07226331474aaf6229a12a759b9c",
      "b1978574f2024c3bb36c43243b4be795",
      "2820c8683b844cc1a224155014b93a0f",
      "743e4ddfea0249ed964580871243e30e",
      "3611328a10c24946aecee6b7d48f5059",
      "87f7892ea3c542c8b5f7159af7670e36",
      "3846a64128df4aa29afc103f17548d5a"
     ]
    },
    "id": "0EJ6FMvrhbW5",
    "outputId": "4f5da5da-86ff-4b85-f853-75eabcf0c64c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: https://minerl.s3.amazonaws.com/v4/MineRLTreechop-v0.tar: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1511.0/1510.73792 [00:25<00:00, 58.46MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7dd4a5e5444ae6bc97e167f603440c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  2000 | Training loss = 0.901\n",
      "Step  4000 | Training loss = 0.878\n",
      "Step  6000 | Training loss = 0.836\n",
      "Step  8000 | Training loss = 0.826\n",
      "Step 10000 | Training loss = 0.828\n",
      "Step 12000 | Training loss = 0.805\n",
      "Step 14000 | Training loss = 0.804\n",
      "Step 16000 | Training loss = 0.773\n",
      "Step 18000 | Training loss = 0.791\n",
      "Step 20000 | Training loss = 0.789\n",
      "Step 22000 | Training loss = 0.789\n",
      "Step 24000 | Training loss = 0.816\n",
      "Step 26000 | Training loss = 0.785\n",
      "Step 28000 | Training loss = 0.769\n",
      "Step 30000 | Training loss = 0.789\n",
      "Step 32000 | Training loss = 0.777\n",
      "Step 34000 | Training loss = 0.763\n",
      "Step 36000 | Training loss = 0.738\n",
      "Step 38000 | Training loss = 0.744\n",
      "Step 40000 | Training loss = 0.751\n",
      "Step 42000 | Training loss = 0.763\n",
      "Step 44000 | Training loss = 0.764\n",
      "Step 46000 | Training loss = 0.744\n",
      "Step 48000 | Training loss = 0.732\n",
      "Step 50000 | Training loss = 0.740\n",
      "Step 52000 | Training loss = 0.748\n",
      "Step 54000 | Training loss = 0.678\n",
      "Step 56000 | Training loss = 0.765\n",
      "Step 58000 | Training loss = 0.727\n",
      "Step 60000 | Training loss = 0.735\n",
      "Step 62000 | Training loss = 0.707\n",
      "Step 64000 | Training loss = 0.716\n",
      "Step 66000 | Training loss = 0.718\n",
      "Step 68000 | Training loss = 0.710\n",
      "Step 70000 | Training loss = 0.692\n",
      "Step 72000 | Training loss = 0.693\n",
      "Step 74000 | Training loss = 0.687\n",
      "Step 76000 | Training loss = 0.695\n",
      "CPU times: user 15min 21s, sys: 55.3 s, total: 16min 16s\n",
      "Wall time: 26min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get data\n",
    "minerl.data.download(directory='data', environment='MineRLTreechop-v0')\n",
    "data = minerl.data.make(\"MineRLTreechop-v0\", data_dir='data', num_workers=2)\n",
    "\n",
    "# Model\n",
    "model = CNN((3, 64, 64), 7).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "step = 0\n",
    "losses = []\n",
    "for state, action, _, _, _ \\\n",
    "          in tqdm(data.batch_iter(num_epochs=6, batch_size=32, seq_len=1)):\n",
    "    # Get pov observations\n",
    "    obs = state['pov'].squeeze().astype(np.float32)\n",
    "    # Transpose and normalize\n",
    "    obs = obs.transpose(0, 3, 1, 2) / 255.0\n",
    "\n",
    "    # Translate batch of actions for the ActionShaping wrapper\n",
    "    actions = dataset_action_batch_to_actions(action)\n",
    "\n",
    "    # Remove samples with no corresponding action\n",
    "    mask = actions != -1\n",
    "    obs = obs[mask]\n",
    "    actions = actions[mask]\n",
    "\n",
    "    # Update weights with backprop\n",
    "    logits = model(torch.from_numpy(obs).float().cuda())\n",
    "    loss = criterion(logits, torch.from_numpy(actions).long().cuda())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss\n",
    "    step += 1\n",
    "    losses.append(loss.item())\n",
    "    if (step % 2000) == 0:\n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "        tqdm.write(f'Step {step:>5} | Training loss = {mean_loss:.3f}')\n",
    "        losses.clear()\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "del data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yWa-uQWDXVAj"
   },
   "source": [
    "Our model is trained. We can now instantiate an environment and see how it behaves. If the training was successful, it should frantically **cut all the trees in sight**.\n",
    "\n",
    "This time, we'll use the `ActionShaping` wrapper to map the array of numbers created with `dataset_action_batch_to_actions` to discrete actions in MineRL.\n",
    "\n",
    "Our model needs a **pov observation** in the correct format and outputs logits. These logits can be turned into a probability distribution over a set of 7 actions with the `softmax` function. We then randomly choose an action based on the probabilities. The selected action is implemented in MineRL thanks to `env.step(action)`.\n",
    "\n",
    "This process is repeated as many times as we want. Let's do it 1000 times and watch the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "P2KXrLoV6K01"
   },
   "outputs": [],
   "source": [
    "model = CNN((3, 64, 64), 7).cuda()\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "env = gym.make('MineRLObtainDiamond-v0')\n",
    "env1 = Recorder(env, './video', fps=60)\n",
    "env = ActionShaping(env1)\n",
    "\n",
    "action_list = np.arange(env.action_space.n)\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for step in tqdm(range(1000)):\n",
    "    # Get input in the correct format\n",
    "    obs = torch.from_numpy(obs['pov'].transpose(2, 0, 1)[None].astype(np.float32) / 255).cuda()\n",
    "    # Turn logits into probabilities\n",
    "    probabilities = torch.softmax(model(obs), dim=1)[0].detach().cpu().numpy()\n",
    "    # Sample action according to the probabilities\n",
    "    action = np.random.choice(action_list, p=probabilities)\n",
    "\n",
    "    obs, reward, _, _ = env.step(action)\n",
    "\n",
    "env1.release()\n",
    "env1.play()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FlM30cHA49kk"
   },
   "source": [
    "{{< video https://youtu.be/umvrmQ_MYSI >}}\n",
    "\n",
    "\n",
    "Our agent is quite chaotic but it manages to chop trees in this **new, unseen environment**. Now, how to find diamonds?\n",
    "\n",
    "## ‚õèÔ∏è III. Script + Imitation Learning\n",
    "\n",
    "A simple yet powerful approach consists of **combining** scripted actions with artificial intelligence. Learn the boring stuff, script the knowledge.\n",
    "\n",
    "In this paradigm, we'll use the CNN to get a healthy amount of wood (3000 steps). Then, we can **script a sequence** to craft planks, sticks, a crafting table, a wooden pickaxe, and start mining stone (it should be below our feet). This stone can then be used to craft a stone pickaxe, which can mine iron ore.\n",
    "\n",
    "<center><img src=\"/images/minerl/sequence3.png\"></center>\n",
    "\n",
    "This is when things get complicated: iron ore is **quite rare**, so we would need to run the game for a while to find a deposit. Then, we would have to craft a furnace and melt it to get the iron pickaxe. Finally, we would have to go even deeper and be **even luckier** to obtain a diamond without falling into lava.\n",
    "\n",
    "As you can see, it's doable but the outcome is fairly random. We could train another agent to [find diamonds](https://minerl.readthedocs.io/en/latest/environments/index.html#minerlobtaindiamond-v0), and even a third one to [create the iron pickaxe](https://minerl.readthedocs.io/en/latest/environments/index.html#minerlobtainironpickaxe-v0). If you're interested in more complex approaches, you can read the results of the [MineRL Diamond 2021 Competition](https://arxiv.org/abs/2202.10583) by Kanervisto et al. It describes several solutions using different clever techniques, including end-to-end deep learning architectures. Nonetheless, it is a complex problem and no team managed to consistently find diamonds, if at all.\n",
    "\n",
    "This is why we will limit ourselves to obtaining a stone pickaxe in the following example, but you can modify the code to go further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-KSoo6cMhUwd"
   },
   "outputs": [],
   "source": [
    "# Craft 4 planks, 2 sticks, 2 crafting tables, and place it\n",
    "script = []\n",
    "script += ['craft:planks'] * 6\n",
    "script += ['craft:stick'] * 2\n",
    "script += ['craft:crafting_table'] * 2\n",
    "script += ['camera:[10,0]'] * 18\n",
    "script += ['attack'] * 20\n",
    "script += [''] * 10\n",
    "script += ['jump']\n",
    "script += [''] * 5\n",
    "script += ['place:crafting_table']\n",
    "script += [''] * 10\n",
    "\n",
    "# Craft a wooden pickaxe and equip it\n",
    "script += ['camera:[-1,0]']\n",
    "script += ['nearbyCraft:wooden_pickaxe']\n",
    "script += ['camera:[1,0]']\n",
    "script += [''] * 10\n",
    "script += ['equip:wooden_pickaxe']\n",
    "script += [''] * 10\n",
    "\n",
    "# Dig stone\n",
    "script += ['attack'] * 500\n",
    "\n",
    "# Craft stone pickaxe\n",
    "script += [''] * 10\n",
    "script += ['jump']\n",
    "script += [''] * 5\n",
    "script += ['place:crafting_table']\n",
    "script += [''] * 10\n",
    "script += ['camera:[-1,0]']\n",
    "script += ['nearbyCraft:stone_pickaxe']\n",
    "script += ['camera:[1,0]']\n",
    "script += [''] * 10\n",
    "script += ['equip:stone_pickaxe']\n",
    "script += [''] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ER7iouq-rPZq"
   },
   "outputs": [],
   "source": [
    "model = CNN((3, 64, 64), 7).cuda()\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "env_script = gym.make('MineRLObtainDiamond-v0')\n",
    "env_cnn = Recorder(env_script, './video', fps=60)\n",
    "env_script = ActionShaping(env_cnn)\n",
    "\n",
    "action_list = np.arange(env_script.action_space.n)\n",
    "\n",
    "for _ in range(10):\n",
    "    obs = env_script.reset()\n",
    "    done = False\n",
    "\n",
    "    # 1. Get wood with the CNN\n",
    "    for i in tqdm(range(3000)):\n",
    "        obs = torch.from_numpy(obs['pov'].transpose(2, 0, 1)[None].astype(np.float32) / 255).cuda()\n",
    "        probabilities = torch.softmax(model(obs), dim=1)[0].detach().cpu().numpy()\n",
    "        action = np.random.choice(action_list, p=probabilities)\n",
    "        obs, reward, done, _ = env_script.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # 2. Craft stone pickaxe with scripted actions\n",
    "    if not done:\n",
    "        for action in tqdm(script):\n",
    "            obs, reward, done, _ = env_cnn.step(str_to_act(env_cnn, action))\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    print(obs[\"inventory\"])\n",
    "    env_cnn.release()\n",
    "    env_cnn.play()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "urN_RLOM7M2e"
   },
   "source": [
    "{{< video https://youtu.be/7LnjA7Bxf6A >}}\n",
    "\n",
    "\n",
    "We can see our agent chopping wood like a madman during the first 3000 steps, then our script takes over and completes the task. It might not be obvious, but the command `print(obs.inventory)` shows a stone pickaxe. Note that this is a **cherry-picked** example: most of the runs don't end that well.\n",
    "\n",
    "There are **several reasons** why the agent may fail: it can spawn in a hostile environment (water, lava, etc.), in an area without wood, or even fall and die. Playing with different seeds will give you a good understanding of the complexity of this problem and, hopefully, ideas to build event better agents.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "I hope you enjoyed this little guide to reinforcement learning in Minecraft. Beyond its obvious popularity, Minecraft is an interesting environment to try and test RL agents. Like [NetHack](https://nethackchallenge.com/), it requires a **thorough knowledge** of its mechanics to plan precise sequences of actions in a procedurally-generated world. In this article,\n",
    "\n",
    "* We learned how to use **MineRL**;\n",
    "* We saw **two approaches** (script and behavior cloning) and how to combine them;\n",
    "* We **visualized** the agent's actions with short videos.\n",
    "\n",
    "The main drawback of the environment is its **slow processing time**. Minecraft is not a lightweight game like NetHack or Pong, which is why the agents take a long time to be trained. If this is a problem for you, I would recommend lighter environments like [Gym Retro](https://openai.com/blog/gym-retro/).\n",
    "\n",
    "Thank you for your attention! Feel free to [follow me on Twitter](https://twitter.com/maximelabonne) if you're interested in AI applied to video games.  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RL_in_Minecraft.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c729a8145e04d199ab702f2177d55f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_210b7fbce33d413fb8963ad68ed8b55e",
      "max": 543,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_20eb7527dfbd4a5bb5938ae70d2f8dad",
      "value": 543
     }
    },
    "20eb7527dfbd4a5bb5938ae70d2f8dad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "210b7fbce33d413fb8963ad68ed8b55e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2820c8683b844cc1a224155014b93a0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "29b3f496af15428d9e3a01e296688cf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2d0650782a0f4b9881bf5d210240f8fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3611328a10c24946aecee6b7d48f5059": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3846a64128df4aa29afc103f17548d5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "41ca07226331474aaf6229a12a759b9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b0051bb39ed49cfaade86a198b96efb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cfbe87f2867d49ff95d344e1dda79985",
       "IPY_MODEL_0c729a8145e04d199ab702f2177d55f6",
       "IPY_MODEL_636c32f65ade41488826702d5f998076"
      ],
      "layout": "IPY_MODEL_551deaa66f16415ca9e165c2071c6029"
     }
    },
    "551deaa66f16415ca9e165c2071c6029": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a7d4e87a15e47319488a0f4c1270661": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e7dd4a5e5444ae6bc97e167f603440c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9276e6197c244d8ca0e247b86c187479",
       "IPY_MODEL_8bea3cd46e944cff9b8c49f1e8a36720",
       "IPY_MODEL_5ff5883624c64cde8f781acaf45254bc"
      ],
      "layout": "IPY_MODEL_41ca07226331474aaf6229a12a759b9c"
     }
    },
    "5ff5883624c64cde8f781acaf45254bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87f7892ea3c542c8b5f7159af7670e36",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_3846a64128df4aa29afc103f17548d5a",
      "value": " 77605/? [26:01&lt;00:00, 99.37it/s]"
     }
    },
    "636c32f65ade41488826702d5f998076": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a7d4e87a15e47319488a0f4c1270661",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_2d0650782a0f4b9881bf5d210240f8fb",
      "value": " 543/543 [00:42&lt;00:00, 12.93it/s]"
     }
    },
    "743e4ddfea0249ed964580871243e30e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "87f7892ea3c542c8b5f7159af7670e36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bea3cd46e944cff9b8c49f1e8a36720": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_743e4ddfea0249ed964580871243e30e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3611328a10c24946aecee6b7d48f5059",
      "value": 1
     }
    },
    "9276e6197c244d8ca0e247b86c187479": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1978574f2024c3bb36c43243b4be795",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_2820c8683b844cc1a224155014b93a0f",
      "value": ""
     }
    },
    "b1978574f2024c3bb36c43243b4be795": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2c27918f8354524a459171d2b8e2210": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfbe87f2867d49ff95d344e1dda79985": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2c27918f8354524a459171d2b8e2210",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_29b3f496af15428d9e3a01e296688cf3",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
