{
  
    
        "post0": {
            "title": "Create a Bot to Find Diamonds in Minecraft",
            "content": "Minecraft is the next frontier for Artificial Intelligence. . It&#39;s a huge game, with many mechanics and complex sequences of actions. It takes an entire wiki with over 8000 pages just to teach humans how to play Minecraft. So how good can be artificial intelligence? . This is the question we&#39;ll answer in this article. We&#39;ll design a bot and try to achieve one of the most difficult challenges in Minecraft: finding diamonds from scratch. To make things even worse, we will take on this challenge in randomly generated worlds so we can&#39;t learn a particular seed. . Sequence of actions to find diamondsWhat we&#39;re gonna talk about is not limited to Minecraft. It can be applied to similar complex environments . More specifically, we will implement two different techniques that will become the backbone of our intelligent agent. . But before we can train an agent, we need to understand how to interact with the environment. Let&#39;s start with a scripted bot to get familiar with the syntax. We&#39;ll use MineRL, a fantastic library to build AI applications in Minecraft. . The code used in this article is available on Google Colab. It is a simplified and finetuned version of the excellent notebooks made by the organizers of the MineRL 2021 competition (MIT License). . !sudo add-apt-repository -y ppa:openjdk-r/ppa &gt; /dev/null 2&gt;&amp;1 !sudo apt purge openjdk-* &gt; /dev/null 2&gt;&amp;1 !sudo apt install openjdk-8-jdk xvfb xserver-xephyr vnc4server python-opengl ffmpeg &gt; /dev/null 2&gt;&amp;1 # # Install MineRL, the virtual display, and a video renderer !pip install -q -U minerl pyvirtualdisplay colabgymrender # RL environment import gym import minerl # Visualization from colabgymrender.recorder import Recorder from pyvirtualdisplay import Display # Others import numpy as np from tqdm.notebook import tqdm import logging logging.disable(logging.ERROR) # Create virtual display display = Display(visible=0, size=(400, 300)) display.start() . &#128220; I. Scripted bot . MineRL allows us to launch Minecraft in Python and interact with the game. This is done through the popular gym library. . env = gym.make(&#39;MineRLObtainDiamond-v0&#39;) env = Recorder(env, &#39;./video&#39;, fps=60) env.seed(21) obs = env.reset() env.release() env.play() . We are in front of a tree. As you can see, the resolution is quite low. A low resolution means fewer pixels, which speeds things up. Fortunately for us, neural networks don&#39;t need a 4K resolution to understand what&#39;s happening on screen. . Now, we would like to interact with the game. What can our agent do? Here&#39;s the list of possible actions: . The first step to find diamonds is to get wood to make a crafting table and a wooden pickaxe. . Let&#39;s try to get closer to the tree. It means that we need to hold the &quot;forward&quot; button for less than a second. With MineRL, there are 20 actions processed per second: we don&#39;t need a full second so let&#39;s process it 5 times, and wait for 40 more ticks. . script = [&#39;forward&#39;] * 5 + [&#39;&#39;] * 40 env = gym.make(&#39;MineRLObtainDiamond-v0&#39;) env = Recorder(env, &#39;./video&#39;, fps=60) env.seed(21) obs = env.reset() for action in script: # Get the action space (dict of possible actions) action_space = env.action_space.noop() # Activate the selected action in the script action_space[action] = 1 # Update the environment with the new action space obs, reward, done, _ = env.step(action_space) env.release() env.play() . Great, let&#39;s chop this tree now. We need four actions in total: . Forward to go in front of the tree; | Attack to chop the tree; | Camera to look up or down; | Jump to get the final piece of wood. | . Handling the camera can be a hassle. To simplify the syntax, we&#39;re gonna use the str_to_act function from this GitHub repository (MIT license). This is what the new script looks like: . script = [] script += [&#39;&#39;] * 20 script += [&#39;forward&#39;] * 5 script += [&#39;attack&#39;] * 61 script += [&#39;camera:[-10,0]&#39;] * 7 # Look up script += [&#39;attack&#39;] * 240 script += [&#39;jump&#39;] script += [&#39;forward&#39;] * 10 # Jump forward script += [&#39;camera:[-10,0]&#39;] * 2 # Look up script += [&#39;attack&#39;] * 150 script += [&#39;camera:[10,0]&#39;] * 7 # Look down script += [&#39;&#39;] * 40 . def str_to_act(env, actions): action_space = env.action_space.noop() for action in actions.split(): if &#39;:&#39; in action: k, v = action.split(&#39;:&#39;) if k == &#39;camera&#39;: action_space[k] = eval(v) else: action_space[k] = v else: action_space[action] = 1 return action_space env = gym.make(&#39;MineRLObtainDiamond-v0&#39;) env = Recorder(env, &#39;./video&#39;, fps=60) env.seed(21) obs = env.reset() for action in tqdm(script): obs, reward, done, _ = env.step(str_to_act(env, action)) env.release() env.play() . . The agent efficiently chopped the entire tree. This is a good start, but we would like to do it in a more automated way... . &#129504; II. Deep Learning . Our bot works well in a fixed environment, but what happens if we change the seed or its starting point? . Everything is scripted so the agent would probably try to chop a non-existent tree. . This approach is too static for our requirements: we need something that can adapt to new environments. Instead of scripting orders, we want an AI that knows how to chop trees. Naturally, reinforcement learning is a pertinent framework to train this agent. More specifically, deep RL seems to be the solution since we&#39;re processing images to select the best actions. . There are two ways of implementing it: . Pure deep RL: the agent is trained from scratch by interacting with the environment. It is rewarded every time it chops a tree. | Imitation learning: the agent learns how to chop trees from a dataset. In this case, it is a sequence of actions to chop trees made by a human. | . The two approaches have the same outcome, but they&#39;re not equivalent. According to the authors of the MineRL 2021 competition, it takes 8 hours for the pure RL solution and 15 minutes for the imitation learning agent to reach the same level of performance. . We don&#39;t have that much time to spend, so we&#39;re going for the Imitation Learning solution. This technique is also called Behavior Cloning, which is the simplest form of imitation. . Note that Imitation Learning is not always more efficient than RL. If you want to know more about it, Kumar et al. wrote a great blog post about this topic. . The problem is reduced to a multi-class classification task. Our dataset consists of mp4 videos, so we&#39;ll use a Convolutional Neural Network (CNN) to translate these images into relevant actions. Our goal is also to limit the number of actions (classes) that can be taken so the CNN has fewer options, which means it&#39;ll be trained more efficiently. . import torch import torch.nn as nn class CNN(nn.Module): def __init__(self, input_shape, output_dim): super().__init__() n_input_channels = input_shape[0] self.cnn = nn.Sequential( nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4), nn.BatchNorm2d(32), nn.ReLU(), nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.BatchNorm2d(64), nn.ReLU(), nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.BatchNorm2d(64), nn.ReLU(), nn.Flatten(), nn.Linear(1024, 512), nn.ReLU(), nn.Linear(512, output_dim) ) def forward(self, observations): return self.cnn(observations) def dataset_action_batch_to_actions(dataset_actions, camera_margin=5): camera_actions = dataset_actions[&quot;camera&quot;].squeeze() attack_actions = dataset_actions[&quot;attack&quot;].squeeze() forward_actions = dataset_actions[&quot;forward&quot;].squeeze() jump_actions = dataset_actions[&quot;jump&quot;].squeeze() batch_size = len(camera_actions) actions = np.zeros((batch_size,), dtype=int) for i in range(len(camera_actions)): if camera_actions[i][0] &lt; -camera_margin: actions[i] = 3 elif camera_actions[i][0] &gt; camera_margin: actions[i] = 4 elif camera_actions[i][1] &gt; camera_margin: actions[i] = 5 elif camera_actions[i][1] &lt; -camera_margin: actions[i] = 6 elif forward_actions[i] == 1: if jump_actions[i] == 1: actions[i] = 2 else: actions[i] = 1 elif attack_actions[i] == 1: actions[i] = 0 else: actions[i] = -1 return actions class ActionShaping(gym.ActionWrapper): def __init__(self, env, camera_angle=10): super().__init__(env) self.camera_angle = camera_angle self._actions = [ [(&#39;attack&#39;, 1)], [(&#39;forward&#39;, 1)], [(&#39;jump&#39;, 1)], [(&#39;camera&#39;, [-self.camera_angle, 0])], [(&#39;camera&#39;, [self.camera_angle, 0])], [(&#39;camera&#39;, [0, self.camera_angle])], [(&#39;camera&#39;, [0, -self.camera_angle])], ] self.actions = [] for actions in self._actions: act = self.env.action_space.noop() for a, v in actions: act[a] = v act[&#39;attack&#39;] = 1 self.actions.append(act) self.action_space = gym.spaces.Discrete(len(self.actions)) def action(self, action): return self.actions[action] . In this example, we manually define 7 relevant actions: attack, forward, jump, and move the camera (left, right, up, down). Another popular approach is to apply K-means in order to automatically retrieve the most relevant actions taken by humans. In any case, the objective is to discard the least useful actions to complete our objective, such as crafting in our example. . Let&#39;s train our CNN on the MineRLTreechop-v0 dataset. Other datasets can be found at this address. We chose a learning rate of 0.0001 and 6 epochs with a batch size of 32. . %%time # Get data minerl.data.download(directory=&#39;data&#39;, environment=&#39;MineRLTreechop-v0&#39;) data = minerl.data.make(&quot;MineRLTreechop-v0&quot;, data_dir=&#39;data&#39;, num_workers=2) # Model model = CNN((3, 64, 64), 7).cuda() optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) criterion = nn.CrossEntropyLoss() # Training loop step = 0 losses = [] for state, action, _, _, _ in tqdm(data.batch_iter(num_epochs=6, batch_size=32, seq_len=1)): # Get pov observations obs = state[&#39;pov&#39;].squeeze().astype(np.float32) # Transpose and normalize obs = obs.transpose(0, 3, 1, 2) / 255.0 # Translate batch of actions for the ActionShaping wrapper actions = dataset_action_batch_to_actions(action) # Remove samples with no corresponding action mask = actions != -1 obs = obs[mask] actions = actions[mask] # Update weights with backprop logits = model(torch.from_numpy(obs).float().cuda()) loss = criterion(logits, torch.from_numpy(actions).long().cuda()) optimizer.zero_grad() loss.backward() optimizer.step() # Print loss step += 1 losses.append(loss.item()) if (step % 2000) == 0: mean_loss = sum(losses) / len(losses) tqdm.write(f&#39;Step {step:&gt;5} | Training loss = {mean_loss:.3f}&#39;) losses.clear() torch.save(model.state_dict(), &#39;model.pth&#39;) del data . Download: https://minerl.s3.amazonaws.com/v4/MineRLTreechop-v0.tar: 100%|██████████| 1511.0/1510.73792 [00:25&lt;00:00, 58.46MB/s] . Step 2000 | Training loss = 0.901 Step 4000 | Training loss = 0.878 Step 6000 | Training loss = 0.836 Step 8000 | Training loss = 0.826 Step 10000 | Training loss = 0.828 Step 12000 | Training loss = 0.805 Step 14000 | Training loss = 0.804 Step 16000 | Training loss = 0.773 Step 18000 | Training loss = 0.791 Step 20000 | Training loss = 0.789 Step 22000 | Training loss = 0.789 Step 24000 | Training loss = 0.816 Step 26000 | Training loss = 0.785 Step 28000 | Training loss = 0.769 Step 30000 | Training loss = 0.789 Step 32000 | Training loss = 0.777 Step 34000 | Training loss = 0.763 Step 36000 | Training loss = 0.738 Step 38000 | Training loss = 0.744 Step 40000 | Training loss = 0.751 Step 42000 | Training loss = 0.763 Step 44000 | Training loss = 0.764 Step 46000 | Training loss = 0.744 Step 48000 | Training loss = 0.732 Step 50000 | Training loss = 0.740 Step 52000 | Training loss = 0.748 Step 54000 | Training loss = 0.678 Step 56000 | Training loss = 0.765 Step 58000 | Training loss = 0.727 Step 60000 | Training loss = 0.735 Step 62000 | Training loss = 0.707 Step 64000 | Training loss = 0.716 Step 66000 | Training loss = 0.718 Step 68000 | Training loss = 0.710 Step 70000 | Training loss = 0.692 Step 72000 | Training loss = 0.693 Step 74000 | Training loss = 0.687 Step 76000 | Training loss = 0.695 CPU times: user 15min 21s, sys: 55.3 s, total: 16min 16s Wall time: 26min 46s . Our model is trained. We can now instantiate an environment and see how it behaves. If the training was successful, it should frantically cut all the trees in sight. . This time, we&#39;ll use the ActionShaping wrapper to map the array of numbers created with dataset_action_batch_to_actions to discrete actions in MineRL. . Our model needs a pov observation in the correct format and outputs logits. These logits can be turned into a probability distribution over a set of 7 actions with the softmax function. We then randomly choose an action based on the probabilities. The selected action is implemented in MineRL thanks to env.step(action). . This process is repeated as many times as we want. Let&#39;s do it 1000 times and watch the result. . model = CNN((3, 64, 64), 7).cuda() model.load_state_dict(torch.load(&#39;model.pth&#39;)) env = gym.make(&#39;MineRLObtainDiamond-v0&#39;) env1 = Recorder(env, &#39;./video&#39;, fps=60) env = ActionShaping(env1) action_list = np.arange(env.action_space.n) obs = env.reset() for step in tqdm(range(1000)): # Get input in the correct format obs = torch.from_numpy(obs[&#39;pov&#39;].transpose(2, 0, 1)[None].astype(np.float32) / 255).cuda() # Turn logits into probabilities probabilities = torch.softmax(model(obs), dim=1)[0].detach().cpu().numpy() # Sample action according to the probabilities action = np.random.choice(action_list, p=probabilities) obs, reward, _, _ = env.step(action) env1.release() env1.play() . . Our agent is quite chaotic but it manages to chop trees in this new, unseen environment. Now, how to find diamonds? . &#9935;&#65039; III. Script + Imitation Learning . A simple yet powerful approach consists of combining scripted actions with artificial intelligence. Learn the boring stuff, script the knowledge. . In this paradigm, we&#39;ll use the CNN to get a healthy amount of wood (3000 steps). Then, we can script a sequence to craft planks, sticks, a crafting table, a wooden pickaxe, and start mining stone (it should be below our feet). This stone can then be used to craft a stone pickaxe, which can mine iron ore. . This is when things get complicated: iron ore is quite rare, so we would need to run the game for a while to find a deposit. Then, we would have to craft a furnace and melt it to get the iron pickaxe. Finally, we would have to go even deeper and be even luckier to obtain a diamond without falling into lava. . As you can see, it&#39;s doable but the outcome is fairly random. We could train another agent to find diamonds, and even a third one to create the iron pickaxe. If you&#39;re interested in more complex approaches, you can read the results of the MineRL Diamond 2021 Competition by Kanervisto et al. It describes several solutions using different clever techniques, including end-to-end deep learning architectures. Nonetheless, it is a complex problem and no team managed to consistently find diamonds, if at all. . This is why we will limit ourselves to obtaining a stone pickaxe in the following example, but you can modify the code to go further. . script = [] script += [&#39;craft:planks&#39;] * 6 script += [&#39;craft:stick&#39;] * 2 script += [&#39;craft:crafting_table&#39;] * 2 script += [&#39;camera:[10,0]&#39;] * 18 script += [&#39;attack&#39;] * 20 script += [&#39;&#39;] * 10 script += [&#39;jump&#39;] script += [&#39;&#39;] * 5 script += [&#39;place:crafting_table&#39;] script += [&#39;&#39;] * 10 # Craft a wooden pickaxe and equip it script += [&#39;camera:[-1,0]&#39;] script += [&#39;nearbyCraft:wooden_pickaxe&#39;] script += [&#39;camera:[1,0]&#39;] script += [&#39;&#39;] * 10 script += [&#39;equip:wooden_pickaxe&#39;] script += [&#39;&#39;] * 10 # Dig stone script += [&#39;attack&#39;] * 500 # Craft stone pickaxe script += [&#39;&#39;] * 10 script += [&#39;jump&#39;] script += [&#39;&#39;] * 5 script += [&#39;place:crafting_table&#39;] script += [&#39;&#39;] * 10 script += [&#39;camera:[-1,0]&#39;] script += [&#39;nearbyCraft:stone_pickaxe&#39;] script += [&#39;camera:[1,0]&#39;] script += [&#39;&#39;] * 10 script += [&#39;equip:stone_pickaxe&#39;] script += [&#39;&#39;] * 10 . model = CNN((3, 64, 64), 7).cuda() model.load_state_dict(torch.load(&#39;model.pth&#39;)) env_script = gym.make(&#39;MineRLObtainDiamond-v0&#39;) env_cnn = Recorder(env_script, &#39;./video&#39;, fps=60) env_script = ActionShaping(env_cnn) action_list = np.arange(env_script.action_space.n) for _ in range(10): obs = env_script.reset() done = False # 1. Get wood with the CNN for i in tqdm(range(3000)): obs = torch.from_numpy(obs[&#39;pov&#39;].transpose(2, 0, 1)[None].astype(np.float32) / 255).cuda() probabilities = torch.softmax(model(obs), dim=1)[0].detach().cpu().numpy() action = np.random.choice(action_list, p=probabilities) obs, reward, done, _ = env_script.step(action) if done: break # 2. Craft stone pickaxe with scripted actions if not done: for action in tqdm(script): obs, reward, done, _ = env_cnn.step(str_to_act(env_cnn, action)) if done: break print(obs[&quot;inventory&quot;]) env_cnn.release() env_cnn.play() . . We can see our agent chopping wood like a madman during the first 3000 steps, then our script takes over and completes the task. It might not be obvious, but the command print(obs.inventory) shows a stone pickaxe. Note that this is a cherry-picked example: most of the runs don&#39;t end that well. . There are several reasons why the agent may fail: it can spawn in a hostile environment (water, lava, etc.), in an area without wood, or even fall and die. Playing with different seeds will give you a good understanding of the complexity of this problem and, hopefully, ideas to build event better agents. . Conclusion . I hope you enjoyed this little guide to reinforcement learning in Minecraft. Beyond its obvious popularity, Minecraft is an interesting environment to try and test RL agents. Like NetHack, it requires a thorough knowledge of its mechanics to plan precise sequences of actions in a procedurally-generated world. In this article, . We learned how to use MineRL; | We saw two approaches (script and behavior cloning) and how to combine them; | We visualized the agent&#39;s actions with short videos. | . The main drawback of the environment is its slow processing time. Minecraft is not a lightweight game like NetHack or Pong, which is why the agents take a long time to be trained. If this is a problem for you, I would recommend lighter environments like Gym Retro. . Thank you for your attention! Feel free to follow me on Twitter if you&#39;re interested in AI applied to video games. .",
            "url": "https://mlabonne.github.io/blog/minecraft/",
            "relUrl": "/minecraft/",
            "date": " • May 25, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Introduction to Constraint Programming in Python",
            "content": "Constraint Programming is a technique to find every solution that respects a set of predefined constraints. . It is an invaluable tool for data scientists to solve a huge variety of problems, such as scheduling, timetabling, sequencing, etc. In this article, we&#39;ll see how to use CP in two different ways: . Satisfiability: the goal is to find one or multiple feasible solutions (i.e., solutions that respect our constraints) by narrowing down a large set of potential solutions; | Optimization: the goal is to find the best feasible solution according to an objective function, just like Linear Programming (LP). | We&#39;ll use CP-SAT from Google OR-Tools, an excellent free and open source CP solver. Note that it is different from MPSolver, which is dedicated to Linear and Mixed Integer Programming. The difference between CP and LP is quite confusing, we&#39;ll touch on this topic at the end of the article. . You can run the code with the following Google Colab notebook. . !python -m pip install --upgrade --user -q ortools . &#129686; I. Satisfiability with the 3 scouts problem . In the previous article, we created an army to defeat our opponent. But there was one small problem: we had to guess how powerful his army was. . This time, let&#39;s send scouts to know the exact number. Our 3 scouts observed the enemy camp, and this is what they tell us: . Scout 1: &quot;the number of soldiers is a multiple of 13&quot;; | Scout 2: &quot;the number of soldiers is a multiple of 19&quot;; | Scout 3: &quot;the number of soldiers is a multiple of 37&quot;; | They all agree that the number of soldiers doesn&#39;t exceed 10,000. | . Our scouts have a personal way of counting soldiers, but we can combine these three observations to make a model. . Let&#39;s call the number of soldiers $army$. We can translate our problem into the following congruence system: . $$ army equiv 0 mod 13 army equiv 0 mod 19 army equiv 0 mod 37 $$If you&#39;re not familiar with this notation, this is what it means in programming terms: . $$ army % 13 = 0 army % 19 = 0 army % 37 = 0 $$Let&#39;s implement it with OR-Tools. The first thing we need to do is to import and create the CP-SAT model and solver. . from ortools.sat.python import cp_model # Instantiate model and solver model = cp_model.CpModel() solver = cp_model.CpSolver() . The modeling process is very similar to what we did in Linear Programming. . The first step to create our CP model is to declare the variables. In this example, we only have one: $army$, the number of soldiers. . We have to give lower and upper bounds. The lower bound is 1 since we know there&#39;s an army, and the upper bound is 10,000 according to the scouts: . $$1 leq army leq 10 000$$ . In OR-Tools, we use the NewIntVar method to create this variable. . army = model.NewIntVar(1, 10000, &#39;army&#39;) . The second step is to declare the constraints. . We identified three constraints in this example. Modulo is a special operator, so we need a specific function to handle it with CP-SAT: AddModuloEquality. You can find a reference guide at this address if you need other methods. . # variable % mod = target → (target, variable, mod) model.AddModuloEquality(0, army, 13) model.AddModuloEquality(0, army, 19) model.AddModuloEquality(0, army, 37) . Unlike Linear Programming, we don&#39;t have to define an objective function here. . The reason is simple: there is nothing to optimize! We just want to find a feasible solution that satisfies our constraints, but there is no &quot;good&quot; or &quot;bad&quot; answers. This is a key feature of Constraint Programming. . Our model is complete, we can now ask OR-Tools to solve it. . status = solver.Solve(model) # If a solution has been found, print results if status == cp_model.OPTIMAL or status == cp_model.FEASIBLE: print(&#39;================= Solution =================&#39;) print(f&#39;Solved in {solver.WallTime():.2f} milliseconds&#39;) print() print(f&#39;🪖 Army = {solver.Value(army)}&#39;) print() print(&#39;Check solution:&#39;) print(f&#39; - Constraint 1: {solver.Value(army)} % 13 = {solver.Value(army) % 13}&#39;) print(f&#39; - Constraint 2: {solver.Value(army)} % 19 = {solver.Value(army) % 19}&#39;) print(f&#39; - Constraint 3: {solver.Value(army)} % 37 = {solver.Value(army) % 37}&#39;) else: print(&#39;The solver could not find a solution.&#39;) . ================= Solution ================= Solved in 0.01 milliseconds 🪖 Army = 9139 Check solution: - Constraint 1: 9139 % 13 = 0 - Constraint 2: 9139 % 19 = 0 - Constraint 3: 9139 % 37 = 0 . We obtained our solution in less than a millisecond: there are 9,139 soldiers in the enemy army. Huzzah, we can now fire the scouts! . We limited the search space with an upper bound of 10,000, which gave us a unique solution. But is it still the case if we push this limit? . Another perk of CP is the ability to find every possible solution to a problem. This might take a long time when the search space is large because the solver has to brute force the entire space (instead of reducing it with heuristics). Let&#39;s explore this feature by printing every possible solution with a new upper bound of 100,000. . With OR-Tools, we ask the solver to look for every possible solution thanks to the enumerate_all_solutions parameter. We then assign it a callback class that prints every solution the solver finds. . model = cp_model.CpModel() solver = cp_model.CpSolver() # 1. Variable army = model.NewIntVar(1, 100000, &#39;army&#39;) # 2. Constraints model.AddModuloEquality(0, army, 13) model.AddModuloEquality(0, army, 19) model.AddModuloEquality(0, army, 37) class PrintSolutions(cp_model.CpSolverSolutionCallback): &quot;&quot;&quot;Callback to print every solution.&quot;&quot;&quot; def __init__(self, variable): cp_model.CpSolverSolutionCallback.__init__(self) self.__variable = variable def on_solution_callback(self): print(self.Value(self.__variable)) # Solve with callback solution_printer = PrintSolutions(army) solver.parameters.enumerate_all_solutions = True status = solver.Solve(model, solution_printer) . 9139 18278 27417 36556 45695 54834 63973 73112 82251 91390 . We found 10 solutions! This was to be expected since we increased the upper bound tenfold: these solutions all are multiples of 9,139. . As you can see, this example has nothing to do with optimization: it&#39;s a pure satisfiability problem. On another note, this congruence system can be solved manually with the Chinese remainder theorem. But CP is not limited to that... . &#127867; II. Optimization and beer . Let&#39;s see another problem: our army will face the enemy in a few days. In the meantime, the quartermaster has to prepare the rations that will be used during the campaign. . The space in the supply wagons is limited and some rations are more popular than others. There are three possible rations: . 🥖 Bread: it takes only 1 space but soldiers don&#39;t like it that much with a popularity of 3; | 🥩 Meat: it takes 3 spaces and has a popularity of 10; | 🍺 Beer: it takes 7 spaces but soldiers love it with a popularity of 26. | . The supply wagons have a capacity of 19 spaces. How to select the best rations to maximize the popularity? . This is an optimization problem we&#39;ve already seen: actually, it is a variant of the famous knapsack problem. We could reuse the code from the previous article and just change the input parameters. . This time, we&#39;ll solve it using Constraint Programming. This paradigm is not limited to finding feasible solutions. It can also perform optimization using different algorithms to handle this overhead. . Let&#39;s create a model of the problem. First of all, we have to declare three variables: 🥖bread, 🥩meat, and 🍺beer. It&#39;s possible to have 0 of them, but their number cannot exceed the maximal capacity. . $$0 leq bread leq capacity 0 leq meat leq capacity 0 leq beer leq capacity$$ model = cp_model.CpModel() solver = cp_model.CpSolver() # 1. Variables capacity = 19 bread = model.NewIntVar(0, capacity, &#39;bread&#39;) meat = model.NewIntVar(0, capacity, &#39;meat&#39;) beer = model.NewIntVar(0, capacity, &#39;beer&#39;) . This time, we only have one constraint: the space occupied by the bread, the meat, and the beer cannot exceed the wagons&#39; capacity (19). . $$1 times bread + 3 times meat + 7 times beer leq 19$$ . model.Add(1 * bread + 3 * meat + 7 * beer &lt;= capacity) . We want to maximize the total popularity of the rations that are selected: . $$max 3 times bread + 10 times meat + 26 times beer$$ . model.Maximize(3 * bread + 10 * meat + 26 * beer) . The model is complete, CP-SAT can solve the problem! . status = solver.Solve(model) # If an optimal solution has been found, print results if status == cp_model.OPTIMAL: print(&#39;================= Solution =================&#39;) print(f&#39;Solved in {solver.WallTime():.2f} milliseconds&#39;) print() print(f&#39;Optimal value = {3*solver.Value(bread)+10*solver.Value(meat)+26*solver.Value(beer)} popularity&#39;) print(&#39;Food:&#39;) print(f&#39; - 🥖Bread = {solver.Value(bread)}&#39;) print(f&#39; - 🥩Meat = {solver.Value(meat)}&#39;) print(f&#39; - 🍺Beer = {solver.Value(beer)}&#39;) else: print(&#39;The solver could not find an optimal solution.&#39;) . ================= Solution ================= Solved in 0.01 milliseconds Optimal value = 68 popularity Food: - 🥖Bread = 2 - 🥩Meat = 1 - 🍺Beer = 2 . We obtained the highest popularity (68) possible with a capacity of 19. . Is the constraint respected? Let&#39;s quickly check it: 1×2🥖 + 3×1🥩 + 7×2🍺 = 19, which is indeed ≤ 19. . Okay, I&#39;d like to ask another question: how many solutions to this problem are there? Once again, we can answer it with a specific callback to count them. . class CountSolutions(cp_model.CpSolverSolutionCallback): &quot;&quot;&quot;Count the number of solutions.&quot;&quot;&quot; def __init__(self): cp_model.CpSolverSolutionCallback.__init__(self) self.__solution_count = 0 def on_solution_callback(self): self.__solution_count += 1 def solution_count(self): return self.__solution_count solution_printer = CountSolutions() # Instantiate model and solver model = cp_model.CpModel() solver = cp_model.CpSolver() # 1. Variables capacity = 19 bread = model.NewIntVar(0, capacity, &#39;Bread&#39;) meat = model.NewIntVar(0, capacity, &#39;Meat&#39;) beer = model.NewIntVar(0, capacity, &#39;Beer&#39;) # 2. Constraints model.Add(1 * bread + 3 * meat + 7 * beer &lt;= capacity) # Print results solver.parameters.enumerate_all_solutions = True status = solver.Solve(model, solution_printer) print(solution_printer.solution_count()) . 121 . We found 121 solutions with a capacity of 19. But this number quickly increases: with a capacity of 1000, there are 8,080,104 possible solutions! And yet, CP-SAT finds the optimal solution in less than a second. How is it possible? . CP solvers do not brute force the problem with an exhaustive search, but combine heuristics and combinatorial search instead. More specifically, the three most popular techniques for constraint satisfaction problems are backtracking, constraint propagation, and local search). . CP-SAT is quite particular since it combines CP and SAT: it is part of a broader trend of merging CP, LP, SAT, and metaheuristics. . We said that the previous problem could be solved with Linear Programming, so let&#39;s compare the code of both solutions: . As you can see, the syntax is quite similar but it&#39;s not the same: model/solver vs. solver, NewIntVar instead of IntVar, etc. There&#39;s a bit of translation to do, but it&#39;s easily manageable. . These two techniques are incredibly close to each other: they both handle variables with constraints and perform optimization using math and heuristics. However, CP is limited to discrete parameters, while LP handles continuous ones. On the other hand, you can implement specialized constraints like &quot;all different&quot; in CP, but not in LP. Here is a summary of the main differences between these two technologies: . If you want to know more about this topic, I would recommend this article by Irvin J. Lustig and Jean-François Puget. CPLEX&#39;s documentation also details the differences at this address, in terms of modeling and optimization. . Conclusion . Constraint Programming is another incredible technique in the mathematical optimization toolbox. It is a radically different approach compared to traditional, declarative programming. In this article, . We saw two applications of CP with satisfiability and optimization; | We implemented CP models in OR-Tools and played with the callback function; | We highlighted the differences between CP and LP. | . We limited ourselves to simple problems in this introduction, but CP has amazing applications in complex scheduling and routing problems. This is a topic I&#39;d love to address in a future article. . If you&#39;re interested to know more about it, feel free to follow me on Twitter @maximelabonne. Thanks for your attention! . &#129351; Linear Programming Course . 🔎 Course overview . 📝 Chapter 1: Introduction to Linear Programming . 📝 Chapter 2: Integer vs. Linear Programming . 📝 Chapter 3: Constraint Programming .",
            "url": "https://mlabonne.github.io/blog/constraintprogramming/",
            "relUrl": "/constraintprogramming/",
            "date": " • May 2, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "GIN: How to Design the Most Powerful Graph Neural Network",
            "content": "Graph Neural Networks are not limited to classifying nodes. . One of the most popular applications is graph classification. This is a common task when dealing with molecules: they are represented as graphs and features about each atom (node) can be used to predict the behavior of the entire molecule. . However, GNNs only learn node embeddings. How to combine them in order to produce an entire graph embedding? In this article, we will: . See a new type of layer, called &quot;global pooling&quot;, to combine node embeddings; . | Introduce a new architecture called Graph Isomorphism Network (GIN), designed by Xu et al. in 2018. . | . We&#39;ll detail the advantages of GIN in terms of discriminative power compared to a GCN or GraphSAGE, and its connection to the Weisfeiler-Lehman test. Beyond its powerful aggregator, GIN brings exciting takeaways about GNNs in general. . You can run the code with the following Google Colab notebook. . import torch !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git # Visualization import networkx as nx import matplotlib.pyplot as plt plt.rcParams[&#39;figure.dpi&#39;] = 300 plt.rcParams.update({&#39;font.size&#39;: 24}) . &#127760; I. PROTEINS dataset . 3D plot of a protein by DeepMind.PROTEINS is a popular dataset in bioinformatics. It is a collection of 1113 graphs representing proteins, where nodes are amino acids. Two nodes are connected by an edge when they are close enough (&lt; 0.6 nanometers). The goal is to classify each protein as an enzyme or not. . Enzymes are a particular type of proteins that act as catalysts to speed up chemical reactions in the cell. They are essential for digestion (e.g., lipases), respiration (e.g., oxidases), and other crucial functions of the human body. They are also used in commercial applications, like the production of antibiotics. . This dataset is also available on TUDataset and implemented in PyTorch Geometric. . from torch_geometric.datasets import TUDataset dataset = TUDataset(root=&#39;.&#39;, name=&#39;PROTEINS&#39;).shuffle() # Print information about the dataset print(f&#39;Dataset: {dataset}&#39;) print(&#39;-&#39;) print(f&#39;Number of graphs: {len(dataset)}&#39;) print(f&#39;Number of nodes: {dataset[0].x.shape[0]}&#39;) print(f&#39;Number of features: {dataset.num_features}&#39;) print(f&#39;Number of classes: {dataset.num_classes}&#39;) . Dataset: PROTEINS(1113) - Number of graphs: 1113 Number of nodes: 117 Number of features: 3 Number of classes: 2 . I&#39;m not a biochemist so I&#39;m curious about these proteins. Let&#39;s plot one as a graph to see what it looks like. . from torch_geometric.utils import to_networkx from mpl_toolkits.mplot3d import Axes3D import numpy as np G = to_networkx(dataset[2], to_undirected=True) # 3D spring layout pos = nx.spring_layout(G, dim=3, seed=0) # Extract node and edge positions from the layout node_xyz = np.array([pos[v] for v in sorted(G)]) edge_xyz = np.array([(pos[u], pos[v]) for u, v in G.edges()]) # Create the 3D figure fig = plt.figure(figsize=(16,16)) ax = fig.add_subplot(111, projection=&quot;3d&quot;) # Suppress tick labels for dim in (ax.xaxis, ax.yaxis, ax.zaxis): dim.set_ticks([]) # Plot the nodes - alpha is scaled by &quot;depth&quot; automatically ax.scatter(*node_xyz.T, s=500, c=&quot;#0A047A&quot;) # Plot the edges for vizedge in edge_xyz: ax.plot(*vizedge.T, color=&quot;tab:gray&quot;) # fig.tight_layout() plt.show() . The previous 3D structure is randomly generated: obtaining the correct 3D representation is a problem so difficult it&#39;s the whole point of AlphaFold. . Graphs are not the only way to represent molecules. The simplified molecular-input line-entry system (SMILES) is another popular method, which uses a line (string) notation. It is obtained by printing the nodes encountered in a depth-first tree traversal of a slightly modified molecular graph. . Researchers often use this representation when working with molecules or chemical compounds. Fortunately for us, the PROTEINS dataset is already encoded in the form of graphs. Otherwise, we could have to translate the SMILES strings into networkx graphs. . It doesn&#39;t mean we&#39;ll directly feed the PROTEINS dataset to our GNN. If GraphSAGE taught us anything, it&#39;s that mini-batching is incredibly efficient. It is now an indispensable tool whenever we implement a GNN. . from torch_geometric.loader import DataLoader # Create training, validation, and test sets train_dataset = dataset[:int(len(dataset)*0.8)] val_dataset = dataset[int(len(dataset)*0.8):int(len(dataset)*0.9)] test_dataset = dataset[int(len(dataset)*0.9):] print(f&#39;Training set = {len(train_dataset)} graphs&#39;) print(f&#39;Validation set = {len(val_dataset)} graphs&#39;) print(f&#39;Test set = {len(test_dataset)} graphs&#39;) # Create mini-batches train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True) test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False) print(&#39; nTrain loader:&#39;) for i, subgraph in enumerate(train_loader): print(f&#39; - Subgraph {i}: {subgraph}&#39;) print(&#39; nValidation loader:&#39;) for i, subgraph in enumerate(val_loader): print(f&#39; - Subgraph {i}: {subgraph}&#39;) print(&#39; nTest loader:&#39;) for i, subgraph in enumerate(test_loader): print(f&#39; - Subgraph {i}: {subgraph}&#39;) . Training set = 890 graphs Validation set = 111 graphs Test set = 112 graphs Train loader: - Subgraph 0: DataBatch(edge_index=[2, 7966], x=[2114, 3], y=[64], batch=[2114], ptr=[65]) - Subgraph 1: DataBatch(edge_index=[2, 8492], x=[2263, 3], y=[64], batch=[2263], ptr=[65]) - Subgraph 2: DataBatch(edge_index=[2, 9518], x=[2589, 3], y=[64], batch=[2589], ptr=[65]) - Subgraph 3: DataBatch(edge_index=[2, 10846], x=[3008, 3], y=[64], batch=[3008], ptr=[65]) - Subgraph 4: DataBatch(edge_index=[2, 9618], x=[2586, 3], y=[64], batch=[2586], ptr=[65]) - Subgraph 5: DataBatch(edge_index=[2, 7572], x=[2027, 3], y=[64], batch=[2027], ptr=[65]) - Subgraph 6: DataBatch(edge_index=[2, 10512], x=[2875, 3], y=[64], batch=[2875], ptr=[65]) - Subgraph 7: DataBatch(edge_index=[2, 7034], x=[1855, 3], y=[64], batch=[1855], ptr=[65]) - Subgraph 8: DataBatch(edge_index=[2, 11966], x=[3313, 3], y=[64], batch=[3313], ptr=[65]) - Subgraph 9: DataBatch(edge_index=[2, 9898], x=[2764, 3], y=[64], batch=[2764], ptr=[65]) - Subgraph 10: DataBatch(edge_index=[2, 8798], x=[2411, 3], y=[64], batch=[2411], ptr=[65]) - Subgraph 11: DataBatch(edge_index=[2, 9922], x=[2736, 3], y=[64], batch=[2736], ptr=[65]) - Subgraph 12: DataBatch(edge_index=[2, 10772], x=[2787, 3], y=[64], batch=[2787], ptr=[65]) - Subgraph 13: DataBatch(edge_index=[2, 11140], x=[2782, 3], y=[58], batch=[2782], ptr=[59]) Validation loader: - Subgraph 0: DataBatch(edge_index=[2, 8240], x=[2088, 3], y=[64], batch=[2088], ptr=[65]) - Subgraph 1: DataBatch(edge_index=[2, 5626], x=[1503, 3], y=[47], batch=[1503], ptr=[48]) Test loader: - Subgraph 0: DataBatch(edge_index=[2, 7946], x=[2156, 3], y=[64], batch=[2156], ptr=[65]) - Subgraph 1: DataBatch(edge_index=[2, 6222], x=[1614, 3], y=[48], batch=[1614], ptr=[49]) . PROTEINS is not a huge dataset, but mini-batching will speed up the training nonetheless. We could use a GCN or a GAT, but there&#39;s a new architecture I&#39;d like to introduce: the Graph Isomorphism Network. . &#127870; II. Graph Isomorphism Network (GIN) . GIN was designed by researchers trying to maximize the representational (or discriminative) power of a GNN. But how do you define a &quot;representational power&quot;? . A. Weisfeiler-Lehman test . A way to characterize the &quot;power&quot; of a GNN is to use the Weisfeiler-Lehman (WL) graph isomorphism test. Isomorphic graphs mean that they have the same structure: identical connections but a permutation of nodes. The WL test is able to tell if two graphs are non-isomorphic, but it cannot guarantee that they are isomorphic. . Two isomorphic graphs.This might not seem like much, but it can be extremely difficult to tell two large graphs apart. In fact, this problem is not known to be solvable in polynomial time, nor to be NP-complete. It might even be somewhere in between, in the computational complexity class NP-intermediate (if it only exists). . Okay, but how is it related to GNNs? Some researchers in graph learning noticed that this test and the way GNNs learn are oddly similar. In the WL test, . Every node starts with the same label; | Labels from neighboring nodes are aggregated and hashed to produce a new label; | The previous step is repeated until the labels stop changing. | If you&#39;re interested in the WL test, I would recommend this blog post by David Bieber and this article by Michael Bronstein. . Not only this test is similar to how feature vectors are aggregated in GNNs, but its ability to tell graphs apart makes it more powerful than a lot of architectures, including GCNs and GraphSAGE. This is what inspired Xu et al. to design a new aggregator that is proven to be as good as the WL test. . B. One aggregator to rule them all . To be as good as the WL test, this new aggregator must produce different node embeddings when dealing with non-isomorphic graphs. . We&#39;ll skip the math-heavy part of the paper, but the solution they found is to use two injective functions. Which ones? We don&#39;t know, we can just learn them with a MLP! . With GATs, we used a neural network to learn the best weighting factors for a given task; | With GINs, we now learn the approximation of two injective functions thanks to the Universal Approximation Theorem. | . Here&#39;s how to calculate the hidden vector of a particular node $i$ with GIN: . $$h_i = MLP bigg((1+ɛ) cdot x_i + sum_{j in mathcal{N}_i}x_j bigg)$$ . In this formula, $ɛ$ determines the importance of the target node compared to its neighbors (it has the same importance if $ɛ = 0$). It can be a learnable parameter or a fixed scalar. . Note that we talk about MLPs to highlight the fact there is more than one layer. According to the authors, one layer is not sufficient for graph learning in general. . C. Global pooling . Global pooling or graph-level readout consists of producing a graph embedding using the node embeddings calculated by the GNN. . A simple way to obtain a graph embedding $h_G$ is to use the mean, sum, or max of every node embedding $h_i$: . $${Mean}: h_G = frac{1}{N} sum_{i=0}^N h_i {Sum}: h_G = sum_{i=0}^N h_i {Max}: h_G = {max}_{i=0}^N(h_i)$$The authors make two important points about graph-level readout: . To consider all structural information, it is necessary to keep embeddings from previous layers; | The sum operator is surprisingly more expressive than the mean and the max. | . These observations lead them to propose the following global pooling method: . $$h_G = sum_{i=0}^N{h_i^0} || dots || sum_{i=0}^N{h_i^k}$$ . For each layer, embeddings nodes are summed and the result is concatenated. This solution combines the expressiveness of the sum operator with the memory of previous iterations from the concatenation. . &#129504; III. GIN in PyTorch Geometric . It is always interesting to see the differences between the original design and its implementations. . There is a GINConv layer in PyTorch Geometric with different parameters: . nn: the MLP that is used to approximate our two injective functions; | eps: the initial value of $ɛ$, which is 0 by default; | train_eps: a True/False statement to determine if $ɛ$ is trainable, which is False by default. | . You can see that $ɛ$ is entirely removed by default in this implementation: it&#39;s a hyperparameter we can tune, but probably not an essential one. . There is a second GIN layer in PyTorch Geometric, called GINEConv. It comes from this paper&#39;s implementation of GIN, which applies a $ReLU$ function to the neighbors&#39; features. We won&#39;t use it in this tutorial, since the benefits are not clear. . We still need to design a MLP for the GINConv layer. Here&#39;s the design we&#39;ll implement, inspired by the original paper: . The paper stacks 5 layers but we&#39;ll be more humble with 3 layers instead. Here is what the entire architecture looks like: . I could not find any implementation of GIN with graph embedding concatenation, so here is my version (it improves the accuracy by 1% on average). Let&#39;s compare it to a GCN with a simple mean pooling (and no concatenation). . import torch import torch.nn.functional as F from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout from torch_geometric.nn import GCNConv, GINConv from torch_geometric.nn import global_mean_pool, global_add_pool class GCN(torch.nn.Module): &quot;&quot;&quot;GCN&quot;&quot;&quot; def __init__(self, dim_h): super(GCN, self).__init__() self.conv1 = GCNConv(dataset.num_node_features, dim_h) self.conv2 = GCNConv(dim_h, dim_h) self.conv3 = GCNConv(dim_h, dim_h) self.lin = Linear(dim_h, dataset.num_classes) def forward(self, x, edge_index, batch): # Node embeddings h = self.conv1(x, edge_index) h = h.relu() h = self.conv2(h, edge_index) h = h.relu() h = self.conv3(h, edge_index) # Graph-level readout hG = global_mean_pool(h, batch) # Classifier h = F.dropout(hG, p=0.5, training=self.training) h = self.lin(h) return hG, F.log_softmax(h, dim=1) class GIN(torch.nn.Module): &quot;&quot;&quot;GIN&quot;&quot;&quot; def __init__(self, dim_h): super(GIN, self).__init__() self.conv1 = GINConv( Sequential(Linear(dataset.num_node_features, dim_h), BatchNorm1d(dim_h), ReLU(), Linear(dim_h, dim_h), ReLU())) self.conv2 = GINConv( Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(), Linear(dim_h, dim_h), ReLU())) self.conv3 = GINConv( Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(), Linear(dim_h, dim_h), ReLU())) self.lin1 = Linear(dim_h*3, dim_h*3) self.lin2 = Linear(dim_h*3, dataset.num_classes) def forward(self, x, edge_index, batch): # Node embeddings h1 = self.conv1(x, edge_index) h2 = self.conv2(h1, edge_index) h3 = self.conv3(h2, edge_index) # Graph-level readout h1 = global_add_pool(h1, batch) h2 = global_add_pool(h2, batch) h3 = global_add_pool(h3, batch) # Concatenate graph embeddings h = torch.cat((h1, h2, h3), dim=1) # Classifier h = self.lin1(h) h = h.relu() h = F.dropout(h, p=0.5, training=self.training) h = self.lin2(h) return h, F.log_softmax(h, dim=1) gcn = GCN(dim_h=32) gin = GIN(dim_h=32) . def train(model, loader): criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01) epochs = 100 model.train() for epoch in range(epochs+1): total_loss = 0 acc = 0 val_loss = 0 val_acc = 0 # Train on batches for data in loader: optimizer.zero_grad() _, out = model(data.x, data.edge_index, data.batch) loss = criterion(out, data.y) total_loss += loss / len(loader) acc += accuracy(out.argmax(dim=1), data.y) / len(loader) loss.backward() optimizer.step() # Validation val_loss, val_acc = test(model, val_loader) # Print metrics every 10 epochs if(epoch % 10 == 0): print(f&#39;Epoch {epoch:&gt;3} | Train Loss: {total_loss:.2f} &#39; f&#39;| Train Acc: {acc*100:&gt;5.2f}% &#39; f&#39;| Val Loss: {val_loss:.2f} &#39; f&#39;| Val Acc: {val_acc*100:.2f}%&#39;) test_loss, test_acc = test(model, test_loader) print(f&#39;Test Loss: {test_loss:.2f} | Test Acc: {test_acc*100:.2f}%&#39;) return model @torch.no_grad() def test(model, loader): criterion = torch.nn.CrossEntropyLoss() model.eval() loss = 0 acc = 0 for data in loader: _, out = model(data.x, data.edge_index, data.batch) loss += criterion(out, data.y) / len(loader) acc += accuracy(out.argmax(dim=1), data.y) / len(loader) return loss, acc def accuracy(pred_y, y): &quot;&quot;&quot;Calculate accuracy.&quot;&quot;&quot; return ((pred_y == y).sum() / len(y)).item() gcn = train(gcn, train_loader) gin = train(gin, train_loader) . Epoch 100 | Train Loss: 0.67 | Train Acc: 60.61% | Val Loss: 0.70 | Val Acc: 54.50% Test Loss: 0.69 | Test Acc: 55.99% Epoch 100 | Train Loss: 0.49 | Train Acc: 75.61% | Val Loss: 0.53 | Val Acc: 78.99% Test Loss: 0.60 | Test Acc: 66.93% . This time, there&#39;s no competition! . The GIN architecture completely outperforms the GCN. This gap (10% accuracy on average) is due to several reasons: . GIN&#39;s aggregator is specifically designed to discriminate graphs that the GCN&#39;s aggregator cannot; | Graph hidden vectors from every layer are concatenated instead of only considering the last one; | The sum operator is superior to the mean operator (at least in theory). | . Let&#39;s visualize the proteins we classified with the GCN and the GIN. . fig, ax = plt.subplots(4, 4, figsize=(16,16)) fig.suptitle(&#39;GCN - Graph classification&#39;) for i, data in enumerate(dataset[1113-16:]): # Calculate color (green if correct, red otherwise) _, out = gcn(data.x, data.edge_index, data.batch) color = &quot;green&quot; if out.argmax(dim=1) == data.y else &quot;red&quot; # Plot graph ix = np.unravel_index(i, ax.shape) ax[ix].axis(&#39;off&#39;) G = to_networkx(dataset[i], to_undirected=True) nx.draw_networkx(G, pos=nx.spring_layout(G, seed=0), with_labels=False, node_size=150, node_color=color, width=0.8, ax=ax[ix] ) . fig, ax = plt.subplots(4, 4, figsize=(16,16)) fig.suptitle(&#39;GIN - Graph classification&#39;) for i, data in enumerate(dataset[1113-16:]): # Calculate color (green if correct, red otherwise) _, out = gin(data.x, data.edge_index, data.batch) color = &quot;green&quot; if out.argmax(dim=1) == data.y else &quot;red&quot; # Plot graph ix = np.unravel_index(i, ax.shape) ax[ix].axis(&#39;off&#39;) G = to_networkx(dataset[i], to_undirected=True) nx.draw_networkx(G, pos=nx.spring_layout(G, seed=0), with_labels=False, node_size=150, node_color=color, width=0.8, ax=ax[ix] ) . Interestingly enough, the two models make different mistakes. This is a common result in machine learning when different algorithms are applied to the same problem. . We can take advantage of this behavior by creating an ensemble. There are many ways of combining our graph embeddings. The simplest method is to take the mean of the normalized output vectors. . gcn.eval() gin.eval() acc_gcn = 0 acc_gin = 0 acc = 0 for data in test_loader: # Get classifications _, out_gcn = gcn(data.x, data.edge_index, data.batch) _, out_gin = gin(data.x, data.edge_index, data.batch) out = (out_gcn + out_gin)/2 # Calculate accuracy scores acc_gcn += accuracy(out_gcn.argmax(dim=1), data.y) / len(test_loader) acc_gin += accuracy(out_gin.argmax(dim=1), data.y) / len(test_loader) acc += accuracy(out.argmax(dim=1), data.y) / len(test_loader) # Print results print(f&#39;GCN accuracy: {acc_gcn*100:.2f}%&#39;) print(f&#39;GIN accuracy: {acc_gin*100:.2f}%&#39;) print(f&#39;GCN+GIN accuracy: {acc*100:.2f}%&#39;) . GCN accuracy: 55.99% GIN accuracy: 66.93% GCN+GIN accuracy: 67.45% . This time, we&#39;re lucky enough to see the accuracy improved. . Obviously, it&#39;s not always the case. More sophisticated methods involve building an entirely different ML algorithm for classification, such as a Random Forest. This classifier takes graph embeddings as inputs and outputs the final classification. . Conclusion . Graph Isomorphism Networks are an important step in the understanding of GNNs. . They not only improve the accuracy scores on several benchmarks but also provide a theoretical framework to explain why one architecture is better than another. In this article, . We saw a new task with graph classification, performed with global pooling; | We introduced the WL test and its connection with the new GIN layer; | We implemented a GIN and a GCN and made an simple ensemble with their classifications. | . Although GINs achieve good performance, especially with social graphs, their theoretical superiority doesn&#39;t always translate well in the real world. It is true with other &quot;provably powerful&quot; architectures, which tend to underperform in practice, such as the 3WLGNN. . If you enjoyed this article, please leave a few claps and follow me on Twitter for more graph content! 📣 . &#127760; Graph Neural Network Course . 🔎 Course overview . 📝 Chapter 1: Introduction to Graph Neural Networks . 📝 Chapter 2: Graph Attention Network . 📝 Chapter 3: GraphSAGE . 📝 Chapter 4: Graph Isomorphism Network .",
            "url": "https://mlabonne.github.io/blog/gin/",
            "relUrl": "/gin/",
            "date": " • Apr 25, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "GraphSAGE: Scaling up Graph Neural Networks",
            "content": "What do UberEats and Pinterest have in common? . They both use GraphSAGE to power their recommender systems on a massive scale: millions and billions of nodes and edges. . 🖼️ Pinterest developed its own version called PinSAGE to recommend the most relevant images (pins) to its users. Their graph has 18 billion connections and three billion nodes. | 🍽️ UberEats also reported using a modified version of GraphSAGE to suggest dishes, restaurants, and cuisines. UberEats claims to support more than 600,000 restaurants and 66 million users. Meanwhile, it keeps recommending me tacos that gave me food poisoning. | . In this tutorial, we&#39;ll use a dataset with 20k nodes instead of billions because Google Colab cannot handle our ambitions. We will stick to the original GraphSAGE architecture, but the previous variants also bring exciting features we will discuss. . You can run the code with the following Google Colab notebook. . import torch !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git # Visualization import networkx as nx import matplotlib.pyplot as plt plt.rcParams[&#39;figure.dpi&#39;] = 300 plt.rcParams.update({&#39;font.size&#39;: 24}) . &#127760; I. PubMed dataset . As we saw in the previous article, PubMed is part of the Planetoid dataset (MIT license). Here&#39;s a quick summary: . It contains 19,717 scientific publications about diabetes from PubMed&#39;s database | Node features are TF-IDF weighted word vectors with 500 dimensions, which is an efficient way of summarizing documents without transformers | The task is quite straightforward since it&#39;s a multi-class classification with three categories: diabetes mellitus experimental, diabetes mellitus type 1, and diabetes mellitus type 2 | . Let&#39;s load the dataset and print some information about the graph. . from torch_geometric.datasets import Planetoid dataset = Planetoid(root=&#39;.&#39;, name=&quot;Pubmed&quot;) data = dataset[0] # Print information about the dataset print(f&#39;Dataset: {dataset}&#39;) print(&#39;-&#39;) print(f&#39;Number of graphs: {len(dataset)}&#39;) print(f&#39;Number of nodes: {data.x.shape[0]}&#39;) print(f&#39;Number of features: {dataset.num_features}&#39;) print(f&#39;Number of classes: {dataset.num_classes}&#39;) # Print information about the graph print(f&#39; nGraph:&#39;) print(&#39;&#39;) print(f&#39;Training nodes: {sum(data.train_mask).item()}&#39;) print(f&#39;Evaluation nodes: {sum(data.val_mask).item()}&#39;) print(f&#39;Test nodes: {sum(data.test_mask).item()}&#39;) print(f&#39;Edges are directed: {data.is_directed()}&#39;) print(f&#39;Graph has isolated nodes: {data.has_isolated_nodes()}&#39;) print(f&#39;Graph has loops: {data.has_self_loops()}&#39;) . Dataset: Pubmed() - Number of graphs: 1 Number of nodes: 19717 Number of features: 500 Number of classes: 3 Graph: Training nodes: 60 Evaluation nodes: 500 Test nodes: 1000 Edges are directed: False Graph has isolated nodes: False Graph has loops: False . As we can see, PubMed has an insanely low number of training nodes compared to the whole graph. There are only 60 samples to learn how to classify the 1000 test nodes. . Despite this challenge, GNNs manage to obtain high levels of accuracy. Here&#39;s the leaderboard of known techniques (a more exhaustive benchmark can be found on PapersWithCode): . Model 📝PubMed (accuracy) . Multilayer Perceptron | 71.4% | . Graph Convolutional Network | 79.0% ± 0.3% | . Graph Attention Network | 79.0% ± 0.3% | . GraphSAGE | ??? | . I couldn&#39;t find any result for GraphSAGE on PubMed with this specific setting (60 training nodes, 1000 test nodes), so I don&#39;t expect a great accuracy. But another metric can be just as relevant when working with large graphs: training time. . &#129497;&#8205;&#9794;&#65039; II. GraphSAGE in theory . The GraphSAGE algorithm can be divided into two steps: . Neighbor sampling; | Aggregation. | &#127920; A. Neighbor sampling . Neighbor sampling relies on a classic technique used to train neural networks: mini-batch gradient descent. . Mini-batch gradient descent works by breaking down a dataset into smaller batches. During training, we compute the gradient for every mini-batch instead of every epoch (batch gradient descent) or every training sample (stochastic gradient descent). Mini-batching has several benefits: . Improved accuracy — mini-batches help to reduce overfitting (gradients are averaged), as well as variance in error rates | Increased speed — mini-batches are processed in parallel and take less time to train than larger batches | Improved scalability — an entire dataset can exceed the GPU memory, but smaller batches can get around this limitation | More advanced optimizes like Adam also rely on mini-batching. However, it is not as straightforward with graph data since splitting the dataset into smaller chunks would break essential connections between nodes. . So, what can we do? In recent years, researchers developed different strategies to create graph mini-batches. The one we&#39;re interested in is called neighbor sampling). There are many other techniques you can find on PyG&#39;s documentation, such as subgraph clustering). . &lt;/source&gt; Neighbor sampling considers only a fixed number of random neighbors. Here&#39;s the process: . The sampler randomly selects a defined number of neighbors (1 hop), neighbors of neighbors (2 hops), etc. we would like to have | The sampler outputs a subgraph containing the target and sampled nodes | This process is repeated for every node in a list or the entirety of the graph. However, creating a subgraph for each node is not efficient, which is why we can process them in batches instead. In this case, each subgraph is shared by multiple target nodes. . Neighbor sampling has an added benefit. Sometimes, we observe extremely popular nodes that act like hubs, such as celebrities on social media. Calculating embeddings for these nodes can be computationally very expensive since it requires calculating the hidden vectors of thousands or even millions of neighbors. GraphSAGE fixes this issue by only considering a fixed number of neighbors. . In PyG, neighbor sampling is implemented through the NeighborLoader object. Let&#39;s say we want 5 neighbors and 10 of their neighbors (num_neighbors). As we discussed, we can also specify a batch_size to speed up the process by creating subgraphs for multiple target nodes. . from torch_geometric.loader import NeighborLoader from torch_geometric.utils import to_networkx # Create batches with neighbor sampling train_loader = NeighborLoader( data, num_neighbors=[5, 10], batch_size=16, input_nodes=data.train_mask, ) # Print each subgraph for i, subgraph in enumerate(train_loader): print(f&#39;Subgraph {i}: {subgraph}&#39;) # Plot each subgraph fig = plt.figure(figsize=(16,16)) for idx, (subdata, pos) in enumerate(zip(train_loader, [221, 222, 223, 224])): G = to_networkx(subdata, to_undirected=True) ax = fig.add_subplot(pos) ax.set_title(f&#39;Subgraph {idx}&#39;) plt.axis(&#39;off&#39;) nx.draw_networkx(G, pos=nx.spring_layout(G, seed=0), with_labels=True, node_size=200, node_color=subdata.y, cmap=&quot;cool&quot;, font_size=10 ) plt.show() . Subgraph 0: Data(x=[389, 500], edge_index=[2, 448], y=[389], train_mask=[389], val_mask=[389], test_mask=[389], batch_size=16) Subgraph 1: Data(x=[264, 500], edge_index=[2, 314], y=[264], train_mask=[264], val_mask=[264], test_mask=[264], batch_size=16) Subgraph 2: Data(x=[283, 500], edge_index=[2, 330], y=[283], train_mask=[283], val_mask=[283], test_mask=[283], batch_size=16) Subgraph 3: Data(x=[189, 500], edge_index=[2, 229], y=[189], train_mask=[189], val_mask=[189], test_mask=[189], batch_size=12) . We created four subgraphs of various sizes. It allows us to process them in parallel and they&#39;re easier to fit on a GPU since they&#39;re smaller. . The number of neighbors is an important parameter since pruning our graph removes a lot of information. How much, exactly? Well, quite a lot. We can visualize this effect by looking at the node degrees (number of neighbors). . from torch_geometric.utils import degree from collections import Counter def plot_degree(data): # Get list of degrees for each node degrees = degree(data.edge_index[0]).numpy() # Count the number of nodes for each degree numbers = Counter(degrees) # Bar plot fig, ax = plt.subplots(figsize=(14, 6)) ax.set_xlabel(&#39;Node degree&#39;) ax.set_ylabel(&#39;Number of nodes&#39;) plt.bar(numbers.keys(), numbers.values(), color=&#39;#0A047A&#39;) # Plot node degrees from the original graph plot_degree(data) # Plot node degrees from the last subgraph plot_degree(subdata) . The first plot shows the original distribution of node degrees, and the second one shows the distribution we obtain after neighbor sampling. In this example, we chose to only consider five neighbors, which is much lower than the original maximal value. It&#39;s important to remember this tradeoff when talking about GraphSAGE. . PinSAGE proposes another solution. Instead of neighbor sampling, PinSAGE simulates random walks for each node, which captures a better representation of their neighborhoods. Then, it selects a predefined number of neighbors with the highest visit counts. This technique allows PinSAGE to consider the importance of each neighbor while controlling the size of the computation graph. . &#128165; B. Aggregation . The aggregation process determines how to combine the feature vectors to produce the node embeddings. The original paper presents three ways of aggregating features: . Mean aggregator | LSTM aggregator | Pooling aggregator | . &lt;/source&gt; The mean aggregator is the simplest one. The idea is close to a GCN approach: . The hidden features of the target node and its selected neighbors are averaged (nodes in $ mathcal{ tilde{N}}_i$). | A linear transformation with a weight matrix $ textbf{W}$ is applied. | In other words, we can write: . $$ textbf{h}_i&#39; = textbf{W} cdot mean_{j in mathcal{ tilde{N}}_i}( textbf{h}_j)$$ . The result can then be fed to a nonlinear activation function like ReLU. . The LSTM aggregator may seem counter-intuitive because this architecture is sequential: it assigns an order to our unordered nodes. This is why the authors randomly shuffle them to force the LSTM only to consider the hidden features. Nevertheless, it is the best-performing technique in their benchmarks. . The pooling aggregator feeds each neighbor&#39;s hidden vector to a feedforward neural network. Then, an elementwise max operation is applied to the result to keep the highest value for each feature. . &#129504; III. GraphSAGE in PyTorch Geometric . We can easily implement a GraphSAGE architecture in PyTorch Geometric with the SAGEConv layer. This implementation uses two weight matrices instead of one, like UberEats&#39; version of GraphSAGE: . $$ textbf{h}_i&#39; = textbf{W}_1 textbf{h}_i + textbf{W}_2 cdot mean_{j in mathcal{N}_i}( textbf{h}_j)$$ . Let&#39;s create a network with two SAGEConv layers: . The first one uses $ReLU$ as the activation function and a dropout layer; | The second one directly outputs the node embeddings. | . As we&#39;re dealing with a multi-class classification task, we&#39;ll use the cross-entropy loss as our loss function. I also added an L2 regularization of 0.0005 for good measure. . To see the benefits of GraphSAGE, let&#39;s compare it with a GCN and a GAT without any sampling. . import torch from torch.nn import Linear, Dropout from torch_geometric.nn import SAGEConv, GATv2Conv, GCNConv import torch.nn.functional as F class GraphSAGE(torch.nn.Module): &quot;&quot;&quot;GraphSAGE&quot;&quot;&quot; def __init__(self, dim_in, dim_h, dim_out): super().__init__() self.sage1 = SAGEConv(dim_in, dim_h) self.sage2 = SAGEConv(dim_h, dim_out) self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4) def forward(self, x, edge_index): h = self.sage1(x, edge_index).relu() h = F.dropout(h, p=0.5, training=self.training) h = self.sage2(h, edge_index) return F.log_softmax(h, dim=1) def fit(self, data, epochs): criterion = torch.nn.CrossEntropyLoss() optimizer = self.optimizer self.train() for epoch in range(epochs+1): total_loss = 0 acc = 0 val_loss = 0 val_acc = 0 # Train on batches for batch in train_loader: optimizer.zero_grad() out = self(batch.x, batch.edge_index) loss = criterion(out[batch.train_mask], batch.y[batch.train_mask]) total_loss += loss acc += accuracy(out[batch.train_mask].argmax(dim=1), batch.y[batch.train_mask]) loss.backward() optimizer.step() # Validation val_loss += criterion(out[batch.val_mask], batch.y[batch.val_mask]) val_acc += accuracy(out[batch.val_mask].argmax(dim=1), batch.y[batch.val_mask]) # Print metrics every 10 epochs if(epoch % 10 == 0): print(f&#39;Epoch {epoch:&gt;3} | Train Loss: {total_loss/len(train_loader):.3f} &#39; f&#39;| Train Acc: {acc/len(train_loader)*100:&gt;6.2f}% | Val Loss: &#39; f&#39;{val_loss/len(train_loader):.2f} | Val Acc: &#39; f&#39;{val_acc/len(train_loader)*100:.2f}%&#39;) class GAT(torch.nn.Module): &quot;&quot;&quot;Graph Attention Network&quot;&quot;&quot; def __init__(self, dim_in, dim_h, dim_out, heads=8): super().__init__() self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads) self.gat2 = GATv2Conv(dim_h*heads, dim_out, heads=heads) self.optimizer = torch.optim.Adam(self.parameters(), lr=0.005, weight_decay=5e-4) def forward(self, x, edge_index): h = F.dropout(x, p=0.6, training=self.training) h = self.gat1(x, edge_index) h = F.elu(h) h = F.dropout(h, p=0.6, training=self.training) h = self.gat2(h, edge_index) return F.log_softmax(h, dim=1) def fit(self, data, epochs): criterion = torch.nn.CrossEntropyLoss() optimizer = self.optimizer self.train() for epoch in range(epochs+1): # Training optimizer.zero_grad() out = self(data.x, data.edge_index) loss = criterion(out[data.train_mask], data.y[data.train_mask]) acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask]) loss.backward() optimizer.step() # Validation val_loss = criterion(out[data.val_mask], data.y[data.val_mask]) val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask]) # Print metrics every 10 epochs if(epoch % 10 == 0): print(f&#39;Epoch {epoch:&gt;3} | Train Loss: {loss:.3f} | Train Acc:&#39; f&#39; {acc*100:&gt;6.2f}% | Val Loss: {val_loss:.2f} | &#39; f&#39;Val Acc: {val_acc*100:.2f}%&#39;) class GCN(torch.nn.Module): &quot;&quot;&quot;Graph Convolutional Network&quot;&quot;&quot; def __init__(self, dim_in, dim_h, dim_out): super().__init__() self.gcn1 = GCNConv(dim_in, dim_h) self.gcn2 = GCNConv(dim_h, dim_out) self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4) def forward(self, x, edge_index): h = F.dropout(x, p=0.5, training=self.training) h = self.gcn1(h, edge_index).relu() h = F.dropout(h, p=0.5, training=self.training) h = self.gcn2(h, edge_index) return F.log_softmax(h, dim=1) def fit(self, data, epochs): criterion = torch.nn.CrossEntropyLoss() optimizer = self.optimizer self.train() for epoch in range(epochs+1): # Training optimizer.zero_grad() out = self(data.x, data.edge_index) loss = criterion(out[data.train_mask], data.y[data.train_mask]) acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask]) loss.backward() optimizer.step() # Validation val_loss = criterion(out[data.val_mask], data.y[data.val_mask]) val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask]) # Print metrics every 10 epochs if(epoch % 10 == 0): print(f&#39;Epoch {epoch:&gt;3} | Train Loss: {loss:.3f} | Train Acc:&#39; f&#39; {acc*100:&gt;6.2f}% | Val Loss: {val_loss:.2f} | &#39; f&#39;Val Acc: {val_acc*100:.2f}%&#39;) def accuracy(pred_y, y): &quot;&quot;&quot;Calculate accuracy.&quot;&quot;&quot; return ((pred_y == y).sum() / len(y)).item() @torch.no_grad() def test(model, data): &quot;&quot;&quot;Evaluate the model on test set and print the accuracy score.&quot;&quot;&quot; model.eval() out = model(data.x, data.edge_index) acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask]) return acc . With GraphSAGE, we loop through batches (our four subgraphs) created by the neighbor sampling process. The way we calculate the accuracy and the validation loss is also different because of that. . %%time # Create GraphSAGE graphsage = GraphSAGE(dataset.num_features, 64, dataset.num_classes) print(graphsage) # Train graphsage.fit(data, 200) # Test print(f&#39; nGraphSAGE test accuracy: {test(graphsage, data)*100:.2f}% n&#39;) . GraphSAGE( (sage1): SAGEConv(500, 64) (sage2): SAGEConv(64, 3) ) Epoch 0 | Train Loss: 0.332 | Train Acc: 30.24% | Val Loss: 1.13 | Val Acc: 18.33% Epoch 10 | Train Loss: 0.020 | Train Acc: 100.00% | Val Loss: 0.63 | Val Acc: 72.50% Epoch 20 | Train Loss: 0.005 | Train Acc: 100.00% | Val Loss: 0.57 | Val Acc: 73.17% Epoch 30 | Train Loss: 0.005 | Train Acc: 100.00% | Val Loss: 0.49 | Val Acc: 79.96% Epoch 40 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.63 | Val Acc: 63.33% Epoch 50 | Train Loss: 0.009 | Train Acc: 100.00% | Val Loss: 0.61 | Val Acc: 75.56% Epoch 60 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 0.77 | Val Acc: 71.25% Epoch 70 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 0.50 | Val Acc: 79.79% Epoch 80 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 0.54 | Val Acc: 76.74% Epoch 90 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.65 | Val Acc: 76.74% Epoch 100 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.49 | Val Acc: 78.87% Epoch 110 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.59 | Val Acc: 78.87% Epoch 120 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 0.61 | Val Acc: 73.33% Epoch 130 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.74 | Val Acc: 66.67% Epoch 140 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.74 | Val Acc: 59.35% Epoch 150 | Train Loss: 0.001 | Train Acc: 100.00% | Val Loss: 0.82 | Val Acc: 65.06% Epoch 160 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.73 | Val Acc: 65.00% Epoch 170 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 0.85 | Val Acc: 67.92% Epoch 180 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 0.48 | Val Acc: 81.67% Epoch 190 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.50 | Val Acc: 85.83% Epoch 200 | Train Loss: 0.001 | Train Acc: 100.00% | Val Loss: 0.52 | Val Acc: 83.54% GraphSAGE test accuracy: 77.20% CPU times: user 9.17 s, sys: 370 ms, total: 9.54 s Wall time: 12.4 s . %%time # Create GCN gcn = GCN(dataset.num_features, 64, dataset.num_classes) print(gcn) # Train gcn.fit(data, 200) # Test print(f&#39; nGCN test accuracy: {test(gcn, data)*100:.2f}% n&#39;) . GCN( (gcn1): GCNConv(500, 64) (gcn2): GCNConv(64, 3) ) Epoch 0 | Train Loss: 1.098 | Train Acc: 33.33% | Val Loss: 1.10 | Val Acc: 32.20% Epoch 10 | Train Loss: 0.736 | Train Acc: 91.67% | Val Loss: 0.87 | Val Acc: 74.60% Epoch 20 | Train Loss: 0.400 | Train Acc: 96.67% | Val Loss: 0.67 | Val Acc: 73.80% Epoch 30 | Train Loss: 0.214 | Train Acc: 93.33% | Val Loss: 0.61 | Val Acc: 76.80% Epoch 40 | Train Loss: 0.124 | Train Acc: 100.00% | Val Loss: 0.58 | Val Acc: 75.60% Epoch 50 | Train Loss: 0.092 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 77.20% Epoch 60 | Train Loss: 0.095 | Train Acc: 100.00% | Val Loss: 0.58 | Val Acc: 76.80% Epoch 70 | Train Loss: 0.087 | Train Acc: 100.00% | Val Loss: 0.58 | Val Acc: 77.20% Epoch 80 | Train Loss: 0.085 | Train Acc: 100.00% | Val Loss: 0.63 | Val Acc: 75.60% Epoch 90 | Train Loss: 0.088 | Train Acc: 98.33% | Val Loss: 0.62 | Val Acc: 76.60% Epoch 100 | Train Loss: 0.074 | Train Acc: 98.33% | Val Loss: 0.63 | Val Acc: 75.80% Epoch 110 | Train Loss: 0.085 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 76.60% Epoch 120 | Train Loss: 0.069 | Train Acc: 100.00% | Val Loss: 0.63 | Val Acc: 74.20% Epoch 130 | Train Loss: 0.062 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 76.20% Epoch 140 | Train Loss: 0.043 | Train Acc: 100.00% | Val Loss: 0.61 | Val Acc: 75.20% Epoch 150 | Train Loss: 0.045 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 75.60% Epoch 160 | Train Loss: 0.068 | Train Acc: 100.00% | Val Loss: 0.61 | Val Acc: 76.80% Epoch 170 | Train Loss: 0.070 | Train Acc: 100.00% | Val Loss: 0.60 | Val Acc: 76.80% Epoch 180 | Train Loss: 0.060 | Train Acc: 100.00% | Val Loss: 0.61 | Val Acc: 75.40% Epoch 190 | Train Loss: 0.057 | Train Acc: 100.00% | Val Loss: 0.66 | Val Acc: 75.00% Epoch 200 | Train Loss: 0.052 | Train Acc: 100.00% | Val Loss: 0.65 | Val Acc: 75.20% GCN test accuracy: 78.40% CPU times: user 52.4 s, sys: 606 ms, total: 53 s Wall time: 52.6 s . %%time # Create GAT gat = GAT(dataset.num_features, 64, dataset.num_classes) print(gat) # Train gat.fit(data, 200) # Test print(f&#39; nGAT test accuracy: {test(gat, data)*100:.2f}% n&#39;) . GAT( (gat1): GATv2Conv(500, 64, heads=8) (gat2): GATv2Conv(512, 3, heads=8) ) Epoch 0 | Train Loss: 3.174 | Train Acc: 1.67% | Val Loss: 3.18 | Val Acc: 1.00% Epoch 10 | Train Loss: 0.707 | Train Acc: 86.67% | Val Loss: 0.87 | Val Acc: 71.00% Epoch 20 | Train Loss: 0.363 | Train Acc: 93.33% | Val Loss: 0.64 | Val Acc: 77.20% Epoch 30 | Train Loss: 0.178 | Train Acc: 96.67% | Val Loss: 0.58 | Val Acc: 78.40% Epoch 40 | Train Loss: 0.101 | Train Acc: 100.00% | Val Loss: 0.56 | Val Acc: 78.40% Epoch 50 | Train Loss: 0.087 | Train Acc: 100.00% | Val Loss: 0.57 | Val Acc: 77.80% Epoch 60 | Train Loss: 0.072 | Train Acc: 100.00% | Val Loss: 0.57 | Val Acc: 78.40% Epoch 70 | Train Loss: 0.076 | Train Acc: 100.00% | Val Loss: 0.58 | Val Acc: 77.40% Epoch 80 | Train Loss: 0.064 | Train Acc: 100.00% | Val Loss: 0.59 | Val Acc: 76.40% Epoch 90 | Train Loss: 0.058 | Train Acc: 100.00% | Val Loss: 0.58 | Val Acc: 77.20% Epoch 100 | Train Loss: 0.062 | Train Acc: 100.00% | Val Loss: 0.57 | Val Acc: 79.00% Epoch 110 | Train Loss: 0.050 | Train Acc: 100.00% | Val Loss: 0.59 | Val Acc: 77.80% Epoch 120 | Train Loss: 0.044 | Train Acc: 100.00% | Val Loss: 0.60 | Val Acc: 75.40% Epoch 130 | Train Loss: 0.042 | Train Acc: 100.00% | Val Loss: 0.57 | Val Acc: 78.00% Epoch 140 | Train Loss: 0.045 | Train Acc: 100.00% | Val Loss: 0.60 | Val Acc: 78.00% Epoch 150 | Train Loss: 0.038 | Train Acc: 100.00% | Val Loss: 0.60 | Val Acc: 77.20% Epoch 160 | Train Loss: 0.041 | Train Acc: 100.00% | Val Loss: 0.64 | Val Acc: 77.00% Epoch 170 | Train Loss: 0.033 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 76.00% Epoch 180 | Train Loss: 0.031 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 77.60% Epoch 190 | Train Loss: 0.028 | Train Acc: 100.00% | Val Loss: 0.64 | Val Acc: 78.40% Epoch 200 | Train Loss: 0.026 | Train Acc: 100.00% | Val Loss: 0.65 | Val Acc: 76.60% GAT test accuracy: 77.10% CPU times: user 17min 43s, sys: 9.46 s, total: 17min 53s Wall time: 18min 7s . The three models obtain similar results in terms of accuracy. We expect the GAT to perform better because its aggregation mechanism is more nuanced, but it&#39;s not always the case. . The real difference is the training time: GraphSAGE is 88 times faster than the GAT and four times faster than the GCN in this example! . This is the true benefit of GraphSAGE. While it loses a lot of information by pruning the graph with neighbor sampling, it greatly improves scalability. In turn, it can lead to building larger graphs that can improve accuracy. . GraphSAGE is a popular framework with many flavors. . In this example, we have used GraphSAGE in a transductive setting. We masked information about test nodes during training, but we didn&#39;t hide their presence in the adjacency matrix. On the contrary, in an inductive setting, the test set is never encountered during training. . This difference is essential: an inductive model can calculate embeddings for nodes that have never been seen before. On the other hand, a transductive model has to be re-trained, which can quickly become computationally costly. Thanks to neighbor sampling, GraphSAGE is designed to be an inductive model: it does not require seeing every neighbor to calculate an embedding. . Besides these two settings, GraphSAGE can be trained in an unsupervised way. In this case, we can&#39;t use the cross-entropy loss. We have to engineer a loss function that forces nodes that are nearby in the original graph to remain close to each other in the embedding space. Conversely, the same function must ensure that distant nodes in the graph must have distant representations in the embedding space. This is the loss that is presented in GraphSAGE&#39;s paper. . PinSAGE and UberEeats&#39; modified GraphSAGE are also slightly different since we&#39;re dealing with recommender systems. Their goal is to correctly rank the most relevant items (pins, restaurants) for each user. We don&#39;t only want to get the closest embeddings, but we also have to produce the best rankings possible. This is why these systems are trained in an unsupervised way but with another loss function: a max-margin ranking loss. . Conclusion . GraphSAGE is an incredibly fast architecture that can process large graphs. It might not be as accurate as a GCN or a GAT, but it is an essential model for handling massive amounts of data. It delivers this speed thanks to a clever combination of neighbor sampling and fast aggregation. In this article, . We explored a new dataset with PubMed, which has almost ten times more connections than the previous one (CiteSeer) | We explained the idea behind neighbor sampling, which only considers a predefined number of random neighbors at each hop | We saw the three aggregators presented in GraphSAGE&#39;s paper and focused on the mean aggregator | We benchmarked three models (GraphSAGE, GAT, and GCN) in terms of accuracy and training time | . We saw three architectures with the same end application: node classification. But GNNs have been successfully applied to other tasks. In the next tutorials, I&#39;d like to use them in two different contexts: graph and edge prediction. This will be a good way to discover new datasets and applications where GNNs dominate the state of the art. . If you enjoyed this article, let&#39;s connect on Twitter @maximelabonne for more graph learning content. . Thanks for your attention! 📣 . &#127760; Graph Neural Network Course . 🔎 Course overview . 📝 Chapter 1: Introduction to Graph Neural Networks . 📝 Chapter 2: Graph Attention Network . 📝 Chapter 3: GraphSAGE . 📝 Chapter 4: Graph Isomorphism Network .",
            "url": "https://mlabonne.github.io/blog/graphsage/",
            "relUrl": "/graphsage/",
            "date": " • Apr 6, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "What is a Tensor in Deep Learning?",
            "content": "What is a tensor, exactly? . Most deep learning practitioners know about them but can&#39;t pinpoint an exact definition. . TensorFlow, PyTorch: every deep learning framework relies on the same basic object: tensors. They&#39;re used to store almost everything in deep learning: input data, weights, biases, predictions, etc. . And yet, their definition is incredibly fuzzy: the Wikipedia category alone has over 100 pages related to tensors. . In this article, we&#39;ll give a definitive answer to the following question: what is a tensor in neural networks? . &#128187; Tensors in computer science . So why are there so many definitions? . It&#39;s quite simple: different fields have different definitions. Tensors in mathematics are not quite the same as tensors in physics, which are different from tensors in computer science. . These definitions can be divided into two categories: tensors as a data structure or as objects (in an object-oriented programming sense). . Data structure: this is the definition we use in computer science. Tensors are multidimensional arrays that store a specific type of value. | Objects: this is the definition used in other fields. In mathematics and physics, tensors are not just a data structure: they also have a list of properties, like a specific product. | . This is why you see a lot of people (sometimes quite pedantically) saying &quot;tensors are not n-dimensional arrays/matrices&quot;: they don&#39;t talk about data structures, but about objects with properties. . Even the same words have different meanings. For instance, in computer science, a 2D tensor is a matrix (it&#39;s a tensor of rank 2). In linear algebra, a tensor with 2 dimensions means it only stores two values. The rank also has a completely different definition: it is the maximum number of its linearly independent column (or row) vectors. . In computer science, we&#39;re only interested in a definition focused on the data structure. From this point of view, tensors truly are a generalization in $n$ dimensions of matrices. . But we&#39;re still missing an important nuance when talking about tensors specifically in the context of deep learning... . &#129504; Tensors in deep learning . Icons created by Freepik and smashingstocks - FlaticonSo why are they called &quot;tensors&quot; instead of &quot;multidimensional arrays&quot;? Ok, it is shorter, but is it all there is to it? Actually, people make an implicit assumption when they talk about tensors. . PyTorch&#39;s official documentation gives us a practical answer: . The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either CPU or GPU. . In deep learning, we need performance to compute a lot of matrix multiplications in a highly parallel way. These matrices (and n-dimensional arrays in general) are generally stored and processed on GPUs to speed up training and inference times. . This is what was missing in our previous definition:tensors in deep learning are not just n-dimensional arrays, there&#39;s also the implicit assumption they can be run on a GPU. . &#9876;&#65039; NumPy vs PyTorch . Let&#39;s see the difference between NumPy arrays and PyTorch tensors. . These two objects are very similar: we can initialize a 1D array and a 1D tensor with nearly the same syntax. They also share a lot of methods and can be easily converted into one another. . You can find the code used in this article at this address . import numpy as np import torch array = np.array([1, 2, 3]) print(f&#39;NumPy Array: {array}&#39;) tensor = torch.tensor([1, 2, 3]) print(f&#39;PyTorch Tensor: {tensor}&#39;) . NumPy Array: [1 2 3] PyTorch Tensor: tensor([1, 2, 3]) . Initializing 2D arrays and 2D tensors is not more complicated. . x = np.array([[1, 2, 3], [4, 5, 6]]) print(f&#39;NumPy Array: n{x}&#39;) x = torch.tensor([[1, 2, 3], [4, 5, 6]]) print(f&#39; nPyTorch Tensor: n{x}&#39;) . NumPy Array: [[1 2 3] [4 5 6]] PyTorch Tensor: tensor([[1, 2, 3], [4, 5, 6]]) . We said that the only difference between tensors and arrays was the fact that tensors can be run on GPUs. So in the end, this distinction is based on performance. But is this boost that important? . Let&#39;s compare the performance between NumPy arrays and PyTorch tensors on matrix multiplication. In the following example, we randomly initialize 4D arrays/tensors and multiply them. . device = torch.device(&quot;cuda&quot;) # 4D arrays array1 = np.random.rand(100, 100, 100, 100) array2 = np.random.rand(100, 100, 100, 100) # 4D tensors tensor1 = torch.rand(100, 100, 100, 100).to(device) tensor2 = torch.rand(100, 100, 100, 100).to(device) . %%timeit np.matmul(array1, array2) . 1 loop, best of 5: 1.32 s per loop . %%timeit torch.matmul(tensor1, tensor2) . 1000 loops, best of 5: 25.2 ms per loop . As we can see, PyTorch tensors completed outperformed NumPy arrays: they completed the multiplication 52 times faster! . This is the true power of tensors: they&#39;re blazingly fast! Performance might vary depending on the dimensions, the implementation, and the hardware, but this speed is the reason why tensors (and not arrays) are so common in deep learning. . Conclusion . In this article, we wrote a definition of tensors based on: . Their use in computer science (data structure); | More specifically, in deep learning (they can run on GPUs). | Here&#39;s how we can summarize it in one sentence: . Tensors are n-dimensional arrays with the implicit assumption that they can run on a GPU. . Finally, we saw the difference in performance between tensors and arrays, which motivates the need for tensors in deep learning. . So next time someone tries to explain to you that tensors are not exactly a generalization of matrices, you&#39;ll know that they&#39;re right in a particular definition of tensors, but not in the computer science/deep learning one. . If you&#39;re looking for more data science and machine learning content in n-dimensions, please follow me on twitter @maximelabonne. 📣 .",
            "url": "https://mlabonne.github.io/blog/what-is-a-tensor/",
            "relUrl": "/what-is-a-tensor/",
            "date": " • Mar 28, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Efficiently iterating over rows in a Pandas DataFrame",
            "content": "When I started machine learning, I followed the guidelines and created my own features by combining multiple columns in my dataset. It&#39;s all well and good, but the way I did it was horribly inefficient. I had to wait several minutes to do the most basic operations. . My problem was simple: I didn&#39;t know the fastest way to iterate over rows in Pandas. . I often see people online using the same techniques I used to apply. It&#39;s not elegant but it&#39;s ok if you don&#39;t have much data. However, if you process more than 10k rows, it quickly becomes an obvious performance issue. . In this article, I&#39;m gonna give you the best way to iterate over rows in a Pandas DataFrame, with no extra code required. It&#39;s not just about performance: it&#39;s also about understanding what&#39;s going on under the hood to become a better data scientist. . Let&#39;s import a dataset in Pandas. In this case, I chose the one I worked on when I started: it&#39;s time to fix my past mistakes! 🩹 . You can run the code with the following Google Colab notebook. . import pandas as pd import numpy as np df = pd.read_csv(&#39;https://raw.githubusercontent.com/mlabonne/how-to-data-science/main/data/nslkdd_test.txt&#39;) df . duration protocol_type service flag src_bytes dst_bytes land wrong_fragment urgent hot ... dst_host_same_srv_rate dst_host_diff_srv_rate dst_host_same_src_port_rate dst_host_srv_diff_host_rate dst_host_serror_rate dst_host_srv_serror_rate dst_host_rerror_rate dst_host_srv_rerror_rate attack_type other . 0 0 | tcp | private | REJ | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0.04 | 0.06 | 0.00 | 0.00 | 0.00 | 0.0 | 1.00 | 1.00 | neptune | 21 | . 1 0 | tcp | private | REJ | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0.00 | 0.06 | 0.00 | 0.00 | 0.00 | 0.0 | 1.00 | 1.00 | neptune | 21 | . 2 2 | tcp | ftp_data | SF | 12983 | 0 | 0 | 0 | 0 | 0 | ... | 0.61 | 0.04 | 0.61 | 0.02 | 0.00 | 0.0 | 0.00 | 0.00 | normal | 21 | . 3 0 | icmp | eco_i | SF | 20 | 0 | 0 | 0 | 0 | 0 | ... | 1.00 | 0.00 | 1.00 | 0.28 | 0.00 | 0.0 | 0.00 | 0.00 | saint | 15 | . 4 1 | tcp | telnet | RSTO | 0 | 15 | 0 | 0 | 0 | 0 | ... | 0.31 | 0.17 | 0.03 | 0.02 | 0.00 | 0.0 | 0.83 | 0.71 | mscan | 11 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 22539 0 | tcp | smtp | SF | 794 | 333 | 0 | 0 | 0 | 0 | ... | 0.72 | 0.06 | 0.01 | 0.01 | 0.01 | 0.0 | 0.00 | 0.00 | normal | 21 | . 22540 0 | tcp | http | SF | 317 | 938 | 0 | 0 | 0 | 0 | ... | 1.00 | 0.00 | 0.01 | 0.01 | 0.01 | 0.0 | 0.00 | 0.00 | normal | 21 | . 22541 0 | tcp | http | SF | 54540 | 8314 | 0 | 0 | 0 | 2 | ... | 1.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.0 | 0.07 | 0.07 | back | 15 | . 22542 0 | udp | domain_u | SF | 42 | 42 | 0 | 0 | 0 | 0 | ... | 0.99 | 0.01 | 0.00 | 0.00 | 0.00 | 0.0 | 0.00 | 0.00 | normal | 21 | . 22543 0 | tcp | sunrpc | REJ | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0.08 | 0.03 | 0.00 | 0.00 | 0.00 | 0.0 | 0.44 | 1.00 | mscan | 14 | . 22544 rows × 43 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; This dataset has 22k rows and 43 columns with a combination of categorical and numerical values. Each row describes a connection between two computers. . Let&#39;s say we want to create a new feature: the total number of bytes in the connection. We just have to sum up two existing features: src_bytes and dst_bytes. Let&#39;s see different methods to calculate this new feature. . &#10060;&#10060; 1. Iterrows . According to the official documentation, iterrows() iterates &quot;over the rows of a Pandas DataFrame as (index, Series) pairs&quot;. It converts each row into a Series object, which causes two problems: . It can change the type of your data (dtypes); | The conversion greatly degrades performance. | For these reasons, the ill-named iterrows() is the WORST possible method to actually iterate over rows. . %%timeit -n 10 # Iterrows total = [] for index, row in df.iterrows(): total.append(row[&#39;src_bytes&#39;] + row[&#39;dst_bytes&#39;]) . 10 loops, best of 5: 1.07 s per loop . Now let&#39;s see slightly better techniques... . &#10060; 2. For loop with .loc or .iloc (3&#215; faster) . This is what I used to do when I started: a basic for loop to select rows by index (with .loc or .iloc). . Why is it bad? Because DataFrames are not designed for this purpose. As with the previous method, rows are converted into Pandas Series objects, which degrades performance. . Interestingly enough, .iloc is faster than .loc. It makes sense since Python doesn&#39;t have to check user-defined labels and directly look at where the row is stored in memory. . %%timeit -n 10 # For loop with .loc total = [] for index in range(len(df)): total.append(df[&#39;src_bytes&#39;].loc[index] + df[&#39;dst_bytes&#39;].loc[index]) . 10 loops, best of 5: 600 ms per loop . %%timeit -n 10 # For loop with .iloc total = [] for index in range(len(df)): total.append(df[&#39;src_bytes&#39;].iloc[index] + df[&#39;dst_bytes&#39;].iloc[index]) . 10 loops, best of 5: 377 ms per loop . Even this basic for loop with .iloc is 3 times faster than the first method! . &#10060; 3. Apply (4&#215; faster) . The apply() method is another popular choice to iterate over rows. It creates code that is easy to understand but at a cost: performance is nearly as bad as the previous for loop. . This is why I would strongly advise you to avoid this function for this specific purpose (it&#39;s fine for other applications). . Note that I convert the DataFrame into a list using the to_list() method to obtain identical results. . %%timeit -n 10 # Apply df.apply(lambda row: row[&#39;src_bytes&#39;] + row[&#39;dst_bytes&#39;], axis=1).to_list() . 10 loops, best of 5: 282 ms per loop . The apply() method is a for loop in disguise, which is why the performance doesn&#39;t improve that much: it&#39;s only 4 times faster than the first technique. . &#10060; 4. Itertuples (10&#215; faster) . If you know about iterrows(), you probably know about itertuples(). According to the official documentation, it iterates &quot;over the rows of a DataFrame as namedtuples of the values&quot;. In practice, it means that rows are converted into tuples, which are much lighter objects than Pandas Series. . This is why itertuples() is a better version of iterrows(). This time, we need to access the values with an attribute (or an index). If you want to access them with a string (e.g., if there&#39;s a space in the string), you can use the getattr() function instead. . %%timeit -n 10 # Itertuples total = [] for row in df.itertuples(): total.append(row.src_bytes + row.dst_bytes) . 10 loops, best of 5: 99.3 ms per loop . This is starting to look better: it is now 10 times faster than iterrows(). . &#10060; 5. List comprehensions (200&#215; faster) . List comprehensions are a fancy way to iterate over a list as a one-liner. . For instance, [print(i) for i in range(10)] prints numbers from 0 to 9 without any explicit for loop. I say &quot;explicit&quot; because Python actually processes it as a for loop if we look at the bytecode. . So why is it faster? Quite simply because we don&#39;t call the .append() method in this version. . %%timeit -n 100 # List comprehension [src + dst for src, dst in zip(df[&#39;src_bytes&#39;], df[&#39;dst_bytes&#39;])] . 100 loops, best of 5: 5.54 ms per loop . Indeed, this technique is 200 times faster than the first one! But we can still do better. . &#9989; 6. Pandas vectorization (1500&#215; faster) . Until now, all the techniques used simply add up single values. Instead of adding single values, why not group them into vectors to sum them up? The difference between adding two numbers or two vectors is not significant for a CPU, which should speed things up. . On top of that, Pandas can process Series objects in parallel, using every CPU core available! . The syntax is also the simplest imaginable: this solution is extremely intuitive. Under the hood, Pandas takes care of vectorizing our data with an optimized C code using contiguous memory blocks. . %%timeit -n 1000 # Vectorization (df[&#39;src_bytes&#39;] + df[&#39;dst_bytes&#39;]).to_list() . 1000 loops, best of 5: 734 µs per loop . This code is 1500 times faster than iterrows() and it is even simpler to write. . &#9989;&#9989; 7. NumPy vectorization (1900&#215; faster) . NumPy is designed to handle scientific computing. It has less overhead than Pandas methods since rows and dataframes all become np.array. It relies on the same optimizations as Pandas vectorization. . There are two ways of converting a Series into a np.array: using .values or .to_numpy(). The former has been deprecated for years, which is why we&#39;re gonna use .to_numpy() in this example. . %%timeit -n 1000 # Numpy vectorization (df[&#39;src_bytes&#39;].to_numpy() + df[&#39;dst_bytes&#39;].to_numpy()).tolist() . 1000 loops, best of 5: 575 µs per loop . We found our winner with a technique that is 1900 times faster than our first competitor! Let&#39;s wrap things up. . Conclusion . Don&#39;t be like me: if you need to iterate over rows in a DataFrame, vectorization is the way to go! You can find the code to reproduce the experiments at this address. Vectorization is not harder to read, it doesn&#39;t take longer to write, and the performance gain is incredible. . It&#39;s not just about performance: understanding how each method works under the hood helped me to write better code. Performance gains are always based on the same techniques: transforming data into vectors and matrices to take advantage of parallel processing. Alas, this is often at the expense of readability. But it doesn&#39;t have to be. . Iterating over rows is just an example but it shows that, sometimes, you can have the cake and eat it. 🎂 . If you liked this article, follow me on Twitter @maximelabonne for more tips about data science and machine learning! . &#128200; Bonus . We can measure the performance of each method depending on the size of the DataFrame. I reimplemented all of them in this dummy example using perfplot to show that the leaderboard might be different under 300 rows. Anyway, such a dataset would be so small that we wouldn&#39;t need much optimization. . !pip install -q perfplot import perfplot import matplotlib.pyplot as plt plt.rcParams.update({&#39;font.size&#39;: 22}) # Techniques def forloop(df): total = [] for index in range(len(df)): total.append(df[&#39;col1&#39;].iloc[index] + df[&#39;col2&#39;].iloc[index]) return total def itertuples(df): total = [] for row in df.itertuples(): total.append(row[1] + row[2]) return total def iterrows(df): total = [] for index, row in df.iterrows(): total.append(row[&#39;col1&#39;] + row[&#39;col2&#39;]) return total def apply(df): return df.apply(lambda row: row[&#39;col1&#39;] + row[&#39;col2&#39;], axis=1).to_list() def comprehension(df): return [src + dst for src, dst in zip(df[&#39;col1&#39;], df[&#39;col2&#39;])] def pd_vectorize(df): return (df[&#39;col1&#39;] + df[&#39;col2&#39;]).to_list() def np_vectorize(df): return (df[&#39;col1&#39;].to_numpy() + df[&#39;col2&#39;].to_numpy()).tolist() # Perfplot functions = [iterrows, forloop, apply, itertuples, comprehension, pd_vectorize, np_vectorize] df = pd.DataFrame({&#39;col1&#39;: [1, 2], &#39;col2&#39;: [3, 4]}) out = perfplot.bench( setup=lambda n: pd.concat([df]*n, ignore_index=True), kernels=functions, labels=[str(f.__name__) for f in functions], n_range=[2**n for n in range(20)], xlabel=&#39;Number of rows&#39;, ) plt.figure(figsize=(20,12)) out.show() . . . (8.117999999999996e-06, 51.558470168) .",
            "url": "https://mlabonne.github.io/blog/iterating-over-rows-pandas-dataframe/",
            "relUrl": "/iterating-over-rows-pandas-dataframe/",
            "date": " • Mar 21, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Graph Attention Networks: Self-Attention for GNNs",
            "content": "Graph Attention Networks (GATs) are one of the most popular types of Graph Neural Networks. . Instead of calculating static weights based on node degrees like Graph Convolutional Networks (GCNs), they assign dynamic weights to node features through a process called self-attention. The main idea behind GATs is that some neighbors are more important than others, regardless of their node degrees. . Node 4 is more important than node 3, which is more important than node 2In this article, we will see how to calculate these attention scores and implement an efficient GAT in PyTorch Geometric (PyG). You can run the code of this tutorial with the following Google Colab notebook. . import torch !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git # Numpy for matrices import numpy as np np.random.seed(0) # Visualization import networkx as nx from sklearn.manifold import TSNE import matplotlib.pyplot as plt plt.rcParams[&#39;figure.dpi&#39;] = 300 plt.rcParams.update({&#39;font.size&#39;: 24}) . &#127760; I. Graph data . CiteSeer datasetLet&#39;s perform a node classification task with a GAT. We can use three classic graph datasets (MIT license) for this work. They represent networks of research papers, where each connection is a citation. . Cora: it consists of 2,708 machine learning papers that belong to one of seven categories. ➡️ Node features represent the presence (1) or absence (0) of 1,433 words in a paper (binary bag of words). | CiteSeer: it is a bigger but similar dataset of 3,327 scientific papers to classify into one of six categories. ➡️ Node features represent the presence (1) or absence (0) of 3,703 words in a paper. | PubMed: it is an even bigger dataset with 19,717 scientific publications about diabetes from PubMed&#39;s database, classified into three categories. ➡️ Node features are TF-IDF weighted word vector from a dictionary of 500 unique words. | . These datasets have been widely used by the scientific community. As a challenge, we can compare our accuracy scores to those obtained in the literature (with standard deviation) using Multilayer Perceptrons (MLPs), GCNs, and GATs: . Dataset 📝Cora 📝CiteSeer 📝PubMed . MLP | 55.1% | 46.5% | 71.4% | . GCN | 81.4 ± 0.5% | 70.9% ± 0.5% | 79.0% ± 0.3% | . GAT | 83.0% ± 0.7% | 72.5% ± 0.7% | 79.0% ± 0.3% | . PubMed is quite large, so it would take longer to process and train a GNN on it. On the other hand, Cora is the most studied one in the literature, so let&#39;s focus on CiteSeer as a middle ground. . We can directly import any of these datasets in PyTorch Geometric with the Planetoid class: . from torch_geometric.datasets import Planetoid # Import dataset from PyTorch Geometric dataset = Planetoid(root=&quot;.&quot;, name=&quot;CiteSeer&quot;) data = dataset[0] # Print information about the dataset print(f&#39;Number of graphs: {len(dataset)}&#39;) print(f&#39;Number of nodes: {data.x.shape[0]}&#39;) print(f&#39;Number of features: {dataset.num_features}&#39;) print(f&#39;Number of classes: {dataset.num_classes}&#39;) print(f&#39;Has isolated nodes: {data.has_isolated_nodes()}&#39;) . Number of graphs: 1 Number of nodes: 3327 Number of features: 3703 Number of classes: 6 Has isolated nodes: True . The CiteSeer dataset correctly exhibits the characteristics we previously described. However, some nodes are isolated (48 to be precise)! Correctly classifying these isolated nodes will be a challenge since we cannot rely on any aggregation. This is how an MLP processes nodes: it cannot consider the adjacency matrix, which decreases its accuracy. . Let&#39;s plot the number of connections of each node with degree: . from torch_geometric.utils import degree from collections import Counter # Get the list of degrees for each node degrees = degree(data.edge_index[0]).numpy() # Count the number of nodes for each degree numbers = Counter(degrees) # Bar plot fig, ax = plt.subplots(figsize=(18, 6)) ax.set_xlabel(&#39;Node degree&#39;) ax.set_ylabel(&#39;Number of nodes&#39;) plt.bar(numbers.keys(), numbers.values(), color=&#39;#0A047A&#39;) . &lt;BarContainer object of 32 artists&gt; . Most nodes only have 1 or 2 neighbors. It could explain why CiteSeer obtains lower accuracy scores than the two other datasets… . &#9888;&#65039; II. Graph Attention Layer . Introduced by Veličković et al. in 2017, self-attention in GATs relies on a simple idea: some nodes are more important than others. In this context, we talk about self-attention (and not just attention) because inputs are compared to each other. . In the previous figure, self-attention calculates the importance of nodes 2, 3, and 4&#39;s features to node 1. We denote $ alpha_{ij}$ the importance of node $j$&#39;s features to node $i$. . Each node $i$ has an attribute vector $x_i$. The GAT layer calculates the embedding of node 1 as a sum of attention coefficients multiplied by a shared weight matrix $ mathbf{W}$ : . $$h_i = alpha_{11} mathbf{W}x_1 + alpha_{12} mathbf{W}x_2 + alpha_{13} mathbf{W}x_3 + alpha_{14} mathbf{W}x_4$$ . But how do we calculate these attention coefficients? We could write a static formula, but there&#39;s a smarter solution: we can learn their values with a neural network. There are four steps in this process: . Linear transformation | Activation function | Softmax normalization | Multi-head attention | 1. Linear transformation . To calculate the attention coefficient, we need to consider pairs of nodes. An easy way to create these pairs is to concatenate attribute vectors from both nodes. . Then, we can apply a new linear transformation with a weight matrix $W_{att}$: . $$a_{ij} = W_{att}^t[ mathbf{W}x_i mathbin Vert mathbf{W}x_j]$$ . 2. Activation function . We&#39;re building a neural network, so the second step is to add nonlinearity with an activation function. In this case, the paper&#39;s authors chose the LeakyReLU function. . $$e_{ij} = LeakyReLU(a_{ij})$$ . 3. Softmax normalization . The output of our neural network is not normalized, which is a problem since we want to compare these coefficients. For example, to be able to say if node 2 is more important to node 1 than node 3 ($ alpha_{12} &gt; alpha_{13}$), we need to use the same scale. . A common way to do it with neural networks is to use the softmax function. Here, we apply it to every neighboring node, including the target node itself: . $$ alpha_{ij} = softmax_j(e_{ij}) = frac{exp(e_{ij})}{ sum_{k in mathcal{N}_i}{exp(e_{ik})}}$$ . This equation produces the final attention coefficients $ alpha_{ij}$. The only problem is... self-attention is not very stable. In order to improve performance, Vaswani et al. introduced multi-head attention in the transformer architecture. . 4. Multi-head attention . This should not be a big surprise if you&#39;re familiar with the transformer architecture, but transformers are a special case of GNNs. This is why GATs look so much like a simplified version of transformers. The good thing is that we can reuse some ideas from Natural Language Processing here, like multi-head attention. . In GATs, multi-head attention consists of replicating the same three steps several times in order to average or concatenate the results. Instead of a single embedding $h_1$, we get one embedding per attention head (denoted $h_1^k$ for the head $k$). One of the two following schemes can then be applied: . Average: we sum the different $h_i^k$ and normalize the result by the number of attention heads $n$; | . $$h_i = frac{1}{n} sum_{k=1}^n{h_i^k}$$ . Concatenation: we concatenate the different $h_i^k$. | . $$h_i = mathbin Vert_{k=1}^n{h_i^k}$$ . In practice, we use the concatenation scheme when it&#39;s a hidden layer and the average scheme when it&#39;s the last (output) layer. Most of the time, we will stack several GAT layers to aggregate a larger neighborhood and thus combine these two schemes in the same GAT model. . &#129504; III. Implementing a Graph Attention Network . Let&#39;s now implement a GAT in PyTorch Geometric. This library has two different graph attention layers: GATConv and GATv2Conv. . The layer we talked about in the previous section is the GatConv layer, but in 2021 Brody et al. introduced an improved layer by modifying the order of operations. In Gatv2Conv, the weight matrix $ mathbf{W}$ is applied after the concatenation and the attention weight matrix $W_{att}$ after the $LeakyReLU$ function. In summary: . GatConv: $e_{ij} = LeakyReLU(W_{att}^t[ mathbf{W}x_i mathbin Vert mathbf{W}x_j])$ | Gatv2Conv: $e_{ij} = W_{att}^tLeakyReLU( mathbf{W}[x_i mathbin Vert x_j])$ | . Which one should you use? According to the authors, Gatv2Conv consistently outperforms GatConv and thus should be preferred. We&#39;ll follow their advice and implement this improved layer in our example. . Okay, let&#39;s classify the papers from CiteSeer! I tried to (roughly) reproduce the experiments of the original authors without adding too much complexity. You can find the official implementation of GAT on GitHub. . Note that we use graph attention layers in two configurations: . The first layer concatenates 8 outputs (multi-head attention); | The second layer only has 1 head, which produces our final embeddings. | . We&#39;re also going to train and test a GCN with two GCN layers (and dropout) to compare the accuracy scores. . import torch.nn.functional as F from torch.nn import Linear, Dropout from torch_geometric.nn import GCNConv, GATv2Conv class GCN(torch.nn.Module): &quot;&quot;&quot;Graph Convolutional Network&quot;&quot;&quot; def __init__(self, dim_in, dim_h, dim_out): super().__init__() self.gcn1 = GCNConv(dim_in, dim_h) self.gcn2 = GCNConv(dim_h, dim_out) self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4) def forward(self, x, edge_index): h = F.dropout(x, p=0.5, training=self.training) h = self.gcn1(h, edge_index).relu() h = F.dropout(h, p=0.5, training=self.training) h = self.gcn2(h, edge_index) return h, F.log_softmax(h, dim=1) class GAT(torch.nn.Module): &quot;&quot;&quot;Graph Attention Network&quot;&quot;&quot; def __init__(self, dim_in, dim_h, dim_out, heads=8): super().__init__() self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads) self.gat2 = GATv2Conv(dim_h*heads, dim_out, heads=1) self.optimizer = torch.optim.Adam(self.parameters(), lr=0.005, weight_decay=5e-4) def forward(self, x, edge_index): h = F.dropout(x, p=0.6, training=self.training) h = self.gat1(h, edge_index) h = F.elu(h) h = F.dropout(h, p=0.6, training=self.training) h = self.gat2(h, edge_index) return h, F.log_softmax(h, dim=1) def accuracy(pred_y, y): &quot;&quot;&quot;Calculate accuracy.&quot;&quot;&quot; return ((pred_y == y).sum() / len(y)).item() def train(model, data): &quot;&quot;&quot;Train a GNN model and return the trained model.&quot;&quot;&quot; criterion = torch.nn.CrossEntropyLoss() optimizer = model.optimizer epochs = 200 model.train() for epoch in range(epochs+1): # Training optimizer.zero_grad() _, out = model(data.x, data.edge_index) loss = criterion(out[data.train_mask], data.y[data.train_mask]) acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask]) loss.backward() optimizer.step() # Validation val_loss = criterion(out[data.val_mask], data.y[data.val_mask]) val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask]) # Print metrics every 10 epochs if(epoch % 10 == 0): print(f&#39;Epoch {epoch:&gt;3} | Train Loss: {loss:.3f} | Train Acc: &#39; f&#39;{acc*100:&gt;6.2f}% | Val Loss: {val_loss:.2f} | &#39; f&#39;Val Acc: {val_acc*100:.2f}%&#39;) return model @torch.no_grad() def test(model, data): &quot;&quot;&quot;Evaluate the model on test set and print the accuracy score.&quot;&quot;&quot; model.eval() _, out = model(data.x, data.edge_index) acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask]) return acc . %%time # Create GCN model gcn = GCN(dataset.num_features, 16, dataset.num_classes) print(gcn) # Train and test train(gcn, data) acc = test(gcn, data) print(f&#39; nGCN test accuracy: {acc*100:.2f}% n&#39;) . GCN( (gcn1): GCNConv(3703, 16) (gcn2): GCNConv(16, 6) ) Epoch 0 | Train Loss: 1.782 | Train Acc: 20.83% | Val Loss: 1.79 | Val Acc: 17.40% Epoch 10 | Train Loss: 0.580 | Train Acc: 89.17% | Val Loss: 1.31 | Val Acc: 55.40% Epoch 20 | Train Loss: 0.165 | Train Acc: 95.00% | Val Loss: 1.30 | Val Acc: 56.20% Epoch 30 | Train Loss: 0.113 | Train Acc: 97.50% | Val Loss: 1.49 | Val Acc: 54.40% Epoch 40 | Train Loss: 0.069 | Train Acc: 99.17% | Val Loss: 1.66 | Val Acc: 54.60% Epoch 50 | Train Loss: 0.037 | Train Acc: 100.00% | Val Loss: 1.65 | Val Acc: 55.60% Epoch 60 | Train Loss: 0.053 | Train Acc: 99.17% | Val Loss: 1.50 | Val Acc: 56.60% Epoch 70 | Train Loss: 0.084 | Train Acc: 97.50% | Val Loss: 1.50 | Val Acc: 58.00% Epoch 80 | Train Loss: 0.054 | Train Acc: 100.00% | Val Loss: 1.67 | Val Acc: 54.40% Epoch 90 | Train Loss: 0.048 | Train Acc: 98.33% | Val Loss: 1.54 | Val Acc: 57.80% Epoch 100 | Train Loss: 0.062 | Train Acc: 99.17% | Val Loss: 1.62 | Val Acc: 56.20% Epoch 110 | Train Loss: 0.082 | Train Acc: 96.67% | Val Loss: 1.52 | Val Acc: 56.60% Epoch 120 | Train Loss: 0.043 | Train Acc: 100.00% | Val Loss: 1.66 | Val Acc: 55.00% Epoch 130 | Train Loss: 0.058 | Train Acc: 98.33% | Val Loss: 1.55 | Val Acc: 59.80% Epoch 140 | Train Loss: 0.058 | Train Acc: 98.33% | Val Loss: 1.68 | Val Acc: 58.40% Epoch 150 | Train Loss: 0.031 | Train Acc: 100.00% | Val Loss: 1.65 | Val Acc: 58.40% Epoch 160 | Train Loss: 0.037 | Train Acc: 100.00% | Val Loss: 1.44 | Val Acc: 64.20% Epoch 170 | Train Loss: 0.025 | Train Acc: 100.00% | Val Loss: 1.58 | Val Acc: 58.40% Epoch 180 | Train Loss: 0.036 | Train Acc: 99.17% | Val Loss: 1.65 | Val Acc: 58.00% Epoch 190 | Train Loss: 0.041 | Train Acc: 97.50% | Val Loss: 1.69 | Val Acc: 57.60% Epoch 200 | Train Loss: 0.093 | Train Acc: 95.83% | Val Loss: 1.73 | Val Acc: 56.80% GCN test accuracy: 67.70% CPU times: user 25.1 s, sys: 847 ms, total: 25.9 s Wall time: 32.4 s . %%time # Create GAT model gat = GAT(dataset.num_features, 8, dataset.num_classes) print(gat) # Train and test train(gat, data) acc = test(gat, data) print(f&#39; nGAT test accuracy: {acc*100:.2f}% n&#39;) . GAT( (gat1): GATv2Conv(3703, 8, heads=8) (gat2): GATv2Conv(64, 6, heads=1) ) Epoch 0 | Train Loss: 1.790 | Train Acc: 17.50% | Val Loss: 1.81 | Val Acc: 12.80% Epoch 10 | Train Loss: 0.114 | Train Acc: 96.67% | Val Loss: 1.05 | Val Acc: 67.20% Epoch 20 | Train Loss: 0.040 | Train Acc: 98.33% | Val Loss: 1.21 | Val Acc: 64.80% Epoch 30 | Train Loss: 0.021 | Train Acc: 99.17% | Val Loss: 1.30 | Val Acc: 65.80% Epoch 40 | Train Loss: 0.027 | Train Acc: 99.17% | Val Loss: 1.20 | Val Acc: 67.20% Epoch 50 | Train Loss: 0.012 | Train Acc: 99.17% | Val Loss: 1.18 | Val Acc: 67.20% Epoch 60 | Train Loss: 0.009 | Train Acc: 100.00% | Val Loss: 1.11 | Val Acc: 67.00% Epoch 70 | Train Loss: 0.007 | Train Acc: 100.00% | Val Loss: 1.19 | Val Acc: 64.80% Epoch 80 | Train Loss: 0.013 | Train Acc: 100.00% | Val Loss: 1.16 | Val Acc: 66.80% Epoch 90 | Train Loss: 0.009 | Train Acc: 100.00% | Val Loss: 1.10 | Val Acc: 66.60% Epoch 100 | Train Loss: 0.013 | Train Acc: 100.00% | Val Loss: 1.07 | Val Acc: 67.20% Epoch 110 | Train Loss: 0.012 | Train Acc: 100.00% | Val Loss: 1.14 | Val Acc: 67.20% Epoch 120 | Train Loss: 0.014 | Train Acc: 100.00% | Val Loss: 1.12 | Val Acc: 66.40% Epoch 130 | Train Loss: 0.009 | Train Acc: 100.00% | Val Loss: 1.12 | Val Acc: 68.20% Epoch 140 | Train Loss: 0.007 | Train Acc: 100.00% | Val Loss: 1.19 | Val Acc: 65.40% Epoch 150 | Train Loss: 0.008 | Train Acc: 100.00% | Val Loss: 1.14 | Val Acc: 66.80% Epoch 160 | Train Loss: 0.007 | Train Acc: 100.00% | Val Loss: 1.16 | Val Acc: 68.40% Epoch 170 | Train Loss: 0.008 | Train Acc: 100.00% | Val Loss: 1.11 | Val Acc: 68.20% Epoch 180 | Train Loss: 0.006 | Train Acc: 100.00% | Val Loss: 1.13 | Val Acc: 68.60% Epoch 190 | Train Loss: 0.005 | Train Acc: 100.00% | Val Loss: 1.14 | Val Acc: 68.60% Epoch 200 | Train Loss: 0.007 | Train Acc: 100.00% | Val Loss: 1.13 | Val Acc: 68.40% GAT test accuracy: 70.00% CPU times: user 53.4 s, sys: 2.68 s, total: 56.1 s Wall time: 55.9 s . This experiment is not super rigorous: we&#39;d need to repeat it $n$ times and report the average accuracy with a standard deviation as the final result. . In this example, we can see that the GAT outperforms the GCN in terms of accuracy (70.00% vs. 67.70%), but takes longer to train (55.9s vs. 32.4s). It&#39;s a tradeoff that can cause scalability issues when working with large graphs. . The authors obtained 72.5% for the GAT and 70.3% for the GCN, which is significantly better than what we got. The difference can be explained by additional preprocessing steps, some tweaks in the models, and a different training setting (e.g., a patience of 100 instead of a fixed number of epochs). We kept the code as simple as possible here, but feel free to modify it to improve the results. . Beyond the accuracy score, it is interesting to see what the GAT actually learned. We can visualize it with t-SNE plot, a powerful method to plot high-dimensional data in 2D or 3D. First, let&#39;s see what the embeddings looked like before any training: it should be random since they&#39;re produced by randomly initialized weight matrices. . untrained_gat = GAT(dataset.num_features, 8, dataset.num_classes) # Get embeddings h, _ = untrained_gat(data.x, data.edge_index) # Train TSNE tsne = TSNE(n_components=2, learning_rate=&#39;auto&#39;, init=&#39;pca&#39;).fit_transform(h.detach()) # Plot TSNE plt.figure(figsize=(10, 10)) plt.axis(&#39;off&#39;) plt.scatter(tsne[:, 0], tsne[:, 1], s=50, c=data.y) plt.show() . /usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence. FutureWarning, . Indeed, there&#39;s no apparent structure. But do the embeddings produced by our trained model look better? . h, _ = gat(data.x, data.edge_index) # Train TSNE tsne = TSNE(n_components=2, learning_rate=&#39;auto&#39;, init=&#39;pca&#39;).fit_transform(h.detach()) # Plot TSNE plt.figure(figsize=(10, 10)) plt.axis(&#39;off&#39;) plt.scatter(tsne[:, 0], tsne[:, 1], s=50, c=data.y) plt.show() . /usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence. FutureWarning, . The difference is noticeable: nodes belonging to the same classes cluster together. We can see six clusters, corresponding to the six classes of papers. There are outliers, but this was to be expected: our accuracy score is far from perfect. . Previously, I speculated that poorly connected nodes might negatively impact performance on CiteSeer. So let&#39;s verify that by calculating the model&#39;s accuracy for each degree. . from torch_geometric.utils import degree # Get model&#39;s classifications _, out = gat(data.x, data.edge_index) # Calculate the degree of each node degrees = degree(data.edge_index[0]).numpy() # Store accuracy scores and sample sizes accuracies = [] sizes = [] # Accuracy for degrees between 0 and 5 for i in range(0, 6): mask = np.where(degrees == i)[0] accuracies.append(accuracy(out.argmax(dim=1)[mask], data.y[mask])) sizes.append(len(mask)) # Accuracy for degrees &gt; 5 mask = np.where(degrees &gt; 5)[0] accuracies.append(accuracy(out.argmax(dim=1)[mask], data.y[mask])) sizes.append(len(mask)) # Bar plot fig, ax = plt.subplots(figsize=(18, 9)) ax.set_xlabel(&#39;Node degree&#39;) ax.set_ylabel(&#39;Accuracy score&#39;) ax.set_facecolor(&#39;#EFEEEA&#39;) plt.bar([&#39;0&#39;,&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;,&#39;&gt;5&#39;], accuracies, color=&#39;#0A047A&#39;) for i in range(0, 7): plt.text(i, accuracies[i], f&#39;{accuracies[i]*100:.2f}%&#39;, ha=&#39;center&#39;, color=&#39;#0A047A&#39;) for i in range(0, 7): plt.text(i, accuracies[i]//2, sizes[i], ha=&#39;center&#39;, color=&#39;white&#39;) . These results confirm our intuition: nodes with few neighbors are indeed harder to classify. This is due to the nature of GNNs: the more relevant connections you have, the more information you can aggregate. . Conclusion . While they take longer to train, GATs often provide a substantial improvement over GCNs in terms of accuracy. The self-attention mechanism automatically calculates weights instead of static coefficients to produce better embeddings. In this article, . We learned how to calculate dynamic weights using self-attention | We implemented and compared two architectures (a GCN and a GAT) in PyTorch Geometric | We visualized what the GAT learned with a t-SNE plot and the accuracy score for each degree | . GATs are a standard architecture in a lot of GNN applications. However, their slow training time can become a problem when applied to massive graph datasets. Scalability is an important factor in deep learning: more data can often lead to better performance. . In the next article, we&#39;ll see how to improve scalability with mini-batching and a new GNN architecture, called GraphSAGE. . If you enjoyed this tutorial, feel free to follow me on Twitter for more GNN content. Thank you, and see you in the next article! 📣 . &#127760; Graph Neural Network Course . 🔎 Course overview . 📝 Chapter 1: Introduction to Graph Neural Networks . 📝 Chapter 2: Graph Attention Network . 📝 Chapter 3: GraphSAGE . 📝 Chapter 4: Graph Isomorphism Network .",
            "url": "https://mlabonne.github.io/blog/gat/",
            "relUrl": "/gat/",
            "date": " • Mar 9, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Integer vs. Linear Programming in Python",
            "content": "Why is linear programming called that way? . Both terms are confusing: . Linear implies that nonlinear programming exists; | Programming actually means &quot;planning&quot; in this context. | . In summary, it has nothing to do with code: linear or not. It&#39;s about optimizing variables with various constraints. . In this article, we&#39;re gonna talk about another type of optimization: integer programming. We&#39;ll see why a good understanding of the problem we face is necessary to choose the right solver. Finally, we will write a model that can take on a bigger challenge and actually solve a whole class of optimization problems. . You can run the code from this tutorial with the following Google Colab notebook. . &#128202; I. Optimization problem&#160;types . In the introduction to linear programming, we optimized an army composition. Here was the result: . ================= Solution ================= Solved in 87.00 milliseconds in 2 iterations Optimal power = 1800.0 💪power Army: - 🗡️Swordsmen = 6.0000000000000036 - 🏹Bowmen = 0.0 - 🐎Horsemen = 5.999999999999999 . How can we have 5.999… horsemen? We specified that our variables should be integers with VarInt. What was wrong with our code? . The problem is not the model but the choice of the solver. . GLOP is a pure linear programming solver. This means that it cannot understand the concept of integers. It is limited to continuous parameters with a linear relationship. . This is the difference between linear programming (LP) and integer linear programming (ILP). In summary, LP solvers can only use real numbers and not integers as variables. So why did we declare our variables as integers if it doesn&#39;t take them into account? . GLOP cannot solve ILP problems, but other solvers can. Actually, a lot of them are mixed integer linear programming (MILP, commonly called MIP) solvers. This means that they can consider both continuous (real numbers) and discrete (integers) variables. A particular case of discrete values is Boolean variables to represent decisions with 0–1 values. . Other solvers like SCIP or CBC can solve both MILP and MINLP (mixed integer nonlinear programming) problems. Thanks to OR-Tools, we can use the same model and just change the solver to SCIP or CBC. . solver = pywraplp.Solver(&#39;Maximize army power&#39;, pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING) # 1. Create the variables we want to optimize swordsmen = solver.IntVar(0, solver.infinity(), &#39;swordsmen&#39;) bowmen = solver.IntVar(0, solver.infinity(), &#39;bowmen&#39;) horsemen = solver.IntVar(0, solver.infinity(), &#39;horsemen&#39;) # 2. Add constraints for each resource solver.Add(swordsmen*60 + bowmen*80 + horsemen*140 &lt;= 1200) solver.Add(swordsmen*20 + bowmen*10 &lt;= 800) solver.Add(bowmen*40 + horsemen*100 &lt;= 600) # 3. Maximize the objective function solver.Maximize(swordsmen*70 + bowmen*95 + horsemen*230) # Solve problem status = solver.Solve() # If an optimal solution has been found, print results if status == pywraplp.Solver.OPTIMAL: print(&#39;================= Solution =================&#39;) print(f&#39;Solved in {solver.wall_time():.2f} milliseconds in {solver.iterations()} iterations&#39;) print() print(f&#39;Optimal value = {solver.Objective().Value()} 💪power&#39;) print(&#39;Army:&#39;) print(f&#39; - 🗡️Swordsmen = {swordsmen.solution_value()}&#39;) print(f&#39; - 🏹Bowmen = {bowmen.solution_value()}&#39;) print(f&#39; - 🐎Horsemen = {horsemen.solution_value()}&#39;) else: print(&#39;The solver could not find an optimal solution.&#39;) . ================= Solution ================= Solved in 3.00 milliseconds in 0 iterations Optimal value = 1800.0 💪power Army: - 🗡️Swordsmen = 6.0 - 🏹Bowmen = 0.0 - 🐎Horsemen = 6.0 . Strictly speaking, our variables are still floats (type(swordsmen.solution_value()) = float) but we can see that they don&#39;t have weird decimals anymore: the CBC solver really considered them as integers. . In this example, we would generally just round up these values since the error is insignificant. However, it is important to remember to choose the appropriate solver according to the studied problem: . LP for continuous variables; | MIP/MILP for a combination of continuous and discrete variables. | . There are other types such as quadratic (QP) or nonlinear (NLP or MINLP, with an exponential objective function or constraints for instance) problems. They&#39;re applied in different contexts, but follow the same principles as LP or MIP solvers. . &#129521; II. Building a general model . But what if our resources change? Or if the cost of a unit evolved? What if we upgraded horsemen and their power increased? . One of the best perks of OR-Tools is that it uses a general-purpose programming language like Python. Instead of static numbers, we can store our parameters in objects like dictionaries or lists. . The code won&#39;t be as readable, but it becomes much more flexible: actually, it can be so flexible that we can solve an entire class of optimization problems without changing the model (just the parameters). . Let&#39;s transform our input parameters into Python lists and feed them to the solver through a function. . UNITS = [&#39;🗡️Swordsmen&#39;, &#39;🏹Bowmen&#39;, &#39;🐎Horsemen&#39;] DATA = [[60, 20, 0, 70], [80, 10, 40, 95], [140, 0, 100, 230]] RESOURCES = [1200, 800, 600] def solve_army(UNITS, DATA, RESOURCES): # Create the linear solver using the CBC backend solver = pywraplp.Solver(&#39;Maximize army power&#39;, pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING) # 1. Create the variables we want to optimize units = [solver.IntVar(0, solver.infinity(), unit) for unit in UNITS] # 2. Add constraints for each resource for r, _ in enumerate(RESOURCES): solver.Add(sum(DATA[u][r] * units[u] for u, _ in enumerate(units)) &lt;= RESOURCES[r]) # 3. Maximize the objective function solver.Maximize(sum(DATA[u][-1] * units[u] for u, _ in enumerate(units))) # Solve problem status = solver.Solve() # If an optimal solution has been found, print results if status == pywraplp.Solver.OPTIMAL: print(&#39;================= Solution =================&#39;) print(f&#39;Solved in {solver.wall_time():.2f} milliseconds in {solver.iterations()} iterations&#39;) print() print(f&#39;Optimal value = {solver.Objective().Value()} 💪power&#39;) print(&#39;Army:&#39;) for u, _ in enumerate(units): print(f&#39; - {units[u].name()} = {units[u].solution_value()}&#39;) else: print(&#39;The solver could not find an optimal solution.&#39;) solve_army(UNITS, DATA, RESOURCES) . ================= Solution ================= Solved in 2.00 milliseconds in 0 iterations Optimal value = 1800.0 💪power Army: - 🗡️Swordsmen = 6.0 - 🏹Bowmen = 0.0 - 🐎Horsemen = 6.0 . We obtain the same results: our code seems to work. Now let&#39;s change the parameters to tackle a slightly more complex problem. . Imagine we have a lot more resources: 🌾183000, 🪵90512, and 🪙80150, so we can also produce a lot more units! This is the new table: . Unit 🌾Food 🪵Wood 🪙Gold 💪Attack ❤️Health . 🗡️Swordsman | 60 | 20 | 0 | 6 | 70 | . 🛡️Man-at-arms | 100 | 0 | 20 | 12 | 155 | . 🏹Bowman | 30 | 50 | 0 | 5 | 70 | . ❌Crossbowman | 80 | 0 | 40 | 12 | 80 | . 🔫Handcannoneer | 120 | 0 | 120 | 35 | 150 | . 🐎Horseman | 100 | 20 | 0 | 9 | 125 | . ♞Knight | 140 | 0 | 100 | 24 | 230 | . 🐏Battering ram | 0 | 300 | 0 | 200 | 700 | . 🎯Springald | 0 | 250 | 250 | 30 | 200 | . Notice that we transformed the 💪power into two values: 💪attack and ❤️health, which is a little more detailed. Health values are higher than attack values, which is why we want to add a weighting factor to make them more comparable. . Let&#39;s take 10 as an example, so $power = 10 times attack + health$. Our objective function becomes: . $$maximize sum_{u in units} (10 times attack + health) cdot u$$ . Adapting our code to this new problem is actually quite simple: we just have to change the input parameters and update the objective function. . UNITS = [ &#39;🗡️Swordsmen&#39;, &#39;🛡️Men-at-arms&#39;, &#39;🏹Bowmen&#39;, &#39;❌Crossbowmen&#39;, &#39;🔫Handcannoneers&#39;, &#39;🐎Horsemen&#39;, &#39;♞Knights&#39;, &#39;🐏Battering rams&#39;, &#39;🎯Springalds&#39;, &#39;🪨Mangonels&#39;, ] DATA = [ [60, 20, 0, 6, 70], [100, 0, 20, 12, 155], [30, 50, 0, 5, 70], [80, 0, 40, 12, 80], [120, 0, 120, 35, 150], [100, 20, 0, 9, 125], [140, 0, 100, 24, 230], [0, 300, 0, 200, 700], [0, 250, 250, 30, 200], [0, 400, 200, 12*3, 240] ] RESOURCES = [183000, 90512, 80150] def solve_army(UNITS, DATA, RESOURCES): # Create the linear solver using the CBC backend solver = pywraplp.Solver(&#39;Maximize army power&#39;, pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING) # 1. Create the variables we want to optimize units = [solver.IntVar(0, solver.infinity(), unit) for unit in UNITS] # 2. Add constraints for each resource for r, _ in enumerate(RESOURCES): solver.Add(sum(DATA[u][r] * units[u] for u, _ in enumerate(units)) &lt;= RESOURCES[r]) # 3. Maximize the new objective function solver.Maximize(sum((10*DATA[u][-2] + DATA[u][-1]) * units[u] for u, _ in enumerate(units))) # Solve problem status = solver.Solve() # If an optimal solution has been found, print results if status == pywraplp.Solver.OPTIMAL: print(&#39;================= Solution =================&#39;) print(f&#39;Solved in {solver.wall_time():.2f} milliseconds in {solver.iterations()} iterations&#39;) print() print(f&#39;Optimal value = {solver.Objective().Value()} 💪power&#39;) print(&#39;Army:&#39;) for u, _ in enumerate(units): print(f&#39; - {units[u].name()} = {units[u].solution_value()}&#39;) else: print(&#39;The solver could not find an optimal solution.&#39;) solve_army(UNITS, DATA, RESOURCES) . ================= Solution ================= Solved in 74.00 milliseconds in 412 iterations Optimal value = 1393145.0 💪power Army: - 🗡️Swordsmen = 2.0 - 🛡️Men-at-arms = 1283.0 - 🏹Bowmen = 3.0 - ❌Crossbowmen = 0.0 - 🔫Handcannoneers = 454.0 - 🐎Horsemen = 0.0 - ♞Knights = 0.0 - 🐏Battering rams = 301.0 - 🎯Springalds = 0.0 - 🪨Mangonels = 0.0 . This problem would take a long time for humans to address, but the ILP solver did it in the blink of an eye. Better than that: it also gives us the guarantee that our solution is optimal, which means that our enemy cannot find a better army composition for the same cost! . We could increase the number of units and give billions of resources but you get the picture: it would just take longer to obtain a solution, but it wouldn&#39;t change the problem. . &#9876;&#65039; III. Combining constraints . Now, let&#39;s say we scouted our enemy and know that their army has a 💪power of 1,000,000. We could build a much better army, but our resources are precious and it wouldn&#39;t be very efficient: all we have to do is to build an army with a 💪power higher than 1,000,000 (even 1,000,001 would be enough). . In other words, the total power is now a constraint (💪 &gt; 1,000,000) instead of the objective to maximize. The new goal is to minimize the resources we need to produce this army. However, we can reuse our input parameters since they didn&#39;t change. . The new constraint can be translated as &quot;the sum of the power of the selected units must be strictly greater than 1,000,000&quot;. . $$ sum_{u in units} (10 times attack + health) cdot u &gt; 1 ,000 ,000$$ . In code, we can loop through our units and resources to design this constraint. . The objective function also has to change. Our goal is to minimize the sum of resources spent to build the army. . $$minimize sum_{u in units} (food + wood + gold) cdot u$$ . Once again, we can loop through our resources to implement it in OR-Tools. . def solve_army(UNITS, DATA, RESOURCES): # Create the linear solver using the CBC backend solver = pywraplp.Solver(&#39;Minimize resource consumption&#39;, pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING) # 1. Create the variables we want to optimize units = [solver.IntVar(0, solver.infinity(), unit) for unit in UNITS] # 2. Add constraints for each resource for r, _ in enumerate(RESOURCES): solver.Add(sum((10 * DATA[u][-2] + DATA[u][-1]) * units[u] for u, _ in enumerate(units)) &gt;= 1000001) # 3. Minimize the objective function solver.Minimize(sum((DATA[u][0] + DATA[u][1] + DATA[u][2]) * units[u] for u, _ in enumerate(units))) # Solve problem status = solver.Solve() # If an optimal solution has been found, print results if status == pywraplp.Solver.OPTIMAL: print(&#39;================= Solution =================&#39;) print(f&#39;Solved in {solver.wall_time():.2f} milliseconds in {solver.iterations()} iterations&#39;) print() power = sum((10 * DATA[u][-2] + DATA[u][-1]) * units[u].solution_value() for u, _ in enumerate(units)) print(f&#39;Optimal value = {solver.Objective().Value()} 🌾🪵🪙resources&#39;) print(f&#39;Power = 💪{power}&#39;) print(&#39;Army:&#39;) for u, _ in enumerate(units): print(f&#39; - {units[u].name()} = {units[u].solution_value()}&#39;) print() food = sum((DATA[u][0]) * units[u].solution_value() for u, _ in enumerate(units)) wood = sum((DATA[u][1]) * units[u].solution_value() for u, _ in enumerate(units)) gold = sum((DATA[u][2]) * units[u].solution_value() for u, _ in enumerate(units)) print(&#39;Resources:&#39;) print(f&#39; - 🌾Food = {food}&#39;) print(f&#39; - 🪵Wood = {wood}&#39;) print(f&#39; - 🪙Gold = {gold}&#39;) else: print(&#39;The solver could not find an optimal solution.&#39;) solve_army(UNITS, DATA, RESOURCES) . ================= Solution ================= Solved in 4.00 milliseconds in 0 iterations Optimal value = 111300.0 🌾🪵🪙resources Power = 💪1001700.0 Army: - 🗡️Swordsmen = 0.0 - 🛡️Men-at-arms = 0.0 - 🏹Bowmen = 0.0 - ❌Crossbowmen = 0.0 - 🔫Handcannoneers = 0.0 - 🐎Horsemen = 0.0 - ♞Knights = 0.0 - 🐏Battering rams = 371.0 - 🎯Springalds = 0.0 - 🪨Mangonels = 0.0 Resources: - 🌾Food = 0.0 - 🪵Wood = 111300.0 - 🪙Gold = 0.0 . The solver found an optimal solution: we need to build 371 🐏battering rams for a total cost of 111,300 🪵wood. Wait, what if we don&#39;t have that much wood? In the previous section, we only had 🪵90512: we cannot produce 371 🐏battering rams. 😱 . So is it possible to take these limited resources into account and still try to build the best army? Actually, it&#39;s super easy: we just have to copy/paste the constraints from the previous section. . In this version, we have two types of constraints: . The total power must be greater than 1,000,000; | We cannot spend more than our limited resources. | . def solve_army(UNITS, DATA, RESOURCES): # Create the linear solver using the CBC backend solver = pywraplp.Solver(&#39;Minimize resource consumption&#39;, pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING) # 1. Create the variables we want to optimize units = [solver.IntVar(0, solver.infinity(), unit) for unit in UNITS] # 2. Add constraints for each resource for r, _ in enumerate(RESOURCES): solver.Add(sum((10 * DATA[u][-2] + DATA[u][-1]) * units[u] for u, _ in enumerate(units)) &gt;= 1000001) # Old constraints for limited resources for r, _ in enumerate(RESOURCES): solver.Add(sum(DATA[u][r] * units[u] for u, _ in enumerate(units)) &lt;= RESOURCES[r]) # 3. Minimize the objective function solver.Minimize(sum((DATA[u][0] + DATA[u][1] + DATA[u][2]) * units[u] for u, _ in enumerate(units))) # Solve problem status = solver.Solve() # If an optimal solution has been found, print results if status == pywraplp.Solver.OPTIMAL: print(&#39;================= Solution =================&#39;) print(f&#39;Solved in {solver.wall_time():.2f} milliseconds in {solver.iterations()} iterations&#39;) print() power = sum((10 * DATA[u][-2] + DATA[u][-1]) * units[u].solution_value() for u, _ in enumerate(units)) print(f&#39;Optimal value = {solver.Objective().Value()} 🌾🪵🪙resources&#39;) print(f&#39;Power = 💪{power}&#39;) print(&#39;Army:&#39;) for u, _ in enumerate(units): print(f&#39; - {units[u].name()} = {units[u].solution_value()}&#39;) print() food = sum((DATA[u][0]) * units[u].solution_value() for u, _ in enumerate(units)) wood = sum((DATA[u][1]) * units[u].solution_value() for u, _ in enumerate(units)) gold = sum((DATA[u][2]) * units[u].solution_value() for u, _ in enumerate(units)) print(&#39;Resources:&#39;) print(f&#39; - 🌾Food = {food}&#39;) print(f&#39; - 🪵Wood = {wood}&#39;) print(f&#39; - 🪙Gold = {gold}&#39;) else: print(&#39;The solver could not find an optimal solution.&#39;) solve_army(UNITS, DATA, RESOURCES) . ================= Solution ================= Solved in 28.00 milliseconds in 1 iterations Optimal value = 172100.0 🌾🪵🪙resources Power = 💪1000105.0 Army: - 🗡️Swordsmen = 1.0 - 🛡️Men-at-arms = 681.0 - 🏹Bowmen = 0.0 - ❌Crossbowmen = 0.0 - 🔫Handcannoneers = 0.0 - 🐎Horsemen = 0.0 - ♞Knights = 0.0 - 🐏Battering rams = 301.0 - 🎯Springalds = 0.0 - 🪨Mangonels = 0.0 Resources: - 🌾Food = 68160.0 - 🪵Wood = 90320.0 - 🪙Gold = 13620.0 . Since we now have a limited resource of 🪵wood, the number of 🐏battering rams sadly dropped from 371 to 301. In exchange, we got 681 🛡️men-at-arms and 1 lost 🗡️swordsman (welcome to them). . The total cost of the army is 172,100, which is much higher than the 111,300 we previously found (+65% increase) but it truly is the optimal solution under these constraints. It shows that we should produce more wood because these 🐏 battering rams are extremely cost-efficient! . This example shows how modular LP models can be. It is possible to reuse parts of the code, like constraints, in another model to combine them and solve more complex problems. . &#129504; IV. Linear Programming vs Machine&#160;Learning . Let&#39;s talk about the elephant in the room. Why not use machine learning (in a broad sense) instead of linear programming? It&#39;s not like this problem cannot be solved with a genetic algorithm for instance. . Mathematical optimization is often neglected in favor of machine learning techniques, but both have their merits: . Linear programming can produce an optimal solution in an undetermined amount of time (it can take years), while machine learning can approximate complex functions in no time. | There is no training in LP, but an expert is required to build a mathematical model. Machine learning needs data, but the models can be used as black boxes to solve a problem. | As a rule of thumb, problems that do not have a particular time constraint and/or are not extremely complex can be advantageously solved with linear programming. | . Conclusion . In this tutorial, we dived deeper into our understanding of mathematical optimization. . We talked about solvers and types of optimization problems: LP, MIP, NLP; | We modeled and solved an extremely common optimization problem in an optimal way and generalized our model through a function; | We reframed this problem and merged two sets of constraints to obtain the best army composition for the lowest price; | We compared the pros and cons of linear programming and machine learning. | . There are a lot more problems where optimization can be applied. For instance, how to create school timetables that satisfy everybody&#39;s requirements? How to deliver 1,000 different orders in a minimum amount of time? Where to create a new metro line to maximize its usefulness? . In future articles, we&#39;ll talk about new types of applications for these techniques, including satisfiability and nonlinear problems. . I hope you enjoyed this more advanced article. If you like machine learning and optimization, let&#39;s connect on Twitter! . &#129351; Linear Programming Course . 🔎 Course overview . 📝 Chapter 1: Introduction to Linear Programming . 📝 Chapter 2: Integer vs. Linear Programming . 📝 Chapter 3: Constraint Programming .",
            "url": "https://mlabonne.github.io/blog/integerprogramming/",
            "relUrl": "/integerprogramming/",
            "date": " • Mar 5, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Introduction to Linear Programming in Python",
            "content": "Linear programming is a technique to optimize any problem with multiple variables and constraints. It&#39;s a simple but powerful tool every data scientist should master. . Imagine you are a strategist recruiting an army. You have: . Three resources: 🌾food, 🪵wood, and 🪙gold | Three units: 🗡️swordsmen, 🏹bowmen, and 🐎horsemen. | . Horsemen are stronger than bowmen, who are in turn stronger than swordsmen. The following table provides the cost and power of each unit: . Unit 🌾Food 🪵Wood 🪙Gold 💪Power . 🗡️Swordsman | 60 | 20 | 0 | 70 | . 🏹Bowman | 80 | 10 | 40 | 95 | . 🐎Horseman | 140 | 0 | 100 | 230 | . Now we have 1200 🌾food, 800 🪵wood, and 600 🪙gold. How should we maximize the power of our army considering these resources? . We could simply find the unit with the best power/cost ratio, take as many of them as possible, and repeat the process with the other two units. But this &quot;guess and check&quot; solution might not even be optimal... . Now imagine we have millions of units and resources: the previous greedy strategy is likely to completely miss the optimal solution. It is possible to use a machine learning algorithm (e.g., a genetic algorithm) to solve this problem, but we have no guarantee that the solution will be optimal either. . Fortunately for us, there is a method that can solve our problem in an optimal way: linear programming (or linear optimization), which is part of the field of operations research (OR). In this article, we&#39;ll use it to find the best numbers of swordsmen, bowmen, and horsemen to build the army with the highest power possible. . You can run the code from this tutorial with the following Google Colab notebook. . &#129504; I.&#160;Solvers . In Python, there are different libraries for linear programming such as the multi-purposed SciPy, the beginner-friendly PuLP, the exhaustive Pyomo, and many others. . Today, we are going to use Google OR-Tools, which is quite user-friendly, comes with several prepackaged solvers, and has by far the most stars on GitHub. . If the installation doesn&#39;t work, please restart the kernel and try again: it can fail sometimes. ¯ _(ツ)_/¯ . !python -m pip install --upgrade --user -q ortools . All these libraries have a hidden benefit: they act as interfaces to use the same model with different solvers. Solvers like Gurobi, Cplex, or SCIP have their own APIs, but the models they create are tied to a specific solver. . OR-Tools allows us to use an abstract (and quite pythonic) way of modeling our problems. We can then choose one or several solvers to find an optimal solution. The model we built is thus highly reusable! . &lt;/source&gt; OR-Tools comes with its own linear programming solver, called GLOP (Google Linear Optimization Package). It is an open-source project created by Google&#39;s Operations Research Team and written in C++. . Other solvers are available such as SCIP, an excellent non-commercial solver created in 2005 and updated and maintained to this day. We could also use popular commercial options like Gurobi and Cplex. However, we would need to install them on top of OR-Tools and get the appropriate licenses (which can be quite costly). For now, let&#39;s try GLOP. . from ortools.linear_solver import pywraplp # Create a solver using the GLOP backend solver = pywraplp.Solver(&#39;Maximize army power&#39;, pywraplp.Solver.GLOP_LINEAR_PROGRAMMING) . &#129518; II. Variables . We created an instance of the OR-Tools solver using GLOP. Now, how to use linear programming? The first thing we want to define is the variables we want to optimize. . In our example, we have three variables: the number of 🗡️swordsmen, 🏹bowmen, and 🐎horsemen in the army. OR-Tools accepts three types of variables: . NumVar for continuous variables; | IntVar for integer variables; | BoolVar for boolean variables. | . We&#39;re looking for round numbers of units, so let&#39;s choose IntVar. We then need to specify lower and upper bounds for these variables. We want at least 0 unit, but we don&#39;t really have an upper bound. So we can say that our upper bound is infinity (or any big number we will never reach). It can be written as: . $$0 leq swordsmen &lt; infty 0 leq bowmen &lt; infty 0 leq horsemen &lt; infty$$Let&#39;s translate it into code. Infinity is replaced by solver.infinity() in OR-Tools. Other than that, the syntax is quite straightforward: . swordsmen = solver.IntVar(0, solver.infinity(), &#39;swordsmen&#39;) bowmen = solver.IntVar(0, solver.infinity(), &#39;bowmen&#39;) horsemen = solver.IntVar(0, solver.infinity(), &#39;horsemen&#39;) . &#9939;&#65039; III. Constraints . We defined our variables, but the constraints are just as important. . Perhaps counter-intuitively, adding more constraints helps the solver to find an optimal solution faster. Why is this the case? Think of the solver as a tree: constraints help it trim branches and reduce the search space. . In our case, we have a limited number of resources we can use to produce units. In other words, we can&#39;t spend more resources than we have. For instance, the 🌾food spent to recruit units cannot be higher than 1200. The same is true with 🪵wood (800) and 🪙gold (600). . According to our table, units have the following costs: . 1 swordsman = 🌾60 + 🪵20; | 1 bowman = 🌾80 + 🪵10 + 🪙40; | 1 horseman = 🌾140 + 🪙100. | . We can write one constraint per resource as follows: . $$60 times swordsmen + 80 times bowmen + 140 times horsemen leq 1200 20 times swordsmen + 10 times bowmen leq 800 40 times bowmen + 100 times horsemen leq 600$$In OR-Tools, we simply add the constraints to our solver instance with solver.Add(). . solver.Add(swordsmen*60 + bowmen*80 + horsemen*140 &lt;= 1200) # Food solver.Add(swordsmen*20 + bowmen*10 &lt;= 800) # Wood solver.Add(bowmen*40 + horsemen*100 &lt;= 600) # Gold . &#127919; IV. Objective . Now that we have our variables and constraints, we want to define our goal (or objective function). . In linear programming, this function has to be linear (like the constraints), so of the form $ax + by + cz + d$. In our example, the objective is quite clear: we want to recruit the army with the highest power. The table gives us the following power values: . 1 swordsman = 💪70; | 1 bowman = 💪95; | 1 horseman = 💪230. | . Maximizing the power of the army amounts to maximizing the sum of the power of each unit. Our objective function can be written as: . $$max 70 times swordsmen + 95 times bowmen + 230 times horsemen$$ . In general, there are two types of objective functions: maximizing and minimizing. In OR-Tools, we declare this goal with solver.Maximize() or solver.Minimize(). . solver.Maximize(swordsmen*70 + bowmen*95 + horsemen*230) . And we&#39;re done! There are three steps to model any linear optimization problem: . Declaring the variables to optimize with lower and upper bounds; | Adding constraints to these variables; | Defining the objective function to maximize or to minimize. | Now that is clear, we can ask the solver to find an optimal solution for us. . &#129351; V. Optimize! . Calculating the optimal solution is done with solver.Solve(). This function returns a status that can be used to check that the solution is indeed optimal. . Let&#39;s print the highest total power we can get with the best army configuration. . status = solver.Solve() # If an optimal solution has been found, print results if status == pywraplp.Solver.OPTIMAL: print(&#39;================= Solution =================&#39;) print(f&#39;Solved in {solver.wall_time():.2f} milliseconds in {solver.iterations()} iterations&#39;) print() print(f&#39;Optimal power = {solver.Objective().Value()} 💪power&#39;) print(&#39;Army:&#39;) print(f&#39; - 🗡️Swordsmen = {swordsmen.solution_value()}&#39;) print(f&#39; - 🏹Bowmen = {bowmen.solution_value()}&#39;) print(f&#39; - 🐎Horsemen = {horsemen.solution_value()}&#39;) else: print(&#39;The solver could not find an optimal solution.&#39;) . ================= Solution ================= Solved in 87.00 milliseconds in 2 iterations Optimal power = 1800.0 💪power Army: - 🗡️Swordsmen = 6.0000000000000036 - 🏹Bowmen = 0.0 - 🐎Horsemen = 5.999999999999999 . Great! The solver found an optimal solution: our army has a total power of 💪1800 with 6 🗡️swordsmen and 6 🐎horsemen (sorry bowmen!). . Let&#39;s unpack this result: . The solver decided to take the maximum number of 🐎horsemen (6, since we only have 🪙600 and they each cost 🪙100); | The remaining resources are spent in 🗡️swordsmen: we have $1200 – 6*140 = 360$🌾food left, which is why the solver chose 6 🗡️swordsmen; | We can deduce that the horsemen are the best unit and the bowmen are the worst one because they haven&#39;t been chosen at all. | . Okay, but there&#39;s something quite weird: these numbers are not round, even though we specified that we wanted integers (IntVar). So what happened? . Unfortunately, answering this question requires a deep dive into linear programming… To keep things simple in this introduction, let&#39;s say it&#39;s because of GLOP. Solvers have characteristics we have to take into account, and GLOP doesn&#39;t handle integers. This is another proof that building reusable models is more than just convenient. . We&#39;ll explain why GLOP has this strange behavior and how to fix it in a more advanced tutorial. . Conclusion . We saw through this example the five main steps of any linear optimization problem: . Choosing a solver: in our case, we selected GLOP for convenience. | Declaring variables: the parameters to optimize were the number of swordsmen, bowmen, and horsemen. | Declaring constraints: each of these units has a cost. The total cost could not exceed our limited resources. | Defining objective: the criterion to maximize was the total power of this army. It could have been something else, like the number of units. | Optimizing: GLOP found an optimal solution to this problem in less than a second. | This is the main benefit of linear programming: the algorithm gives us a guarantee that the solution that was found is optimal (with a certain error). This guarantee is powerful, but comes at a cost: the model can be so complex that the solver takes years (or more) to find an optimal solution. In this scenario, we have two options: . We can stop the solver after a certain time (and probably obtain a suboptimal answer); | We can use a metaheuristic like a genetic algorithm to calculate an excellent solution in a short amount of time. | . In the next article, we&#39;ll talk about the different types of optimization problems and generalize our approach to an entire class of them. . I hope you enjoyed this introduction! Feel free to share it and spread the knowledge about linear optimization. Let&#39;s connect on Twitter where I post summaries of these articles. Cheers! . &#129351; Linear Programming Course . 🔎 Course overview . 📝 Chapter 1: Introduction to Linear Programming . 📝 Chapter 2: Integer vs. Linear Programming . 📝 Chapter 3: Constraint Programming .",
            "url": "https://mlabonne.github.io/blog/linearoptimization/",
            "relUrl": "/linearoptimization/",
            "date": " • Mar 2, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Graph Convolutional Networks: Introduction to GNNs",
            "content": "Graph Neural Networks (GNNs) are one of the most interesting and fast-growing architectures in deep learning. . In this series of tutorials, I would like to give a practical overview of this field and present new applications for machine learning practictioners. . Among GNNs, the Graph Convolutional Networks (GCNs) are the most popular and widely-applied model. In this article, we will see how the GCN layer works and how to apply it to node classification using PyTorch Geometric. . PyTorch Geometric is an extension of PyTorch dedicated to GNNs. To install it, we need PyTorch (already installed on Google Colab) and run the following commands. If the installation does not work for you, please check PyTorch Geometric&#39;s documentation. . import torch !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git # Numpy for matrices import numpy as np # Visualization libraries import matplotlib.pyplot as plt import networkx as nx . Now that PyTorch Geometric is installed, let&#39;s explore the dataset we will use in this tutorial. . &#127760; I. Graph data . Graphs are a nonlinear type of data you can find everywhere: social networks, computer networks, molecules, text, images, and so on. In this article, we will study the infamous and much-used Zachary&#39;s karate club dataset. . Zachary&#39;s karate club represents the relationships within a karate club studied by Wayne W. Zachary in the 1970s. It is a kind of social network, where every node is a member, and members who interacted outside the club are connected to each other. . In this example, the club is divided into four groups: we would like to assign the right group to every member (node classification) just by looking at their connections. . Let&#39;s import the dataset with PyG&#39;s built-in function and try to understand the Datasets object it uses. . from torch_geometric.datasets import KarateClub # Import dataset from PyTorch Geometric dataset = KarateClub() # Print information print(dataset) print(&#39;&#39;) print(f&#39;Number of graphs: {len(dataset)}&#39;) print(f&#39;Number of features: {dataset.num_features}&#39;) print(f&#39;Number of classes: {dataset.num_classes}&#39;) . KarateClub() Number of graphs: 1 Number of features: 34 Number of classes: 4 . This dataset only has 1 graph, where each node has a feature vector of 34 dimensions and is part of one out of four classes (our four groups). Actually, the Datasets object can be seen as a collection of Data (graph) objects. . We can further inspect our unique graph to know more about it. . print(f&#39;Graph: {dataset[0]}&#39;) . Graph: Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34]) . The Data object is particularly interesting. Printing it offers a good summary of the graph we&#39;re studying: . x=[34, 34] is the node feature matrix with shape (number of nodes, number of features). In our case, it means that we have 34 nodes (our 34 members), each node being associated to a 34-dim feature vector. | edge_index=[2, 156] represents the graph connectivity (how the nodes are connected) with shape (2, number of directed edges). | y=[34] is the node ground-truth labels. In this problem, every node is assigned to one class (group), so we have one value for each node. | train_mask=[34] is an optional attribute that tells which nodes should be used for training with a list of True or False statements. | . Let&#39;s print each of these tensors to understand what they store. Let&#39;s start with the node features. . data = dataset[0] print(f&#39;x = {data.x.shape}&#39;) print(data.x) . x = torch.Size([34, 34]) tensor([[1., 0., 0., ..., 0., 0., 0.], [0., 1., 0., ..., 0., 0., 0.], [0., 0., 1., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 1., 0., 0.], [0., 0., 0., ..., 0., 1., 0.], [0., 0., 0., ..., 0., 0., 1.]]) . Here, the node feature matrix x is an identity matrix: it doesn&#39;t contain any relevant information about the nodes. It could contain information like age, skill level, etc. but this is not the case in this dataset. It means we&#39;ll have to classify our nodes just by looking at their connections. . Now, let&#39;s print the edge index. . print(f&#39;edge_index = {data.edge_index.shape}&#39;) print(data.edge_index) . edge_index = torch.Size([2, 156]) tensor([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 8, 9, 9, 10, 10, 10, 11, 12, 12, 13, 13, 13, 13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 19, 20, 20, 21, 21, 22, 22, 23, 23, 23, 23, 23, 24, 24, 24, 25, 25, 25, 26, 26, 27, 27, 27, 27, 28, 28, 28, 29, 29, 29, 29, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33], [ 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 17, 19, 21, 31, 0, 2, 3, 7, 13, 17, 19, 21, 30, 0, 1, 3, 7, 8, 9, 13, 27, 28, 32, 0, 1, 2, 7, 12, 13, 0, 6, 10, 0, 6, 10, 16, 0, 4, 5, 16, 0, 1, 2, 3, 0, 2, 30, 32, 33, 2, 33, 0, 4, 5, 0, 0, 3, 0, 1, 2, 3, 33, 32, 33, 32, 33, 5, 6, 0, 1, 32, 33, 0, 1, 33, 32, 33, 0, 1, 32, 33, 25, 27, 29, 32, 33, 25, 27, 31, 23, 24, 31, 29, 33, 2, 23, 24, 33, 2, 31, 33, 23, 26, 32, 33, 1, 8, 32, 33, 0, 24, 25, 28, 32, 33, 2, 8, 14, 15, 18, 20, 22, 23, 29, 30, 31, 33, 8, 9, 13, 14, 15, 18, 19, 20, 22, 23, 26, 27, 28, 29, 30, 31, 32]]) . The edge_index has a quite counter-intuitive way of storing the graph connectivity. Here, we have two lists of 156 directed edges (78 bidirectional edges) because the first list contains the sources and the second one the destinations. It is called a coordinate list (COO) and is just one way of efficiently storing a sparse matrix. . A more intuitive way to represent the graph connectivity would be a simple adjacency matrix $A$, where a non-zero element $A_{ij}$ indicates a connection from $i$ to $j$. . The adjacency matrix can be inferred from the edge_index with a utility function. . from torch_geometric.utils import to_dense_adj A = to_dense_adj(data.edge_index)[0].numpy().astype(int) print(f&#39;A = {A.shape}&#39;) print(A) . A = (34, 34) [[0 1 1 ... 1 0 0] [1 0 1 ... 0 0 0] [1 1 0 ... 0 1 0] ... [1 0 0 ... 0 1 1] [0 0 1 ... 1 0 1] [0 0 0 ... 1 1 0]] . With graph data, nodes are rarely highly interconnected. For example, our adjacency matrix $A$ is very sparse (filled with zeros). Storing so many zeros is not efficient at all, which is why the COO format is adopted by PyG. . On the contrary, ground-truth labels are easy to understand. . print(f&#39;y = {data.y.shape}&#39;) print(data.y) . y = torch.Size([34]) tensor([1, 1, 1, 1, 3, 3, 3, 1, 0, 1, 3, 1, 1, 1, 0, 0, 3, 1, 0, 1, 0, 1, 0, 0, 2, 2, 0, 0, 2, 0, 0, 2, 0, 0]) . Our node ground-truth labels stored in y simply encode the group number (0, 1, 2, 3) for each node, which is why we have 34 values. . Finally, let&#39;s print the train mask. . print(f&#39;train_mask = {data.train_mask.shape}&#39;) print(data.train_mask) . train_mask = torch.Size([34]) tensor([ True, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False]) . The train mask shows which nodes are supposed to be used for training with True statements. These nodes represent the training set, while the others can be considered as the test set. . But we&#39;re not done yet! The Data object has a lot more to offer: many graph properties can be checked using utility functions. For example: . is_directed() tells you if the graph is directed, which means that the adjacency matrix is not symmetric | isolated_nodes() checks if some nodes are not connected to the rest of the graph (and will probably be harder to classify) | has_self_loops() indicates if at least one node is connected to itself. This is not the same as loops: loops mean that you can take a path that starts and ends at the same node. | . All of these properties return False for Zachary&#39;s karate club. . print(f&#39;Edges are directed: {data.is_directed()}&#39;) print(f&#39;Graph has isolated nodes: {data.has_isolated_nodes()}&#39;) print(f&#39;Graph has loops: {data.has_self_loops()}&#39;) . Edges are directed: False Graph has isolated nodes: False Graph has loops: False . Finally, we can convert a graph from PyTorch Geometric to the popular graph library NetworkX using to_networkx. This is particularly useful to visualize a small graph with NetworkX and Matplotlib. . Let&#39;s plot our dataset with a different color for each group. . from torch_geometric.utils import to_networkx G = to_networkx(data, to_undirected=True) plt.figure(figsize=(12,12)) plt.axis(&#39;off&#39;) nx.draw_networkx(G, pos=nx.spring_layout(G, seed=0), with_labels=True, node_size=800, node_color=data.y, cmap=&quot;hsv&quot;, vmin=-2, vmax=3, width=0.8, edge_color=&quot;grey&quot;, font_size=14 ) plt.show() . This plot of Zachary&#39;s karate club displays our 34 nodes, 78 (bidirectional) edges, and 4 labels with 4 different colors. Now that we&#39;ve seen the essentials of loading and handling a dataset with PyTorch Geometric, we can introduce the Graph Convolutional Network. . &#9993;&#65039; II. Graph Convolutional Network . In this section, let&#39;s try to redesign the graph convolutional layer from scratch. . In neural networks, linear layers apply a linear transformation to the incoming data. They transform input features $x$ into hidden vectors $h$ using a weight matrix $W$. If we ignore biases, we can write: . $$h = mathbf{W} x$$ . With graph data, we have access to connections between nodes. Why is that relevant? In most networks, we make the hypothesis that similar nodes are more likely to be connected to each other than dissimilar ones (it&#39;s called network homophily). . We can enrich our node representation by aggregating its features with those of its neighbors. This operation is called convolution, or neighborhood aggregation. Let&#39;s denote $ tilde{ mathcal{N}}_i$ the neighborhood of node $i$ including itself. . $$h_i = sum_{j in tilde{ mathcal{N}}_i} mathbf{W} x_j$$ . Unlike filters in Convolutional Neural Networks (CNNs), our weight matrix $W$ is unique and shared among every node. But there is another issue: nodes do not have a fixed number of neighbors like pixels do. . What if one node only has 1 neighbor, and another one has 500 of them? We would add 500 values instead of just one: the resulting embedding $h$ would be much larger for the node with 500 neighbors. . However, this doesn&#39;t make sense: nodes should always be comparable, so they need to have a similar range of values. To address this issue, we can normalize the result based on the number of connections. In graph theory, this number is called a degree. . $$h_i = dfrac{1}{ deg(i)} sum_{j in tilde{ mathcal{N}}_i} mathbf{W} x_j$$ . We&#39;re almost there! Introduced by Kipf et al. in 2016, the graph convolutional layer has one final improvement. . Indeed, the authors noticed that features from nodes with a lot of neighbors will spread much more easily than those from more isolated nodes. To counterbalance this effect, they proposed to give bigger weights to features from nodes with few neighbors. This operation can be written as follows: . $$h_i = sum_{j in tilde{ mathcal{N}}_i}} dfrac{1}{ sqrt{ deg(i)} sqrt{ deg(j)}} mathbf{W} x_j$$ . Notice that when $i$ and $j$ have the same number of neighbors, it is equivalent to our own layer. Now, let&#39;s see how to implement it in Python. . &#129504; III. Implementing a GCN . PyTorch Geometric directly implements the graph convolutional layer using GCNConv. . In this example, we will create a simple GCN with only one GCN layer, a ReLU activation function, and one linear layer. This final layer will output four values, corresponding to our four groups. The highest value will determine the class of each node. . In the following code block, we define the GCN layer with a 3-dim hidden layer. . from torch.nn import Linear from torch_geometric.nn import GCNConv class GCN(torch.nn.Module): def __init__(self): super().__init__() self.gcn = GCNConv(dataset.num_features, 3) self.out = Linear(3, dataset.num_classes) def forward(self, x, edge_index): h = self.gcn(x, edge_index).relu() z = self.out(h) return h, z model = GCN() print(model) . GNN( (gcn): GCNConv(34, 3) (out): Linear(in_features=3, out_features=4, bias=True) ) . If we added a second GCN layer, our model would not only aggregate feature vectors from the neighbors of each node, but also from the neighbors of these neighbors. . We can stack several graph layers to aggregate more and more distant values, but there&#39;s a catch: if we add too many layers, the aggregation becomes so intense that all the embeddings end up looking the same. This phenomenon is called over-smoothing and can be a real problem when you have too many layers. . Now that we&#39;ve defined our GNN, let&#39;s write a simple training loop with PyTorch. I chose a regular cross-entropy loss since it&#39;s a multi-class classification task, with Adam as optimizer. We could use the training mask, but we will ignore it for this exploratory exercise. . The training loop is standard: we try to predict the correct labels, and we compare the GCN&#39;s results to the values stored in data.y. The error is calculated by the cross-entropy loss and backpropagated with Adam to fine-tune our GNN&#39;s weights and biases. Finally, we print metrics every 10 epochs. . criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.02) # Calculate accuracy def accuracy(pred_y, y): return (pred_y == y).sum() / len(y) # Data for animations embeddings = [] losses = [] accuracies = [] outputs = [] # Training loop for epoch in range(201): # Clear gradients optimizer.zero_grad() # Forward pass h, z = model(data.x, data.edge_index) # Calculate loss function loss = criterion(z, data.y) # Calculate accuracy acc = accuracy(z.argmax(dim=1), data.y) # Compute gradients loss.backward() # Tune parameters optimizer.step() # Store data for animations embeddings.append(h) losses.append(loss) accuracies.append(acc) outputs.append(z.argmax(dim=1)) # Print metrics every 10 epochs if epoch % 10 == 0: print(f&#39;Epoch {epoch:&gt;3} | Loss: {loss:.2f} | Acc: {acc*100:.2f}%&#39;) . Epoch 0 | Loss: 1.35 | Acc: 38.24% Epoch 10 | Loss: 1.21 | Acc: 38.24% Epoch 20 | Loss: 1.08 | Acc: 41.18% Epoch 30 | Loss: 0.92 | Acc: 70.59% Epoch 40 | Loss: 0.72 | Acc: 73.53% Epoch 50 | Loss: 0.54 | Acc: 88.24% Epoch 60 | Loss: 0.41 | Acc: 88.24% Epoch 70 | Loss: 0.33 | Acc: 88.24% Epoch 80 | Loss: 0.29 | Acc: 88.24% Epoch 90 | Loss: 0.26 | Acc: 88.24% Epoch 100 | Loss: 0.24 | Acc: 88.24% Epoch 110 | Loss: 0.23 | Acc: 88.24% Epoch 120 | Loss: 0.22 | Acc: 88.24% Epoch 130 | Loss: 0.22 | Acc: 88.24% Epoch 140 | Loss: 0.21 | Acc: 88.24% Epoch 150 | Loss: 0.20 | Acc: 88.24% Epoch 160 | Loss: 0.20 | Acc: 91.18% Epoch 170 | Loss: 0.19 | Acc: 97.06% Epoch 180 | Loss: 0.17 | Acc: 100.00% Epoch 190 | Loss: 0.14 | Acc: 100.00% Epoch 200 | Loss: 0.12 | Acc: 100.00% . Great! Without much surprise, we reach 100% accuracy on the training set. It means that our model learned to correctly assign every member of the karate club to its correct group. . We can produce a neat visualization by animating the graph and see the evolution of the GNN&#39;s predictions during the training process. . %%capture from IPython.display import HTML from matplotlib import animation plt.rcParams[&quot;animation.bitrate&quot;] = 3000 def animate(i): G = to_networkx(data, to_undirected=True) nx.draw_networkx(G, pos=nx.spring_layout(G, seed=0), with_labels=True, node_size=800, node_color=outputs[i], cmap=&quot;hsv&quot;, vmin=-2, vmax=3, width=0.8, edge_color=&quot;grey&quot;, font_size=14 ) plt.title(f&#39;Epoch {i} | Loss: {losses[i]:.2f} | Acc: {accuracies[i]*100:.2f}%&#39;, fontsize=18, pad=20) fig = plt.figure(figsize=(12, 12)) plt.axis(&#39;off&#39;) anim = animation.FuncAnimation(fig, animate, np.arange(0, 200, 10), interval=500, repeat=True) html = HTML(anim.to_html5_video()) . display(html) . Your browser does not support the video tag. The first predictions are random, but the GCN perfectly labels every node after a while. Indeed, the final graph is the same as the one we plotted at the end of the first section. But what does the GCN really learn? . By aggregating features from neighboring nodes, the GNN learns a vector representation (or embedding) of every node in the network. In our model, the final layer just learns how to use these representations to produce the best classifications. However, embeddings are the real products of GNNs. . Let&#39;s print the embeddings learned by our model. . print(f&#39;Final embeddings = {h.shape}&#39;) print(h) . Final embeddings = torch.Size([34, 3]) tensor([[2.3756e+00, 5.1330e-01, 0.0000e+00], [3.2511e+00, 1.4347e+00, 0.0000e+00], [2.0562e+00, 1.5209e+00, 0.0000e+00], [2.9461e+00, 1.1436e+00, 0.0000e+00], [0.0000e+00, 0.0000e+00, 0.0000e+00], [0.0000e+00, 0.0000e+00, 0.0000e+00], [0.0000e+00, 0.0000e+00, 0.0000e+00], [2.4295e+00, 1.0172e+00, 0.0000e+00], [6.3575e-01, 2.6594e+00, 0.0000e+00], [1.9876e+00, 1.3767e+00, 0.0000e+00], [0.0000e+00, 6.0713e-04, 0.0000e+00], [2.2577e+00, 1.1747e+00, 0.0000e+00], [2.3823e+00, 1.1449e+00, 0.0000e+00], [2.2940e+00, 1.3844e+00, 0.0000e+00], [1.9155e-01, 2.7673e+00, 0.0000e+00], [1.8206e-01, 2.7194e+00, 0.0000e+00], [0.0000e+00, 2.0684e-03, 0.0000e+00], [2.3367e+00, 1.1042e+00, 0.0000e+00], [1.7925e-01, 2.7942e+00, 0.0000e+00], [2.0630e+00, 1.4096e+00, 0.0000e+00], [1.9360e-01, 2.7587e+00, 0.0000e+00], [2.2845e+00, 1.1088e+00, 0.0000e+00], [1.8486e-01, 2.7376e+00, 0.0000e+00], [0.0000e+00, 2.8447e+00, 0.0000e+00], [0.0000e+00, 8.9724e-01, 0.0000e+00], [0.0000e+00, 9.5606e-01, 0.0000e+00], [2.1157e-01, 2.8055e+00, 0.0000e+00], [2.6385e-01, 2.4765e+00, 0.0000e+00], [2.9965e-01, 8.5145e-01, 0.0000e+00], [0.0000e+00, 3.3316e+00, 0.0000e+00], [4.0497e-01, 2.8716e+00, 0.0000e+00], [0.0000e+00, 6.8132e-01, 0.0000e+00], [0.0000e+00, 4.1963e+00, 0.0000e+00], [0.0000e+00, 3.8991e+00, 0.0000e+00]], grad_fn=&lt;ReluBackward0&gt;) . As you can see, embeddings do not need to have the same dimensions as feature vectors. Here, I chose to reduce the number of dimensions from 34 (dataset.num_features) to three to get a nice visualization in 3D. . Let&#39;s plot these embeddings before any training happens, at epoch 0. . embed = h[0].detach().cpu().numpy() fig = plt.figure(figsize=(12, 12)) ax = fig.add_subplot(projection=&#39;3d&#39;) ax.patch.set_alpha(0) plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False) ax.scatter(embed[:, 0], embed[:, 1], embed[:, 2], s=200, c=data.y, cmap=&quot;hsv&quot;, vmin=-2, vmax=3) plt.show() . We see every node from Zachary&#39;s karate club with their true labels (and not the model&#39;s predictions). For now, they&#39;re all over the place since the GNN is not trained yet. But if we plot these embeddings at each step of the training loop, we&#39;d be able to visualize what the GNN truly learns. . Let&#39;s see how they evolve over time, as the GCN gets better and better at classifying nodes. . %%capture def animate(i): embed = embeddings[i].detach().cpu().numpy() ax.clear() ax.scatter(embed[:, 0], embed[:, 1], embed[:, 2], s=200, c=data.y, cmap=&quot;hsv&quot;, vmin=-2, vmax=3) plt.title(f&#39;Epoch {i} | Loss: {losses[i]:.2f} | Acc: {accuracies[i]*100:.2f}%&#39;, fontsize=18, pad=40) fig = plt.figure(figsize=(12, 12)) plt.axis(&#39;off&#39;) ax = fig.add_subplot(projection=&#39;3d&#39;) plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False) anim = animation.FuncAnimation(fig, animate, np.arange(0, 200, 10), interval=800, repeat=True) html = HTML(anim.to_html5_video()) . display(html) . Your browser does not support the video tag. We see that our GCN learned embeddings that group nodes from the same classes into nice clusters. Then, the final layer can easily separate them into different classes. . Embeddings are not unique to GNNs: they can be found everywhere in deep learning. They don&#39;t have to be 3D either: actually, they rarely are. For instance, language models like BERT produce embeddings with 768 or even 1024 dimensions. . Additional dimensions store more information about nodes, text, images, etc. but they also create bigger models that are more difficult to train. This is why it&#39;s better to keep low-dimensional embeddings as long as possible. . Conclusion . Graph Convolutional Networks are an incredibly versatile architecture that can be applied in many contexts. In this article, . We learned to use the PyTorch Geometric library to explore graph data with the Datasets and Data objects | We redesigned a graph convolutional layer from scratch | We implemented a GNN with a GCN layer | We visualized what training means for a GCN | . Zachary&#39;s karate club is a simplistic dataset, but it is good enough to understand the most important concepts in graph data and GNNs. . We only talked about node classification in this article, but there are other tasks GNNs can accomplish: link prediction (e.g., to recommend a friend), graph classification (e.g., to label molecules), graph generation (e.g., to create new molecules), and so on. . Beyond GCN, numerous GNN layers and architectures have been proposed by researchers. In the next article, we&#39;re gonna talk about Graph Attention Networks (GATs), which implicitly compute GCN&#39;s normalization factor and the importance of each connection with an attention mechanism. . If you enjoyed this article, feel free to follow me on Twitter for more GNN content. Thank you and have a great day! 📣 . &#127760; Graph Neural Network Course . 🔎 Course overview . 📝 Chapter 1: Introduction to Graph Neural Networks . 📝 Chapter 2: Graph Attention Network . 📝 Chapter 3: GraphSAGE . 📝 Chapter 4: Graph Isomorphism Network .",
            "url": "https://mlabonne.github.io/blog/intrognn/",
            "relUrl": "/intrognn/",
            "date": " • Feb 20, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Q-learning for beginners",
            "content": "The goal of this article is to teach an AI how to solve the ❄️Frozen Lake environment using reinforcement learning. Instead of reading Wikipedia articles and explaining formulas, we&#39;re going to start from scratch and try to recreate the 🤖Q-learning algorithm by ourselves. We&#39;ll not just understand how it works, but more importantly why it works: why was it designed that way? What are the hidden assumptions, the details that are never explained in regular courses and tutorials? . At the end of this article, you&#39;ll master the Q-learning algorithm and be able to apply it to other environments and real-world problems. It&#39;s a cool mini-project that gives a better insight into how reinforcement learning works and can hopefully inspire ideas for original and creative applications. . Let&#39;s start by installing the ❄️Frozen Lake environment and importing the necessary libraries: gym for the game, random to generate random numbers, and numpy to do some math. . !pip install -q gym !pip install -q matplotlib import gym import random import numpy as np . &#10052;&#65039; I. Frozen Lake . Now, let&#39;s talk about the game we&#39;re going to be solving in this tutorial. ❄️Frozen Lake is a simple environment composed of tiles, where the AI has to move from an initial tile to a goal. Tiles can be a safe frozen lake ✅, or a hole ❌ that gets you stuck forever. The AI, or agent, has 4 possible actions: go ◀️LEFT, 🔽DOWN, ▶️RIGHT, or 🔼UP. The agent must learn to avoid holes in order to reach the goal in a minimal number of actions. By default, the environment is always in the same configuration. In the environment&#39;s code, each tile is represented by a letter as follows: . S F F F (S: starting point, safe) F H F H (F: frozen surface, safe) F F F H (H: hole, stuck forever) H F F G (G: goal, safe) . &lt;/source&gt; We can try to manually solve the example above to understand the game. Let&#39;s see if the following sequence of actions is a correct solution: RIGHT $ to$ RIGHT $ to$ RIGHT $ to$ DOWN $ to$ DOWN $ to$ DOWN. Our agent starts on tile S, so we move right on a frozen surface ✅, then again ✅, then once more ✅, then we go down and find a hole ❌. . Actually, it&#39;s really easy to find several correct solutions: RIGHT $ to$ RIGHT $ to$ DOWN $ to$ DOWN $ to$ DOWN $ to$ RIGHT is an obvious one. But we could make a sequence of actions that loops around a hole 10 times before reaching the goal. This sequence is valid, but it doesn&#39;t meet our final requirement: the agent needs to meet the goal in a minimum number of actions. In this example, the minimum number of actions to complete the game is 6. We need to remember this fact to check if our agent really masters ❄️Frozen Lake or not. . &lt;/source&gt; Let&#39;s initialize the environment thanks to the gym library. There are two versions of the game: one with slippery ice, where selected actions have a random chance of being disregarded by the agent; and a non-slippery one, where actions cannot be ignored. We&#39;ll use the non-slippery one to begin with because it&#39;s easier to understand. . environment = gym.make(&quot;FrozenLake-v1&quot;, is_slippery=False) environment.reset() environment.render() . SFFF FHFH FFFH HFFG . We can see that the game that was created has the exact same configuration as in our example: it is the same puzzle. The position of our agent is indicated by a red rectangle. Solving this puzzle can be done with a simple script and if...else conditions, which would actually be useful to compare our AI to a simpler approach. However, we want to try a more exciting solution: reinforcement learning. . &#127937; II. Q-table . In ❄️Frozen Lake, there are 16 tiles, which means our agent can be found in 16 different positions, called states. For each state, there are 4 possible actions: go ◀️LEFT, 🔽DOWN, ▶️RIGHT, and 🔼UP. Learning how to play Frozen Lake is like learning which action you should choose in every state. To know which action is the best in a given state, we would like to assign a quality value to our actions. We have 16 states and 4 actions, so want to calculate $16 times 4 = 64$ values. . A nice way of representing it is using a table, known as a Q-table, where rows list every state $s$ and columns list every action $a$. In this Q-table, each cell contains a value $Q(s, a)$, which is the value (quality) of the action $a$ in the state $s$ (1 if it&#39;s the best action possible, 0 if it&#39;s really bad). When our agent is in a particular state $s$, it just has to check this table to see which action has the highest value. Taking the action with the highest value makes sense but we&#39;ll see later that we can design something even better... . State ◀️LEFT 🔽DOWN ▶️RIGHT 🔼UP . S=0 | Q(0, ◀️) | Q(0, 🔽) | Q(0, ▶️) | Q(0, 🔼) | . 1 | Q(1, ◀️) | Q(1, 🔽) | Q(1, ▶️) | Q(1, 🔼) | . 2 | Q(2, ◀️) | Q(2, 🔽) | Q(2, ▶️) | Q(2, 🔼) | . ... | ... | ... | ... | ... | . 14 | Q(14, ◀️) | Q(14, 🔽) | Q(14, ▶️) | Q(14, 🔼) | . G=15 | Q(15, ◀️) | Q(15, 🔽) | Q(15, ▶️) | Q(15, 🔼) | . Example of Q-table, where each cell contains the value $Q(a, s)$ of the action $a$ (column) in a given state $s$ (row)Let&#39;s create our Q-table and fill it with zeros since we still have no idea of the value of each action in each state. . # Our table has the following dimensions: # (rows x columns) = (states x actions) = (16 x 4) qtable = np.zeros((16, 4)) # Alternatively, the gym library can also directly g # give us the number of states and actions using # &quot;env.observation_space.n&quot; and &quot;env.action_space.n&quot; nb_states = environment.observation_space.n # = 16 nb_actions = environment.action_space.n # = 4 qtable = np.zeros((nb_states, nb_actions)) # Let&#39;s see how it looks print(&#39;Q-table =&#39;) print(qtable) . Q-table = [[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]] . Great! We have our Q-table with 16 rows (our 16 states) and 4 columns (our 4 actions) as expected. Let&#39;s try to see what we can do next: every value is set to zero, so we have no information at all. Let&#39;s say that the agent takes a random action: ◀️LEFT, 🔽DOWN, ▶️RIGHT, or 🔼UP. . We can use the random library with the choice method to randomly choose an action. . random.choice([&quot;LEFT&quot;, &quot;DOWN&quot;, &quot;RIGHT&quot;, &quot;UP&quot;]) . &#39;LEFT&#39; . Wait, actually the agent is currently on the initial state S, which means only two actions are possible: ▶️RIGHT and 🔽DOWN. The agent can also take the actions 🔼UP and ◀️LEFT, but it won&#39;t move: its state doesn&#39;t change. Therefore, we do not put any constraint on what actions are possible: the agent will naturally understand that some of them don&#39;t do anything. . We can keep using random.choice(), but the gym library already implements a method to randomly choose an action. It might save us some hassle later, so let&#39;s try it. . environment.action_space.sample() . 0 . Oops... this time it&#39;s a number. We could read gym&#39;s documentation but it is quite scarce unfortunately. No worries though, we can check the source code on GitHub to understand what these numbers mean. It&#39;s actually super straightforward: . ◀️ LEFT = 0 🔽 DOWN = 1 ▶️ RIGHT = 2 🔼 UP = 3 . &lt;/source&gt; Okay, now that we understand how gym connects numbers to directions, let&#39;s try to use it to move our agent to the right ▶️. This time, it can be performed using the step(action) method. We can try to directly provide it the number 2, corresponding to the direction we chose (right), and check if the agent moved. . environment.step(2) environment.render() . (Right) SFFF FHFH FFFH HFFG . Huzzah! The red square moved from the initial state S to the right: our prediction was correct. And that&#39;s all we need to know in order to interact with the environment: . How to randomly choose an action using action_space.sample(); | How to implement this action and move our agent in the desired direction with step(action). | To be completely exhaustive, we can add: . How to display the current map to see what we&#39;re doing with render(); | How to restart the game when the agent falls into a hole or reaches the goal G with reset(). | Now that we understand how to interact with our gym environment, let&#39;s go back to our algorithm. In reinforcement learning, agents are rewarded by the environment when they accomplish a predefined goal. In ❄️Frozen Lake, the agent is only rewarded when it reaches the state G (see the source code). We cannot control this reward, it is set in the environment: it&#39;s 1 when the agent reaches G, and 0 otherwise. . Let&#39;s print it every time we implement an action. The reward is given by the method step(action). . action = environment.action_space.sample() # 2. Implement this action and move the agent in the desired direction new_state, reward, done, info = environment.step(action) # Display the results (reward and map) environment.render() print(f&#39;Reward = {reward}&#39;) . (Left) SFFF FHFH FFFH HFFG Reward = 0.0 . The reward is indeed 0... 😱 wow, I guess we&#39;re in a pickle, because only one state can give us a positive reward in the entire game. How are we supposed to take the right directions at the very beginning when the only validation we have is at the very end? If we ever want to see a reward of 1, we&#39;d need to be lucky enough to find the correct sequence of actions by chance. Unfortunately, that&#39;s exactly how it works... the Q-table will remain filled with zeros until the agent randomly reaches the goal G. . The problem would be much simpler if we could have intermediate, smaller rewards to guide our path towards the goal G. Alas, this is actually one of the main issues of reinforcement learning: this phenomenon, called sparse rewards, makes agents very difficult to train on problems where the only reward is at the end of a long sequence of actions. Different techniques were proposed to mitigate this issue, but we&#39;ll talk about it another time. . &#129302; III. Q-learning . Let&#39;s go back to our problem. Okay, we need to be lucky enough to find the goal G by accident. But once it&#39;s done, how to backpropagate the information to the initial state? The 🤖Q-learning algorithm offers a clever solution to this issue. We need to update the value of our state-action pairs (each cell in the Q-table) considering 1/ the reward for reaching the next state, and 2/ the highest possible value in the next state. . &lt;/source&gt; We know we get a reward of 1 when we move to G. As we just said, the value of the state next to G (let&#39;s call it G-1) with the relevant action to reach G is increased thanks to the reward. Okay good, end of the episode: the agent won and we restart the game. Now, the next time the agent is in a state next to G-1, it will increase the value of this state (let&#39;s call it G-2) with the relevant action to reach G-1. The next time the agent is in a state next to G-2, it will do the same. Rinse and repeat, until the update reaches the initial state S. . Let&#39;s try to find the update formula to backpropagate the values from G to S. Remember: values denote the quality of an action in a specific state (0 if it&#39;s terrible, 1 if it&#39;s the best action possible in this state). We try to update the value of the action $a_t$ (for example, $a_t = 0$ if the action is left) in the state $s_t$ (for example, $s_t = 0$ when the agent is in the initial state S). This value is just a cell in our Q-table, corresponding to the row number $s_t$ and the column number $a_t$: this value is formally called $Q(s_t, a_t)$. . As we said previously, we need to update it using 1/ the reward for the next state (formally noted $r_t$), and 2/ the maximum possible value in the next state ($max_aQ(s_{t+1},a)$). Therefore, the update formula must look like: . $$Q_{new}(s_t, a_t) = Q(s_t, a_t) + r_t + max_aQ(s_{t+1}, a)$$ . The new value is the current one + the reward + the highest value in the next state. We can manually try our formula to check if it looks correct: let&#39;s pretend our agent is in the state G-1 next to the goal G for the first time. We can update the value corresponding to the winning action in this state G-1 with $Q_{new}(G-1, a_t) = Q(G-1, a_t) + r_t + max_aQ(G, a)$, where $Q(G-1, a_t) = 0$ and $max_aQ(G, a) = 0$ because the Q-table is empty, and $r_t = 1$ because we get the only reward in this environment. We obtain $Q_{new}(G-1, a_t) = 1$. The next time the agent is in a state next to this one (G-2), we update it too using the formula and get the same result: $Q_{new}(G-2, a_t) = 1$. In the end, we backpropagate ones in the Q-table from G to S. Okay it works, but the result is binary: either it&#39;s the wrong state-action pair or the best one. We would like more nuance... . Actually, we almost found the true Q-learning update formula with common sense. The nuance we&#39;re looking for adds two parameters: . $ alpha$ is the 💡learning rate (between $0$ and $1$), which is how much we should change the original $Q(s_t, a_t)$ value. If $ alpha = 0$, the value never changes, but if $ alpha = 1$, the value changes extremely fast. In our attempt, we didn&#39;t limit the learning rate so $ alpha = 1$. But this is too fast in reality: the reward and the maximum value in the next state quickly overpower the current value. We need to find a balance between the importance of past and new knowledge. | $ gamma$ is the 📉discount factor (between $0$ and $1$), which determines how much the agent cares about future rewards compared to immediate ones (as the saying goes, &quot;a bird in the hand is worth two in the bush&quot;). If $ gamma = 0$, the agent only focuses on immediate rewards , but if $ gamma = 1$, any potential future reward has the same value than current ones. In ❄️Frozen Lake, we want a high discount factor since there&#39;s only one possible reward at the very end of the game. | . With the real Q-learning algorithm, the new value is calculated as follows: . $$Q_{new}(s_t, a_t) = Q(s_t, a_t) + alpha cdot (r_t + gamma cdot max_aQ(s_{t+1},a) - Q(s_t, a_t))$$ . Okay, let&#39;s try this new formula before implementing it. Once again, we can pretend that our agent is next to the goal G for the first time. We can update the state-action pair to win the game using our formula: $Q_{new}(G-1, a_t) = 0 + alpha cdot (1 + gamma cdot 0 - 0)$. We can assign arbitrary values to $ alpha$ and $ gamma$ to calculate the result. With $ alpha = 0.5$ and $ gamma = 0.9$, we get $Q_{new}(G-1, a_t) = 0 + 0.5 cdot (1 + 0.9 cdot 0 - 0) = 0.5$. The second time the agent is in this state, we would get: $Q_{new}(G-1, a_t) = 0.5 + 0.5 cdot (1 + 0.9 cdot 0 - 0.5) = 0.75$, then $0.875$, $0.9375$, $0.96875$, etc. . &lt;/source&gt; So training our agent in code means: . Choosing a random action (using action_space.sample()) if the values in the current state are just zeros. Otherwise, we take the action with the highest value in the current state with the function np.argmax(); | Implementing this action by moving in the desired direction with step(action); | Updating the value of the original state with the action we took, using information about the new state and the reward given by step(action); | We keep repeating these 3 steps until the agent gets stuck in a hole or reaches the goal G. When it happens, we just restart the environment with reset() and start a new episode until we hit 1,000 episodes. Additionally, we can plot the outcome of each run (failure if it didn&#39;t reach the goal, success otherwise) to observe the progress of our agent. . import matplotlib.pyplot as plt plt.rcParams[&#39;figure.dpi&#39;] = 300 plt.rcParams.update({&#39;font.size&#39;: 17}) # We re-initialize the Q-table qtable = np.zeros((environment.observation_space.n, environment.action_space.n)) # Hyperparameters episodes = 1000 # Total number of episodes alpha = 0.5 # Learning rate gamma = 0.9 # Discount factor # List of outcomes to plot outcomes = [] print(&#39;Q-table before training:&#39;) print(qtable) # Training for _ in range(episodes): state = environment.reset() done = False # By default, we consider our outcome to be a failure outcomes.append(&quot;Failure&quot;) # Until the agent gets stuck in a hole or reaches the goal, keep training it while not done: # Choose the action with the highest value in the current state if np.max(qtable[state]) &gt; 0: action = np.argmax(qtable[state]) # If there&#39;s no best action (only zeros), take a random one else: action = environment.action_space.sample() # Implement this action and move the agent in the desired direction new_state, reward, done, info = environment.step(action) # Update Q(s,a) qtable[state, action] = qtable[state, action] + alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action]) # Update our current state state = new_state # If we have a reward, it means that our outcome is a success if reward: outcomes[-1] = &quot;Success&quot; print() print(&#39;===========================================&#39;) print(&#39;Q-table after training:&#39;) print(qtable) # Plot outcomes plt.figure(figsize=(12, 5)) plt.xlabel(&quot;Run number&quot;) plt.ylabel(&quot;Outcome&quot;) ax = plt.gca() ax.set_facecolor(&#39;#efeeea&#39;) plt.bar(range(len(outcomes)), outcomes, color=&quot;#0A047A&quot;, width=1.0) plt.show() . Q-table before training: [[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]] =========================================== Q-table after training: [[0. 0. 0.59049 0. ] [0. 0. 0.6561 0. ] [0. 0.729 0. 0. ] [0. 0. 0. 0. ] [0. 0.02050313 0. 0. ] [0. 0. 0. 0. ] [0. 0.81 0. 0. ] [0. 0. 0. 0. ] [0. 0. 0.17085938 0. ] [0. 0. 0.49359375 0. ] [0. 0.9 0. 0. ] [0. 0. 0. 0. ] [0. 0. 0. 0. ] [0. 0. 0. 0. ] [0. 0. 1. 0. ] [0. 0. 0. 0. ]] . The agent is trained! Each blue bar on the figure corresponds to a win, so we can see that the agent had a hard time finding the goal at the beginning of the training. But once it found it several times in a row, it began to consistently win. 🥳 The trained Q-table is also very interesting: these values indicate the unique sequence of actions the agent learned to reach the goal. . Now let&#39;s see how it performs by evaluating it on 100 episodes. We consider that the training is over, so we don&#39;t need to update the Q-table anymore. To see how the agent performs, we can calculate the percentage of times the it managed to reach the goal (success rate). . episodes = 100 nb_success = 0 # Evaluation for _ in range(100): state = environment.reset() done = False # Until the agent gets stuck or reaches the goal, keep training it while not done: # Choose the action with the highest value in the current state if np.max(qtable[state]) &gt; 0: action = np.argmax(qtable[state]) # If there&#39;s no best action (only zeros), take a random one else: action = environment.action_space.sample() # Implement this action and move the agent in the desired direction new_state, reward, done, info = environment.step(action) # Update our current state state = new_state # When we get a reward, it means we solved the game nb_success += reward # Let&#39;s check our success rate! print (f&quot;Success rate = {nb_success/episodes*100}%&quot;) . Success rate = 100.0% . Not only our agent has been trained, but it manages to hit a 100% success rate. Great job everyone, the non-slippery ❄️Frozen Lake is solved! . We can even visualize the agent moving on the map by executing the code below and print the sequence of actions it took to check if it&#39;s the best one. . from IPython.display import clear_output import time state = environment.reset() done = False sequence = [] while not done: # Choose the action with the highest value in the current state if np.max(qtable[state]) &gt; 0: action = np.argmax(qtable[state]) # If there&#39;s no best action (only zeros), take a random one else: action = environment.action_space.sample() # Add the action to the sequence sequence.append(action) # Implement this action and move the agent in the desired direction new_state, reward, done, info = environment.step(action) # Update our current state state = new_state # Update the render clear_output(wait=True) environment.render() time.sleep(1) print(f&quot;Sequence = {sequence}&quot;) . (Right) SFFF FHFH FFFH HFFG Sequence = [2, 2, 1, 1, 1, 2] . The agent can learn several correct sequence of actions: [2, 2, 1, 1, 1, 2], [1, 1, 2, 2, 1, 2], etc. The good thing is there&#39;s only 6 actions in our sequence, which was the minimum possible number of actions we counted: it means that our agent learned to solve the game in an optimal way. In the case of [2, 2, 1, 1, 1, 2], which corresponds to RIGHT $ to$ RIGHT $ to$ DOWN $ to$ DOWN $ to$ DOWN $ to$ RIGHT, it&#39;s exactly the sequence we predicted at the very beginning of the article. 📣 . &#128208; IV. Epsilon-Greedy algorithm . Despite this success, there&#39;s something that bothers me with our previous approach: the agent always chooses the action with the highest value. So whenever a state-action pair starts having a non-zero value, the agent will always choose it. The other actions will never be taken, which means we&#39;ll never update their value... But what if one of these actions was better than the one the agent always takes? Shouldn&#39;t we encourage the agent to try news things from time to time and see if it can improve? . In other words, we want to allow our agent to either: . Take the action with the highest value (exploitation); | Choose a random action to try to find even better ones (exploration). | . A tradeoff between these two behaviors is important: if the agent only focuses on exploitation, it cannot try new solutions and thus doesn&#39;t learn anymore. On the other hand, if the agent only takes random actions, the training is pointless since it doesn&#39;t use the Q-table. So we want to change this parameter over time: at the beginning of the training, we want to explore the environment as much as possible. But exploration becomes less and less interesting, as the agent already knows every possible state-action pairs. This parameter represents the amount of randomness in the action selection. . This technique is commonly called the epsilon-greedy algorithm, where epsilon is our parameter. It is a simple but extremely efficient method to find a good tradeoff. Every time the agent has to take an action, it has a probability $ε$ of choosing a random one, and a probability $1-ε$ of choosing the one with the highest value. We can decrease the value of epsilon at the end of each episode by a fixed amount (linear decay), or based on the current value of epsilon (exponential decay). . &lt;/source&gt; Let&#39;s implement a linear decay. Beforehand, I&#39;d like to see how the curve looks like with arbitrary parameters. We&#39;ll start with $ε = 1$ to be in full exploration mode, and decrease this value by $0.001$ after each episode. . Okay now that we have a sound understanding of it, we can implement it for real and see how it changes the agent&#39;s behavior. . qtable = np.zeros((environment.observation_space.n, environment.action_space.n)) # Hyperparameters episodes = 1000 # Total number of episodes alpha = 0.5 # Learning rate gamma = 0.9 # Discount factor epsilon = 1.0 # Amount of randomness in the action selection epsilon_decay = 0.001 # Fixed amount to decrease # List of outcomes to plot outcomes = [] print(&#39;Q-table before training:&#39;) print(qtable) # Training for _ in range(episodes): state = environment.reset() done = False # By default, we consider our outcome to be a failure outcomes.append(&quot;Failure&quot;) # Until the agent gets stuck in a hole or reaches the goal, keep training it while not done: # Generate a random number between 0 and 1 rnd = np.random.random() # If random number &lt; epsilon, take a random action if rnd &lt; epsilon: action = environment.action_space.sample() # Else, take the action with the highest value in the current state else: action = np.argmax(qtable[state]) # Implement this action and move the agent in the desired direction new_state, reward, done, info = environment.step(action) # Update Q(s,a) qtable[state, action] = qtable[state, action] + alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action]) # Update our current state state = new_state # If we have a reward, it means that our outcome is a success if reward: outcomes[-1] = &quot;Success&quot; # Update epsilon epsilon = max(epsilon - epsilon_decay, 0) print() print(&#39;===========================================&#39;) print(&#39;Q-table after training:&#39;) print(qtable) # Plot outcomes plt.figure(figsize=(12, 5)) plt.xlabel(&quot;Run number&quot;) plt.ylabel(&quot;Outcome&quot;) ax = plt.gca() ax.set_facecolor(&#39;#efeeea&#39;) plt.bar(range(len(outcomes)), outcomes, color=&quot;#0A047A&quot;, width=1.0) plt.show() . Q-table before training: [[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]] =========================================== Q-table after training: [[0.531441 0.59049 0.59049 0.531441 ] [0.531441 0. 0.6561 0.56396466] [0.58333574 0.729 0.56935151 0.65055117] [0.65308668 0. 0.33420534 0.25491326] [0.59049 0.6561 0. 0.531441 ] [0. 0. 0. 0. ] [0. 0.81 0. 0.65519631] [0. 0. 0. 0. ] [0.6561 0. 0.729 0.59049 ] [0.6561 0.81 0.81 0. ] [0.72899868 0.9 0. 0.72711067] [0. 0. 0. 0. ] [0. 0. 0. 0. ] [0. 0.81 0.9 0.729 ] [0.81 0.9 1. 0.81 ] [0. 0. 0. 0. ]] . Hey, the agent takes more time to consistently win the game now! And the Q-table has a lot more non-zero values than the previous one, which means the agent has learned several sequences of actions to reach the goal. It is understandable, since this new agent is forced to explore state-action pairs instead of always exploiting ones with non-zero values. . Let&#39;s see if it&#39;s as successful as the previous one to win the game. In evaluation mode, we don&#39;t want exploration anymore because the agent is trained now. . episodes = 100 nb_success = 0 # Evaluation for _ in range(100): state = environment.reset() done = False # Until the agent gets stuck or reaches the goal, keep training it while not done: # Choose the action with the highest value in the current state action = np.argmax(qtable[state]) # Implement this action and move the agent in the desired direction new_state, reward, done, info = environment.step(action) # Update our current state state = new_state # When we get a reward, it means we solved the game nb_success += reward # Let&#39;s check our success rate! print (f&quot;Success rate = {nb_success/episodes*100}%&quot;) . Success rate = 100.0% . Phew, it&#39;s another 100% success rate! We didn&#39;t degrade the model. 😌 The benefits of this approach might not be obvious in this example, but our model became less static and more flexible. It learned different paths (sequences of actions) from S to G instead of just one as in the previous approach. More exploration can degrade performance but it&#39;s necessary to train agents that can adapt to new environments. . &#10052;&#65039; IV. Challenge: slippery Frozen Lake . We didn&#39;t solve the entire ❄️Frozen Lake environment: we only trained an agent on the non-slippery version, using is_slippery = False during initialization. In the slippery variant, the action the agent takes only has 33% chance of succeeding. In case of failure, one of the three other actions is randomly taken instead. This feature adds a lot of randomness to the training, which makes things more difficult for our agent. Let&#39;s see how well our code is doing in this new environment... . environment = gym.make(&quot;FrozenLake-v1&quot;, is_slippery=True) environment.reset() # We re-initialize the Q-table qtable = np.zeros((environment.observation_space.n, environment.action_space.n)) # Hyperparameters episodes = 1000 # Total number of episodes alpha = 0.5 # Learning rate gamma = 0.9 # Discount factor epsilon = 1.0 # Amount of randomness in the action selection epsilon_decay = 0.001 # Fixed amount to decrease # List of outcomes to plot outcomes = [] print(&#39;Q-table before training:&#39;) print(qtable) # Training for _ in range(episodes): state = environment.reset() done = False # By default, we consider our outcome to be a failure outcomes.append(&quot;Failure&quot;) # Until the agent gets stuck in a hole or reaches the goal, keep training it while not done: # Generate a random number between 0 and 1 rnd = np.random.random() # If random number &lt; epsilon, take a random action if rnd &lt; epsilon: action = environment.action_space.sample() # Else, take the action with the highest value in the current state else: action = np.argmax(qtable[state]) # Implement this action and move the agent in the desired direction new_state, reward, done, info = environment.step(action) # Update Q(s,a) qtable[state, action] = qtable[state, action] + alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action]) # Update our current state state = new_state # If we have a reward, it means that our outcome is a success if reward: outcomes[-1] = &quot;Success&quot; # Update epsilon epsilon = max(epsilon - epsilon_decay, 0) print() print(&#39;===========================================&#39;) print(&#39;Q-table after training:&#39;) print(qtable) # Plot outcomes plt.figure(figsize=(12, 5)) plt.xlabel(&quot;Run number&quot;) plt.ylabel(&quot;Outcome&quot;) ax = plt.gca() ax.set_facecolor(&#39;#efeeea&#39;) plt.bar(range(len(outcomes)), outcomes, color=&quot;#0A047A&quot;, width=1.0) plt.show() episodes = 100 nb_success = 0 # Evaluation for _ in range(100): state = environment.reset() done = False # Until the agent gets stuck or reaches the goal, keep training it while not done: # Choose the action with the highest value in the current state action = np.argmax(qtable[state]) # Implement this action and move the agent in the desired direction new_state, reward, done, info = environment.step(action) # Update our current state state = new_state # When we get a reward, it means we solved the game nb_success += reward # Let&#39;s check our success rate! print (f&quot;Success rate = {nb_success/episodes*100}%&quot;) . Q-table before training: [[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]] =========================================== Q-table after training: [[0.06208723 0.02559574 0.02022059 0.01985828] [0.01397208 0.01425862 0.01305446 0.03333396] [0.01318348 0.01294602 0.01356014 0.01461235] [0.01117016 0.00752795 0.00870601 0.01278227] [0.08696239 0.01894036 0.01542694 0.02307306] [0. 0. 0. 0. ] [0.09027682 0.00490451 0.00793372 0.00448314] [0. 0. 0. 0. ] [0.03488138 0.03987256 0.05172554 0.10780482] [0.12444437 0.12321815 0.06462294 0.07084008] [0.13216145 0.09460133 0.09949734 0.08022573] [0. 0. 0. 0. ] [0. 0. 0. 0. ] [0.1606242 0.18174032 0.16636549 0.11444442] [0.4216631 0.42345944 0.40825367 0.74082329] [0. 0. 0. 0. ]] . Success rate = 17.0% . Oof it&#39;s not so good. But can you improve the performance by tweaking the different parameters we talked about? I encourage you to take this little challenge and do it on your own to have fun with reinforcement learning and check if you understood everything we said about Q-learning. And why not implementing exponential decay for the epsilon-greedy algorithm too? During this quick exercise, you might realise that slightly modifying the hyperparameters can completely destroy the results. This is another quirk of reinforcement learning: hyperparameters are quite moody, and it is important to understand their meaning if you want to tweak them. It&#39;s always good to test and try new combinations to build your intuition and become more efficient. Good luck and have fun! . &#128282; V. Conclusion . Q-learning is a simple yet powerful algorithm at the core of reinforcement learning. In this article, . We learned to interact with the gym environment to choose actions and move our agent; | We introduced the idea of a Q-table, where rows are states, columns are actions, and cells are the value of an action in a given state; | We experimentally recreated the Q-learning update formula to tackle the sparse reward problem; | We implemented an entire training and evaluation process, that solved the ❄️Frozen Lake environment with 100% success rate; | We implemented the famous epsilon-greedy algorithm in order to create a tradeoff between the exploration of unknown state-action pairs and the exploitation of the most successful ones. | . The ❄️Frozen Lake is a very simple environment, but others can have so many states and actions that it becomes impossible to store the Q-table in memory. This is especially the case in environments where events are not discrete, but continuous (like Super Mario Bros. or Minecraft). When the problem arises, a popular technique consists of training a deep neural network to approximate the Q-table. This method adds several layers of complexity, since the neural networks are not very stable. But I will cover it in another tutorial with different techniques to stabilize them. . Until then, share this article if it helped you and follow me on Twitter and Medium for more practical content around machine learning and deep learning. 📣 .",
            "url": "https://mlabonne.github.io/blog/reinforcement%20learning/q-learning/frozen%20lake/gym/tutorial/2022/02/13/Q_learning.html",
            "relUrl": "/reinforcement%20learning/q-learning/frozen%20lake/gym/tutorial/2022/02/13/Q_learning.html",
            "date": " • Feb 13, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "👋 Hi, my name is Maxime Labonne and I’m a research scientist in machine learning &amp; deep learning at ✈️ Airbus Defence and Space. I have a PhD in machine learning applied to cyber security from the Polytechnic Institute of Paris (number 49 in QS World University Rankings). I’ve worked for the French Atomic Energy Commission, where I developed new machine learning architectures dedicated to anomaly detection and that are now sold and used in various industrial, national, and European projects. . In this blog, I want to write tutorials and guides about AI in a different way. I want to start with practice and implementation, and then move on to explanations. Even better, I would like to experimentally retrace the thought process of the original authors of the algorithms we use everyday to understand their design, the importance of each component, rather than using them as black boxes. Implementation is at the core of this process, since it is only by implementing by yourself that you discover all the assumptions and other details that are often ignored, but are nonetheless essential. Finally, I think this process is fun and allows you to practice tackling difficult problems to design new and suitable machine learning solutions. . I hope we can have fun together and learn a few things along the way. :) . If you want to contact me, feel free to send a mail at this address. . . Credits: . Thanks to Hamel Husain and all the contributors of fastpages that powers this blog. License: Apache License 2.0. | Emojis used in figures are designed by OpenMoji, the open-source emoji and icon project. License: CC BY-SA 4.0. | Vector icons are provided by Streamline (https://streamlinehq.com). License: CC BY-SA 4.0. | .",
          "url": "https://mlabonne.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Publications",
          "content": "📜 Thesis . Anomaly-based network intrusion detection using machine learningMaxime LabonnePolytechnic Institute of Paris (Institut Polytechnique de Paris), 2020 | . 📝 Proceedings . Anomaly detection in vehicle-to-infrastructure communicationsMichele Russo, Maxime Labonne, Alexis Olivereau, Mohammad Rmayti2018 IEEE 87th Vehicular Technology Conference (VTC Spring) | A Cascade-structured Meta-Specialists Approach for Neural Network-based Intrusion DetectionMaxime Labonne, Alexis Olivereau, Baptiste Polvé, Djamal Zeghlache2019 16th IEEE Annual Consumer Communications &amp; Networking Conference (CCNC) | Unsupervised protocol-based intrusion detection for real-world networksMaxime Labonne, Alexis Olivereau, Baptise Polvé, Djamal Zeghlache2020 International Conference on Computing, Networking and Communications (ICNC) | Predicting Bandwidth Utilization on Network Links Using Machine LearningMaxime Labonne, Charalampos Chatzinakis, Alexis Olivereau2020 European Conference on Networks and Communications (EuCNC) | Priority flow admission and routing in sdn: Exact and heuristic approachesJorge López, Maxime Labonne, Claude Poletti, Dallal Belabed2020 IEEE 19th International Symposium on Network Computing and Applications (NCA) | Short-Term Flow-Based Bandwidth Forecasting using Machine LearningMaxime Labonne, Jorge López, Claude Poletti, Jean-Baptiste Munier2021 IEEE 22nd International Symposium on a World of Wireless, Mobile and Multimedia Networks (WoWMoM) | Toward Formal Data Set Verification for Building Effective Machine Learning ModelsJorge López, Maxime Labonne, Claude PolettiKDIR 2021 - 13th International Conference on Knowledge Discovery and Information Retrieval | .",
          "url": "https://mlabonne.github.io/blog/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  

  
      ,"page5": {
          "title": "",
          "content": "User-agent: * Sitemap: https://mlabonne.github.io/blog/sitemap.xml .",
          "url": "https://mlabonne.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

  

  
  

  
  

  
  

  
  

}