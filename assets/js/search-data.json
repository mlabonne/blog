{
  
    
        "post0": {
            "title": "Introduction to GraphSAGE in Python",
            "content": "In the real world, graphs can quickly become very large. . In the first article, Zachary&#39;s karate club only had 34 nodes and 156 directed edges. In the second article, Citeseer had 3312 nodes and 4732 directed edges. This raises the question of how many nodes and connections a Graph Neural Network (GNN) can handle. . Could we feed a graph with millions or billions of edges to a GNN? The good news is: yes we can. However, it requires a few improvements to properly handle this amount of data. In this article, we&#39;ll introduce a new GNN architecture called GraphSAGE, specifically designed for large graphs. . We don&#39;t want to improve scalability just for the love of science. GraphSAGE is one of the most used architectures in GNNs, and for a good reason: it has become quite popular with big tech companies. As they process huge amounts of data, the contribution of this architecture has become obvious to them. üí∞ . We can cite two public examples of GraphSAGE-like systems in production: . Pinterest developed its own version called PinSAGE to recommend the most relevant images (pins) to its users. According to the authors, this system operates on a graph comprising 18 billion connections and 3 billion nodes. And from my personal experience, their recommender system works really well (think Spotify, not Linkedin). | UberEats also reported using a modified version of GraphSAGE to suggest dishes, restaurants, and cuisines. We don&#39;t know the exact size of this graph, but UberEats proudly claims to support more than 600,000 restaurants and 66 million users. Meanwhile, it keeps recommending me the same tacos that gave me food poisoning. | . In this tutorial, we&#39;ll use a dataset with 20k nodes instead of billions because Google Colab cannot handle our ambitions. We will stick to the original GraphSAGE architecture, but the previous variants also bring interesting features that we will comment on. . So, first things first, let&#39;s install and import PyTorch Geometric with the following commands. . import torch torchversion = torch.__version__ # Install PyTorch Scatter, PyTorch Sparse, and PyTorch Geometric !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torchversion}.html !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torchversion}.html !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git # Visualization import networkx as nx import matplotlib.pyplot as plt plt.rcParams[&#39;figure.dpi&#39;] = 300 plt.rcParams.update({&#39;font.size&#39;: 24}) . &#127760; I. PubMed dataset . As we saw in the previous article, PubMed is the senior member of the Planetoid triumvirate. Here&#39;s a quick summary: . It contains 19,717 scientific publications about diabetes from PubMed&#39;s database; | Node features are TF-IDF weighted word vectors with 500 dimensions, which is a pretty cool way of summarizing documents without transformers; | The task is quite straightforward since it&#39;s a multi-class classification with three categories: diabetes mellitus experimental, diabetes mellitus type 1, and diabetes mellitus type 2. | . This is the beauty and the curse of deep learning: I don&#39;t know anything about diabetes, but I&#39;ll still feel pretty satisfied if we reach 70% accuracy. However, we shouldn&#39;t forget that expert knowledge is required to apply any DL technique to a field, or we might just build the next IBM Watson. . from torch_geometric.datasets import Planetoid dataset = Planetoid(root=&#39;.&#39;, name=&quot;Pubmed&quot;) data = dataset[0] # Print information about the dataset print(f&#39;Dataset: {dataset}&#39;) print(&#39;-&#39;) print(f&#39;Number of graphs: {len(dataset)}&#39;) print(f&#39;Number of nodes: {data.x.shape[0]}&#39;) print(f&#39;Number of features: {dataset.num_features}&#39;) print(f&#39;Number of classes: {dataset.num_classes}&#39;) # Print information about the graph print(f&#39; nGraph:&#39;) print(&#39;&#39;) print(f&#39;Training nodes: {sum(data.train_mask).item()}&#39;) print(f&#39;Evaluation nodes: {sum(data.val_mask).item()}&#39;) print(f&#39;Test nodes: {sum(data.test_mask).item()}&#39;) print(f&#39;Edges are directed: {data.is_directed()}&#39;) print(f&#39;Graph has isolated nodes: {data.has_isolated_nodes()}&#39;) print(f&#39;Graph has loops: {data.has_self_loops()}&#39;) . Dataset: Pubmed() - Number of graphs: 1 Number of nodes: 19717 Number of features: 500 Number of classes: 3 Graph: Training nodes: 60 Evaluation nodes: 500 Test nodes: 1000 Edges are directed: False Graph has isolated nodes: False Graph has loops: False . As we can see, PubMed has an insanely low number of training nodes compared to the full graph. There are only 60 samples to learn how to classify the 1000 test nodes. Despite this challenge, GNNs manage to obtain high levels of accuracy. Here&#39;s the leaderboard of known techniques (a more complete benchmark can be found on PapersWithCode): . Model üìùPubMed (accuracy) . Multilayer Perceptron | 71.4% | . Graph Convolutional Network | 79.0% | . Graph Attention Network | 79.0% | . GraphSAGE | ??? | . . I couldn&#39;t find any result for GraphSAGE on PubMed with this specific setting (60 training nodes, 1000 test nodes), so I don&#39;t expect a great accuracy. But another metric can be just as relevant when working with large graphs: training time. . &#129497;&#8205;&#9794;&#65039; II. GraphSAGE in theory . The GraphSAGE algorithm can be divided into two steps: 1/ neighbor sampling and 2/ aggregation. . &#127920; A. Neighbor sampling . Mini-batching is a common technique used in machine learning. It works by breaking down a dataset into smaller batches, which allows us to train models more effectively. So what are the benefits of using mini-batching? . Improved accuracy - mini-batches help to reduce overfitting (gradients are averaged), as well as variance in error rates; | Increased speed - mini-batches are processed in parallel and take less time to train than larger batches; | Improved scalability - an entire dataset can exceed the GPU memory, but smaller batches can get around this limitation. | Mini-batching is so useful it became standard in regular neural networks. However, it is not as straightforward with graph data, since splitting the dataset into smaller chunks would break essential connections between nodes. ‚ùå . So, what can we do? In recent years, researchers developed different strategies to create graph mini-batches. The one we&#39;re interested in is called neighbor sampling (from GraphSAGE&#39;s paper). There are many other techniques you can find on PyG&#39;s documentation, such as subgraph clustering (from Cluster-GCN). . &lt;/source&gt; Neighbor sampling considers only a fixed number of random neighbors. Here&#39;s the process: . We define the number of neighbors (1 hop), the number of neighbors of neighbors (2 hops), etc. we would like to have. | The sampler looks at the list of neighbors, of neighbors of neighbors, etc. of a target node and randomly selects a predefined number of them; | The sampler outputs a subgraph containing the target node and the randomly selected neighboring nodes. | This process is repeated for every node in a list or the entirety of the graph. However, creating a subgraph for each node is not efficient, that is why we can process them in batches instead. In this case, each subgraph is shared by multiple target nodes. . Neighbor sampling has an added benefit. Sometimes, we observe extremely popular nodes that act like hubs, such as celebrities on social media. Obtaining the hidden vectors of these nodes can be computationally very expensive since it requires calculating the hidden vectors of thousands or even millions of neighbors. GraphSAGE fixes this issue by simply ignoring most of the nodes! . In PyG, neighbor sampling is implemented through the NeighborLoader object. Let&#39;s say we want 5 neighbors and 10 of their neighbors (num_neighbors). As we discussed, we can also specify the batch_size to speed up the process by creating subgraphs for multiple target nodes. . from torch_geometric.loader import NeighborLoader from torch_geometric.utils import to_networkx # Create batches with neighbor sampling train_loader = NeighborLoader( data, num_neighbors=[5, 10], batch_size=16, input_nodes=data.train_mask, ) # Print each subgraph for i, subgraph in enumerate(train_loader): print(f&#39;Subgraph {i}: {subgraph}&#39;) # Plot each subgraph fig = plt.figure(figsize=(16,16)) for idx, (subdata, pos) in enumerate(zip(train_loader, [&#39;221&#39;, &#39;222&#39;, &#39;223&#39;, &#39;224&#39;])): G = to_networkx(subdata, to_undirected=True) ax = fig.add_subplot(pos) ax.set_title(f&#39;Subgraph {idx}&#39;) plt.axis(&#39;off&#39;) nx.draw_networkx(G, pos=nx.spring_layout(G, seed=0), with_labels=True, node_size=200, node_color=subdata.y, cmap=&quot;cool&quot;, font_size=10 ) plt.show() . Subgraph 0: Data(x=[389, 500], edge_index=[2, 448], y=[389], train_mask=[389], val_mask=[389], test_mask=[389], batch_size=16) Subgraph 1: Data(x=[264, 500], edge_index=[2, 314], y=[264], train_mask=[264], val_mask=[264], test_mask=[264], batch_size=16) Subgraph 2: Data(x=[283, 500], edge_index=[2, 330], y=[283], train_mask=[283], val_mask=[283], test_mask=[283], batch_size=16) Subgraph 3: Data(x=[189, 500], edge_index=[2, 229], y=[189], train_mask=[189], val_mask=[189], test_mask=[189], batch_size=12) . We created 4 subgraphs of various sizes ($60 div 16 = 4$). It allows us to process them in parallel and they&#39;re easier to fit on a GPU since they&#39;re smaller. . The number of neighbors is an important parameter since pruning our graph removes a lot of information. How much, exactly? Well, quite a lot. We can visualize this effect by looking at the node degrees. . from torch_geometric.utils import degree from collections import Counter def plot_degree(data): # Get list of degrees for each node degrees = degree(data.edge_index[0]).numpy() # Count the number of nodes for each degree numbers = Counter(degrees) # Bar plot fig, ax = plt.subplots(figsize=(18, 6)) ax.set_xlabel(&#39;Node degree&#39;) ax.set_ylabel(&#39;Number of nodes&#39;) plt.bar(numbers.keys(), numbers.values(), color=&#39;#0A047A&#39;) # Plot node degrees from the original graph plot_degree(data) # Plot node degrees from the last subgraph plot_degree(subdata) . In this example, the maximum node degree of our subgraphs is 5, which is much lower than the original max value. It&#39;s important to remember this tradeoff when talking about GraphSAGE. . PinSAGE offers another sampling solution. In this architecture, random walks are simulated to 1/ sample a fixed number of neighbors (like GraphSAGE) and 2/ obtain their relative importance (important nodes are seen more frequently than others). This strategy feels a bit like an attention mechanism since it assigns weights to nodes and increases the relevance of the most popular ones. . &#128165; B. Aggregation . The aggregation process determines how to combine the feature vectors to produce the node embeddings. The original paper presents three ways of aggregating features: . Mean aggregator | LSTM aggregator | Pooling aggregator | . We could include other aggregators (preferably fast): GraphSAGE is a flexible framework. The sampling technique is more essential than the aggregation step. . &lt;/source&gt; The mean aggregator is the simplest one. The idea is close to a GCN approach: . The hidden features of the target node $ textbf{h}_v$ are concatenated with the hidden features of its neighbors $ textbf{h}_u$; | The resulting vector is averaged; | A linear transformation with a weight matrix $ textbf{W}$ is applied. | $$ textbf{h}_v&#39; = textbf{W} cdot mean([ textbf{h}_v|| textbf{h}_u])$$ . The result can then be fed to a non-linear activation function $ sigma$ (like $tanh$ or $ReLU$). This is the technique we&#39;ll use with PyG and the one selected by UberEats. . The LSTM aggregator can seem like a weird idea because this architecture is sequential: it assigns an order to our unordered nodes. This is why the authors randomly shuffle them to force the LSTM to only consider the hidden features. It is the best performing technique in their benchmarks. . The pooling aggregator feeds each neighbor&#39;s hidden vector to a feedforward neural network. A max-pooling operation is applied to the result. . &#129504; III. GraphSAGE in PyTorch Geometric . We can easily implement a GraphSAGE architecture in PyTorch Geometric with the SAGEConv layer. This implementation is not exactly the same as in the paper since it uses 2 matrices instead of one: . $$ textbf{h}_v&#39; = textbf{W}_1 textbf{h}_v + textbf{W}_2 cdot mean( textbf{h}_u)$$ . Okay, let&#39;s create a network with two SAGEConv layers. The first one will use $ReLU$ as the activation function and a dropout layer. The second one will directly output the node embeddings. As we&#39;re dealing with a multi-class classification task, we&#39;ll use the cross-entropy loss as our loss function. . To see the benefits of GraphSAGE, let&#39;s compare it with a GCN and a GAT without any sampling. . from torch.nn import Linear, Dropout from torch_geometric.nn import SAGEConv, GATv2Conv, GCNConv import torch.nn.functional as F class GraphSAGE(torch.nn.Module): &quot;&quot;&quot;GraphSAGE&quot;&quot;&quot; def __init__(self, dim_in, dim_h, dim_out): super().__init__() self.sage1 = SAGEConv(dim_in, dim_h) self.sage2 = SAGEConv(dim_h, dim_out) self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4) def forward(self, x, edge_index): h = self.sage1(x, edge_index) h = torch.relu(h) h = F.dropout(h, p=0.5, training=self.training) h = self.sage2(h, edge_index) return h, F.log_softmax(h, dim=1) def fit(self, data, epochs): criterion = torch.nn.CrossEntropyLoss() optimizer = self.optimizer self.train() for epoch in range(epochs+1): total_loss = 0 acc = 0 val_loss = 0 val_acc = 0 # Train on batches for batch in train_loader: optimizer.zero_grad() _, out = self(batch.x, batch.edge_index) loss = criterion(out[batch.train_mask], batch.y[batch.train_mask]) total_loss += loss acc += accuracy(out[batch.train_mask].argmax(dim=1), batch.y[batch.train_mask]) loss.backward() optimizer.step() # Validation val_loss += criterion(out[batch.val_mask], batch.y[batch.val_mask]) val_acc += accuracy(out[batch.val_mask].argmax(dim=1), batch.y[batch.val_mask]) # Print metrics every 10 epochs if(epoch % 10 == 0): print(f&#39;Epoch {epoch:&gt;3} | Train Loss: {loss/len(train_loader):.3f} &#39; f&#39;| Train Acc: {acc/len(train_loader)*100:&gt;6.2f}% | Val Loss: &#39; f&#39;{val_loss/len(train_loader):.2f} | Val Acc: &#39; f&#39;{val_acc/len(train_loader)*100:.2f}%&#39;) class GAT(torch.nn.Module): &quot;&quot;&quot;Graph Attention Network&quot;&quot;&quot; def __init__(self, dim_in, dim_h, dim_out, heads=8): super().__init__() self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads) self.gat2 = GATv2Conv(dim_h*heads, dim_out, heads=heads) self.optimizer = torch.optim.Adam(self.parameters(), lr=0.005, weight_decay=5e-4) def forward(self, x, edge_index): h = F.dropout(x, p=0.6, training=self.training) h = self.gat1(x, edge_index) h = F.elu(h) h = F.dropout(h, p=0.6, training=self.training) h = self.gat2(h, edge_index) return h, F.log_softmax(h, dim=1) def fit(self, data, epochs): criterion = torch.nn.CrossEntropyLoss() optimizer = self.optimizer self.train() for epoch in range(epochs+1): # Training optimizer.zero_grad() _, out = self(data.x, data.edge_index) loss = criterion(out[data.train_mask], data.y[data.train_mask]) acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask]) loss.backward() optimizer.step() # Validation val_loss = criterion(out[data.val_mask], data.y[data.val_mask]) val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask]) # Print metrics every 10 epochs if(epoch % 10 == 0): print(f&#39;Epoch {epoch:&gt;3} | Train Loss: {loss:.3f} | Train Acc:&#39; f&#39; {acc*100:&gt;6.2f}% | Val Loss: {val_loss:.2f} | &#39; f&#39;Val Acc: {val_acc*100:.2f}%&#39;) class GCN(torch.nn.Module): &quot;&quot;&quot;Graph Convolutional Network&quot;&quot;&quot; def __init__(self, dim_in, dim_h, dim_out): super().__init__() self.gcn1 = GCNConv(dim_in, dim_h) self.gcn2 = GCNConv(dim_h, dim_out) self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4) def forward(self, x, edge_index): h = F.dropout(x, p=0.5, training=self.training) h = self.gcn1(h, edge_index) h = torch.relu(h) h = F.dropout(h, p=0.5, training=self.training) h = self.gcn2(h, edge_index) return h, F.log_softmax(h, dim=1) def fit(self, data, epochs): criterion = torch.nn.CrossEntropyLoss() optimizer = self.optimizer self.train() for epoch in range(epochs+1): # Training optimizer.zero_grad() _, out = self(data.x, data.edge_index) loss = criterion(out[data.train_mask], data.y[data.train_mask]) acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask]) loss.backward() optimizer.step() # Validation val_loss = criterion(out[data.val_mask], data.y[data.val_mask]) val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask]) # Print metrics every 10 epochs if(epoch % 10 == 0): print(f&#39;Epoch {epoch:&gt;3} | Train Loss: {loss:.3f} | Train Acc:&#39; f&#39; {acc*100:&gt;6.2f}% | Val Loss: {val_loss:.2f} | &#39; f&#39;Val Acc: {val_acc*100:.2f}%&#39;) def accuracy(pred_y, y): &quot;&quot;&quot;Calculate accuracy.&quot;&quot;&quot; return ((pred_y == y).sum() / len(y)).item() def test(model, data): &quot;&quot;&quot;Evaluate the model on test set and print the accuracy score.&quot;&quot;&quot; model.eval() _, out = model(data.x, data.edge_index) acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask]) return acc . Notice that the training loop is different between GraphSAGE and the two other GNNs: they don&#39;t use neighbor sampling, so they have a regular training loop. . With GraphSAGE, we loop through batches (our 4 subgraphs) created by the neighbor sampling process. The way we calculate the accuracy and the validation loss is also different because of that. . %%time # Create GraphSAGE graphsage = GraphSAGE(dataset.num_features, 64, dataset.num_classes) print(graphsage) # Train graphsage.fit(data, 200) # Test print(f&#39; nGraphSAGE test accuracy: {test(graphsage, data)*100:.2f}% n&#39;) . GraphSAGE( (sage1): SAGEConv(500, 64) (sage2): SAGEConv(64, 3) ) Epoch 0 | Train Loss: 0.332 | Train Acc: 30.24% | Val Loss: 1.13 | Val Acc: 18.33% Epoch 10 | Train Loss: 0.020 | Train Acc: 100.00% | Val Loss: 0.63 | Val Acc: 72.50% Epoch 20 | Train Loss: 0.005 | Train Acc: 100.00% | Val Loss: 0.57 | Val Acc: 73.17% Epoch 30 | Train Loss: 0.005 | Train Acc: 100.00% | Val Loss: 0.49 | Val Acc: 79.96% Epoch 40 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.63 | Val Acc: 63.33% Epoch 50 | Train Loss: 0.009 | Train Acc: 100.00% | Val Loss: 0.61 | Val Acc: 75.56% Epoch 60 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 0.77 | Val Acc: 71.25% Epoch 70 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 0.50 | Val Acc: 79.79% Epoch 80 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 0.54 | Val Acc: 76.74% Epoch 90 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.65 | Val Acc: 76.74% Epoch 100 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.49 | Val Acc: 78.87% Epoch 110 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.59 | Val Acc: 78.87% Epoch 120 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 0.61 | Val Acc: 73.33% Epoch 130 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.74 | Val Acc: 66.67% Epoch 140 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.74 | Val Acc: 59.35% Epoch 150 | Train Loss: 0.001 | Train Acc: 100.00% | Val Loss: 0.82 | Val Acc: 65.06% Epoch 160 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 0.73 | Val Acc: 65.00% Epoch 170 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 0.85 | Val Acc: 67.92% Epoch 180 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 0.48 | Val Acc: 81.67% Epoch 190 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.50 | Val Acc: 85.83% Epoch 200 | Train Loss: 0.001 | Train Acc: 100.00% | Val Loss: 0.52 | Val Acc: 83.54% GraphSAGE test accuracy: 77.20% CPU times: user 9.17 s, sys: 370 ms, total: 9.54 s Wall time: 12.4 s . %%time # Create GCN gcn = GCN(dataset.num_features, 64, dataset.num_classes) print(gcn) # Train gcn.fit(data, 200) # Test print(f&#39; nGCN test accuracy: {test(gcn, data)*100:.2f}% n&#39;) . GCN( (gcn1): GCNConv(500, 64) (gcn2): GCNConv(64, 3) ) Epoch 0 | Train Loss: 1.098 | Train Acc: 33.33% | Val Loss: 1.10 | Val Acc: 32.20% Epoch 10 | Train Loss: 0.736 | Train Acc: 91.67% | Val Loss: 0.87 | Val Acc: 74.60% Epoch 20 | Train Loss: 0.400 | Train Acc: 96.67% | Val Loss: 0.67 | Val Acc: 73.80% Epoch 30 | Train Loss: 0.214 | Train Acc: 93.33% | Val Loss: 0.61 | Val Acc: 76.80% Epoch 40 | Train Loss: 0.124 | Train Acc: 100.00% | Val Loss: 0.58 | Val Acc: 75.60% Epoch 50 | Train Loss: 0.092 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 77.20% Epoch 60 | Train Loss: 0.095 | Train Acc: 100.00% | Val Loss: 0.58 | Val Acc: 76.80% Epoch 70 | Train Loss: 0.087 | Train Acc: 100.00% | Val Loss: 0.58 | Val Acc: 77.20% Epoch 80 | Train Loss: 0.085 | Train Acc: 100.00% | Val Loss: 0.63 | Val Acc: 75.60% Epoch 90 | Train Loss: 0.088 | Train Acc: 98.33% | Val Loss: 0.62 | Val Acc: 76.60% Epoch 100 | Train Loss: 0.074 | Train Acc: 98.33% | Val Loss: 0.63 | Val Acc: 75.80% Epoch 110 | Train Loss: 0.085 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 76.60% Epoch 120 | Train Loss: 0.069 | Train Acc: 100.00% | Val Loss: 0.63 | Val Acc: 74.20% Epoch 130 | Train Loss: 0.062 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 76.20% Epoch 140 | Train Loss: 0.043 | Train Acc: 100.00% | Val Loss: 0.61 | Val Acc: 75.20% Epoch 150 | Train Loss: 0.045 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 75.60% Epoch 160 | Train Loss: 0.068 | Train Acc: 100.00% | Val Loss: 0.61 | Val Acc: 76.80% Epoch 170 | Train Loss: 0.070 | Train Acc: 100.00% | Val Loss: 0.60 | Val Acc: 76.80% Epoch 180 | Train Loss: 0.060 | Train Acc: 100.00% | Val Loss: 0.61 | Val Acc: 75.40% Epoch 190 | Train Loss: 0.057 | Train Acc: 100.00% | Val Loss: 0.66 | Val Acc: 75.00% Epoch 200 | Train Loss: 0.052 | Train Acc: 100.00% | Val Loss: 0.65 | Val Acc: 75.20% GCN test accuracy: 78.40% CPU times: user 52.4 s, sys: 606 ms, total: 53 s Wall time: 52.6 s . %%time # Create GAT gat = GAT(dataset.num_features, 64, dataset.num_classes) print(gat) # Train gat.fit(data, 200) # Test print(f&#39; nGAT test accuracy: {test(gat, data)*100:.2f}% n&#39;) . GAT( (gat1): GATv2Conv(500, 64, heads=8) (gat2): GATv2Conv(512, 3, heads=8) ) Epoch 0 | Train Loss: 3.174 | Train Acc: 1.67% | Val Loss: 3.18 | Val Acc: 1.00% Epoch 10 | Train Loss: 0.707 | Train Acc: 86.67% | Val Loss: 0.87 | Val Acc: 71.00% Epoch 20 | Train Loss: 0.363 | Train Acc: 93.33% | Val Loss: 0.64 | Val Acc: 77.20% Epoch 30 | Train Loss: 0.178 | Train Acc: 96.67% | Val Loss: 0.58 | Val Acc: 78.40% Epoch 40 | Train Loss: 0.101 | Train Acc: 100.00% | Val Loss: 0.56 | Val Acc: 78.40% Epoch 50 | Train Loss: 0.087 | Train Acc: 100.00% | Val Loss: 0.57 | Val Acc: 77.80% Epoch 60 | Train Loss: 0.072 | Train Acc: 100.00% | Val Loss: 0.57 | Val Acc: 78.40% Epoch 70 | Train Loss: 0.076 | Train Acc: 100.00% | Val Loss: 0.58 | Val Acc: 77.40% Epoch 80 | Train Loss: 0.064 | Train Acc: 100.00% | Val Loss: 0.59 | Val Acc: 76.40% Epoch 90 | Train Loss: 0.058 | Train Acc: 100.00% | Val Loss: 0.58 | Val Acc: 77.20% Epoch 100 | Train Loss: 0.062 | Train Acc: 100.00% | Val Loss: 0.57 | Val Acc: 79.00% Epoch 110 | Train Loss: 0.050 | Train Acc: 100.00% | Val Loss: 0.59 | Val Acc: 77.80% Epoch 120 | Train Loss: 0.044 | Train Acc: 100.00% | Val Loss: 0.60 | Val Acc: 75.40% Epoch 130 | Train Loss: 0.042 | Train Acc: 100.00% | Val Loss: 0.57 | Val Acc: 78.00% Epoch 140 | Train Loss: 0.045 | Train Acc: 100.00% | Val Loss: 0.60 | Val Acc: 78.00% Epoch 150 | Train Loss: 0.038 | Train Acc: 100.00% | Val Loss: 0.60 | Val Acc: 77.20% Epoch 160 | Train Loss: 0.041 | Train Acc: 100.00% | Val Loss: 0.64 | Val Acc: 77.00% Epoch 170 | Train Loss: 0.033 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 76.00% Epoch 180 | Train Loss: 0.031 | Train Acc: 100.00% | Val Loss: 0.62 | Val Acc: 77.60% Epoch 190 | Train Loss: 0.028 | Train Acc: 100.00% | Val Loss: 0.64 | Val Acc: 78.40% Epoch 200 | Train Loss: 0.026 | Train Acc: 100.00% | Val Loss: 0.65 | Val Acc: 76.60% GAT test accuracy: 77.10% CPU times: user 17min 43s, sys: 9.46 s, total: 17min 53s Wall time: 18min 7s . The three models obtain similar results in terms of accuracy. We expect the GAT to perform better because its aggregation mechanism is more nuanced, but it&#39;s not always the case. . The real difference is the training time: GraphSAGE is 88 times faster than the GAT and 4 times faster than the GCN in this example! . Here lies the true power of GraphSAGE. We do lose a lot of information by pruning our graph with neighbor sampling. The final node embeddings might not be as good as what we could find with a GCN or a GAT. But this is not the point: GraphSAGE is designed to improve scalability. In turn, it can lead to building larger graphs that can improve accuracy. . This work was done in a supervised training setting (node classification), but we could also train GraphSAGE in an unsupervised way. In this case, we can&#39;t use the cross-entropy loss. We have to engineer a loss function that forces nodes that are nearby in the original graph to remain close to each other in the embedding space. Conversely, the same function must ensure that distant nodes in the graph must have distant representations in the embedding space. This is the loss that is presented in GraphSAGE&#39;s paper. . In the case of PinSAGE and UberEeats&#39; modified GraphSAGE, we&#39;re dealing with recommender systems. The goal is to correctly rank the most relevant items (pins, restaurants) for each user, which is very different. We don&#39;t only want to know what the closest embeddings are, we have to produce the best rankings possible. This is why these systems are also trained in an unsupervised way, but with another loss function: a max-margin ranking loss. . &#128282; IV. Conclusion . GraphSAGE is an incredibly fast architecture to process large graphs. It might not be as accurate as a GCN or a GAT, but it is an essential model when dealing with a massive amount of data. It delivers this speed thanks to a clever combination of 1/ neighbor sampling to prune the graph and 2/ fast aggregation with a mean aggregator in this example. In this article, . We explored a new dataset with PubMed, which is several times larger than the previous one; | We explained the idea behind neighbor sampling, which only considers a predefined number of random neighbors at each hop; | We saw the three aggregators presented in GraphSAGE&#39;s paper and focused on the mean aggregator; | We benchmarked three models (GraphSAGE, GAT, and GCN) in terms of accuracy and training time. | . We saw three architectures with the same end application: node classification. But GNNs have been successfully applied to other tasks. In the next tutorials, I&#39;d like to use them in two different contexts: edge and graph prediction. This will be a good way to discover new datasets and applications where GNNs dominate the state of the art. . I hope you enjoyed this article! Feel free to follow me on Twitter @maximelabonne for more illustrations and content about Graph Neural Networks. Thanks for your attention! üì£ .",
            "url": "https://mlabonne.github.io/blog/graphsage/",
            "relUrl": "/graphsage/",
            "date": " ‚Ä¢ Apr 2, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "What is a Tensor in Deep Learning?",
            "content": "What is a tensor, exactly? . Most deep learning practitioners know about them but can&#39;t pinpoint an exact definition. . TensorFlow, PyTorch: every deep learning framework relies on the same basic object: tensors. They&#39;re used to store almost everything in deep learning: input data, weights, biases, predictions, etc. . And yet, their definition is incredibly fuzzy: the Wikipedia category alone has over 100 pages related to tensors. . In this article, we&#39;ll give a definitive answer to the following question: what is a tensor in neural networks? . &#128187; Tensors in computer science . So why are there so many definitions? . It&#39;s quite simple: different fields have different definitions. Tensors in mathematics are not quite the same as tensors in physics, which are different from tensors in computer science. . These definitions can be divided into two categories: tensors as a data structure or as objects (in an object-oriented programming sense). . Data structure: this is the definition we use in computer science. Tensors are multidimensional arrays that store a specific type of value. | Objects: this is the definition used in other fields. In mathematics and physics, tensors are not just a data structure: they also have a list of properties, like a specific product. | . This is why you see a lot of people (sometimes quite pedantically) saying &quot;tensors are not n-dimensional arrays/matrices&quot;: they don&#39;t talk about data structures, but about objects with properties. . Even the same words have different meanings. For instance, in computer science, a 2D tensor is a matrix (it&#39;s a tensor of rank 2). In linear algebra, a tensor with 2 dimensions means it only stores two values. The rank also has a completely different definition: it is the maximum number of its linearly independent column (or row) vectors. . In computer science, we&#39;re only interested in a definition focused on the data structure. From this point of view, tensors truly are a generalization in $n$ dimensions of matrices. . But we&#39;re still missing an important nuance when talking about tensors specifically in the context of deep learning... . &#129504; Tensors in deep learning . Icons created by Freepik and smashingstocks - FlaticonSo why are they called &quot;tensors&quot; instead of &quot;multidimensional arrays&quot;? Ok, it is shorter, but is it all there is to it? Actually, people make an implicit assumption when they talk about tensors. . PyTorch&#39;s official documentation gives us a practical answer: . The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either CPU or GPU. . In deep learning, we need performance to compute a lot of matrix multiplications in a highly parallel way. These matrices (and n-dimensional arrays in general) are generally stored and processed on GPUs to speed up training and inference times. . This is what was missing in our previous definition:tensors in deep learning are not just n-dimensional arrays, there&#39;s also the implicit assumption they can be run on a GPU. . &#9876;&#65039; NumPy vs PyTorch . Let&#39;s see the difference between NumPy arrays and PyTorch tensors. . These two objects are very similar: we can initialize a 1D array and a 1D tensor with nearly the same syntax. They also share a lot of methods and can be easily converted into one another. . import numpy as np import torch array = np.array([1, 2, 3]) print(f&#39;NumPy Array: {array}&#39;) tensor = torch.tensor([1, 2, 3]) print(f&#39;PyTorch Tensor: {tensor}&#39;) . NumPy Array: [1 2 3] PyTorch Tensor: tensor([1, 2, 3]) . Initializing 2D arrays and 2D tensors is not more complicated. . x = np.array([[1, 2, 3], [4, 5, 6]]) print(f&#39;NumPy Array: n{x}&#39;) x = torch.tensor([[1, 2, 3], [4, 5, 6]]) print(f&#39; nPyTorch Tensor: n{x}&#39;) . NumPy Array: [[1 2 3] [4 5 6]] PyTorch Tensor: tensor([[1, 2, 3], [4, 5, 6]]) . We said that the only difference between tensors and arrays was the fact that tensors can be run on GPUs. So in the end, this distinction is based on performance. But is this boost that important? . Let&#39;s compare the performance between NumPy arrays and PyTorch tensors on matrix multiplication. In the following example, we randomly initialize 4D arrays/tensors and multiply them. . device = torch.device(&quot;cuda&quot;) # 4D arrays array1 = np.random.rand(100, 100, 100, 100) array2 = np.random.rand(100, 100, 100, 100) # 4D tensors tensor1 = torch.rand(100, 100, 100, 100).to(device) tensor2 = torch.rand(100, 100, 100, 100).to(device) . %%timeit np.matmul(array1, array2) . 1 loop, best of 5: 1.32 s per loop . %%timeit torch.matmul(tensor1, tensor2) . 1000 loops, best of 5: 25.2 ms per loop . As we can see, PyTorch tensors completed outperformed NumPy arrays: they completed the multiplication 52 times faster! . This is the true power of tensors: they&#39;re blazingly fast! Performance might vary depending on the dimensions, the implementation, and the hardware, but this speed is the reason why tensors (and not arrays) are so common in deep learning. . &#128221; Conclusion . In this article, we wrote a definition of tensors based on: . Their use in computer science (data structure); | More specifically, in deep learning (they can run on GPUs). | Here&#39;s how we can summarize it in one sentence: . Tensors are n-dimensional arrays with the implicit assumption that they can run on a GPU. . Finally, we saw the difference of performance between tensors and arrays, which motivates the need for tensors in deep learning. . So next time someone tries to explain to you that tensors are not exactly a generalization of matrices, you&#39;ll know that they&#39;re right in a particular definition of tensors, but not in the computer science/deep learning one. . If you&#39;re looking for more data science and machine learning content in n-dimensions, please follow me on twitter @maximelabonne. You can find the code used in this article at this address. üì£ .",
            "url": "https://mlabonne.github.io/blog/what-is-a-tensor/",
            "relUrl": "/what-is-a-tensor/",
            "date": " ‚Ä¢ Mar 28, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Efficiently iterating over rows in a Pandas DataFrame",
            "content": "When I started machine learning, I followed the guidelines and created my own features by combining multiple columns in my dataset. It&#39;s all well and good, but the way I did it was horribly inefficient. I had to wait several minutes to do the most basic operations. . My problem was simple: I didn&#39;t know the fastest way iterate over rows in Pandas. . I often see people online using the same techniques I used to apply. It&#39;s not elegant but it&#39;s ok if you don&#39;t have much data. However, if you process more than 10k rows, it quickly becomes an obvious performance issue. . In this article, I&#39;m gonna give you the best way to iterate over rows in a Pandas DataFrame, with no extra code required. It&#39;s not just about performance: it&#39;s also about understanding what&#39;s going on under the hood to become a better data scientist. . Let&#39;s import a dataset in Pandas. In this case, I chose the one I worked on when I started: it&#39;s time to fix my past mistakes! ü©π . import pandas as pd import numpy as np df = pd.read_csv(&#39;https://raw.githubusercontent.com/mlabonne/how-to-data-science/main/data/nslkdd_test.txt&#39;) df . duration protocol_type service flag src_bytes dst_bytes land wrong_fragment urgent hot ... dst_host_same_srv_rate dst_host_diff_srv_rate dst_host_same_src_port_rate dst_host_srv_diff_host_rate dst_host_serror_rate dst_host_srv_serror_rate dst_host_rerror_rate dst_host_srv_rerror_rate attack_type other . 0 0 | tcp | private | REJ | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0.04 | 0.06 | 0.00 | 0.00 | 0.00 | 0.0 | 1.00 | 1.00 | neptune | 21 | . 1 0 | tcp | private | REJ | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0.00 | 0.06 | 0.00 | 0.00 | 0.00 | 0.0 | 1.00 | 1.00 | neptune | 21 | . 2 2 | tcp | ftp_data | SF | 12983 | 0 | 0 | 0 | 0 | 0 | ... | 0.61 | 0.04 | 0.61 | 0.02 | 0.00 | 0.0 | 0.00 | 0.00 | normal | 21 | . 3 0 | icmp | eco_i | SF | 20 | 0 | 0 | 0 | 0 | 0 | ... | 1.00 | 0.00 | 1.00 | 0.28 | 0.00 | 0.0 | 0.00 | 0.00 | saint | 15 | . 4 1 | tcp | telnet | RSTO | 0 | 15 | 0 | 0 | 0 | 0 | ... | 0.31 | 0.17 | 0.03 | 0.02 | 0.00 | 0.0 | 0.83 | 0.71 | mscan | 11 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 22539 0 | tcp | smtp | SF | 794 | 333 | 0 | 0 | 0 | 0 | ... | 0.72 | 0.06 | 0.01 | 0.01 | 0.01 | 0.0 | 0.00 | 0.00 | normal | 21 | . 22540 0 | tcp | http | SF | 317 | 938 | 0 | 0 | 0 | 0 | ... | 1.00 | 0.00 | 0.01 | 0.01 | 0.01 | 0.0 | 0.00 | 0.00 | normal | 21 | . 22541 0 | tcp | http | SF | 54540 | 8314 | 0 | 0 | 0 | 2 | ... | 1.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.0 | 0.07 | 0.07 | back | 15 | . 22542 0 | udp | domain_u | SF | 42 | 42 | 0 | 0 | 0 | 0 | ... | 0.99 | 0.01 | 0.00 | 0.00 | 0.00 | 0.0 | 0.00 | 0.00 | normal | 21 | . 22543 0 | tcp | sunrpc | REJ | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0.08 | 0.03 | 0.00 | 0.00 | 0.00 | 0.0 | 0.44 | 1.00 | mscan | 14 | . 22544 rows √ó 43 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; This dataset has 22k rows and 43 columns with a combination of categorical and numerical values. Each row describes a connection between two computers. . Let&#39;s say we want to create a new feature: the total number of bytes in the connection. We just have to sum up two existing features: src_bytes and dst_bytes. Let&#39;s see different methods to calculate this new feature. . &#10060;&#10060; 1. Iterrows . According to the official documentation, iterrows() iterates &quot;over the rows of a Pandas DataFrame as (index, Series) pairs&quot;. It converts each row into a Series object, which causes two problems: . It can change the type of your data (dtypes); | The conversion greatly degrades performance. | For these reasons, the ill-named iterrows() is the WORST possible method to actually iterate over rows. . %%timeit -n 10 # Iterrows total = [] for index, row in df.iterrows(): total.append(row[&#39;src_bytes&#39;] + row[&#39;dst_bytes&#39;]) . 10 loops, best of 5: 1.07 s per loop . Now let&#39;s see slightly better techniques... . &#10060; 2. For loop with .loc or .iloc (3&#215; faster) . This is what I used to do when I started: a basic for loop to select rows by index (with .loc or .iloc). . Why is it bad? Because DataFrames are not designed for this purpose. As with the previous method, rows are converted into Pandas Series objects, which degrades performance. . Interestingly enough, .iloc is faster than .loc. It makes sense since Python doesn&#39;t have to check user-defined labels and directly look at where the row is stored in memory. . %%timeit -n 10 # For loop with .loc total = [] for index in range(len(df)): total.append(df[&#39;src_bytes&#39;].loc[index] + df[&#39;dst_bytes&#39;].loc[index]) . 10 loops, best of 5: 600 ms per loop . %%timeit -n 10 # For loop with .iloc total = [] for index in range(len(df)): total.append(df[&#39;src_bytes&#39;].iloc[index] + df[&#39;dst_bytes&#39;].iloc[index]) . 10 loops, best of 5: 377 ms per loop . Even this basic for loop with .iloc is 3 times faster than the first method! . &#10060; 3. Apply (4&#215; faster) . The apply() method is another popular choice to iterate over rows. It creates code that is easy to understand but at a cost: performance is nearly as bad as the previous for loop. . This is why I would strongly advise you to avoid this function for this specific purpose (it&#39;s fine for other applications). . Note that I convert the DataFrame into a list using the to_list() method to obtain identical results. . %%timeit -n 10 # Apply df.apply(lambda row: row[&#39;src_bytes&#39;] + row[&#39;dst_bytes&#39;], axis=1).to_list() . 10 loops, best of 5: 282 ms per loop . The apply() method is a for loop in disguise, which is why the performance doesn&#39;t improve that much: it&#39;s only 4 times faster than the first technique. . &#10060; 4. Itertuples (10&#215; faster) . If you know about iterrows(), you probably know about itertuples(). According to the official documentation, it iterates &quot;over the rows of a DataFrame as namedtuples of the values&quot;. In practice, it means that rows are converted into tuples, which are much lighter objects than Pandas Series. . This is why itertuples() are a better version of iterrows(). The only issue is that we need to select columns based on their index, which is not as user-friendly as previous techniques. . %%timeit -n 10 # Itertuples total = [] for row in df.itertuples(): total.append(row[5] + row[6]) . 10 loops, best of 5: 99.3 ms per loop . Okay, it&#39;s 10 times faster than iterrows(): this is starting to look better. . &#10060; 5. List comprehensions (200&#215; faster) . List comprehensions are a fancy way to iterate over a list as a one-liner. For instance, [print(i) for i in range(10)] prints numbers from 0 to 9 without any explicit for loop. . I say &quot;explicit&quot; because Python actually processes it as a for loop if we look at the bytecode. So why is it faster? Quite simply because we don&#39;t call the .append() method in this version. . %%timeit -n 100 # List comprehension [src + dst for src, dst in zip(df[&#39;src_bytes&#39;], df[&#39;dst_bytes&#39;])] . 100 loops, best of 5: 5.54 ms per loop . Indeed, this technique is 200 times faster than the first one! But we can still do better. . &#9989; 6. Pandas vectorization (1500&#215; faster) . Until now, all the techniques used simply add up single values. Instead of adding single values, why not group them into vectors to sum them up? The difference between adding two numbers or two vectors is not significant for a CPU, which should speed things up. . On top of that, Pandas can process Series objects in parallel, using every CPU core available! . The syntax is also the simplest imaginable: this solution is extremely intuitive. Under the hood, Pandas takes care of vectorizing our data with an optimized C code using contiguous memory blocks. . %%timeit -n 1000 # Vectorization (df[&#39;src_bytes&#39;] + df[&#39;dst_bytes&#39;]).to_list() . 1000 loops, best of 5: 734 ¬µs per loop . This code is 1500 times faster than iterrows() and it is even simpler to write. . &#9989;&#9989; 7. NumPy vectorization (1900&#215; faster) . NumPy is designed to handle scientific computing. It has less overhead than Pandas methods since rows and dataframes all become np.array. It relies on the same optimizations as Pandas vectorization. . There are two ways of converting a Series into a np.array: using .values or .to_numpy(). The former has been deprecated for years, which is why we&#39;re gonna use .to_numpy() in this example. . %%timeit -n 1000 # Numpy vectorization (df[&#39;src_bytes&#39;].to_numpy() + df[&#39;dst_bytes&#39;].to_numpy()).tolist() . 1000 loops, best of 5: 575 ¬µs per loop . We found our winner with a technique that is 1900 times faster than our first competitor! Let&#39;s wrap things up. . &#127942; Conclusion . Don&#39;t be like me: if you need to iterate over rows in a DataFrame, vectorization is the way to go! You can find the code to reproduce the experiments at this address. Vectorization is not harder to read, it doesn&#39;t take longer to write, and the performance gain is incredible. . It&#39;s not just about performance: understanding how each method works under the hood helped me to write better code. Performance gains are always based on the same techniques: transforming data into vectors and matrices to take advantage of parallel processing. Alas, this is often at the expense of readability. But it doesn&#39;t have to be. . Iterating over rows is just an example but it shows that, sometimes, you can have the cake and eat it. üéÇ . If you liked this article, follow me on Twitter @maximelabonne for more tips about data science and machine learning! . &#128200; Bonus . We can measure the performance of each method depending on the size of the DataFrame. I reimplemented all of them in this dummy example using perfplot to show that the leaderboard might be different under 300 rows. Anyway, such a dataset would be so small that we wouldn&#39;t need much optimization. . !pip install -q perfplot import perfplot import matplotlib.pyplot as plt plt.rcParams.update({&#39;font.size&#39;: 22}) # Techniques def forloop(df): total = [] for index in range(len(df)): total.append(df[&#39;col1&#39;].iloc[index] + df[&#39;col2&#39;].iloc[index]) return total def itertuples(df): total = [] for row in df.itertuples(): total.append(row[1] + row[2]) return total def iterrows(df): total = [] for index, row in df.iterrows(): total.append(row[&#39;col1&#39;] + row[&#39;col2&#39;]) return total def apply(df): return df.apply(lambda row: row[&#39;col1&#39;] + row[&#39;col2&#39;], axis=1).to_list() def comprehension(df): return [src + dst for src, dst in zip(df[&#39;col1&#39;], df[&#39;col2&#39;])] def pd_vectorize(df): return (df[&#39;col1&#39;] + df[&#39;col2&#39;]).to_list() def np_vectorize(df): return (df[&#39;col1&#39;].to_numpy() + df[&#39;col2&#39;].to_numpy()).tolist() # Perfplot functions = [iterrows, forloop, apply, itertuples, comprehension, pd_vectorize, np_vectorize] df = pd.DataFrame({&#39;col1&#39;: [1, 2], &#39;col2&#39;: [3, 4]}) out = perfplot.bench( setup=lambda n: pd.concat([df]*n, ignore_index=True), kernels=functions, labels=[str(f.__name__) for f in functions], n_range=[2**n for n in range(20)], xlabel=&#39;Number of rows&#39;, ) plt.figure(figsize=(20,12)) out.show() . . . (8.117999999999996e-06, 51.558470168) .",
            "url": "https://mlabonne.github.io/blog/iterating-over-rows-pandas-dataframe/",
            "relUrl": "/iterating-over-rows-pandas-dataframe/",
            "date": " ‚Ä¢ Mar 21, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Graph Attention Networks: Self-Attention for GNNs",
            "content": "In the previous article, we implemented our first Graph Neural Network (GNN) with a graph convolutional layer. We saw that nodes have features, but we can get even more information by looking at their neighbors&#39; features. Graph convolutional layers contextualize nodes by aggregating the features of the nodes they&#39;re connected to. In practice, they simply calculate the weighted average of these features and normalize the result by the number of neighbors. The output of this operation is called an embedding. During training, Graph Convolutional Networks (GCNs) learn the best weights (i.e., how to create the best embeddings) to accomplish a task, like node classification. . This approach works well, but there&#39;s something quite weird when you think about it: we apply the exact same weights to every node. It means that we do not distinguish nodes: they all have the same importance to our GNN. Indeed, we divide the result by the number of neighbors, but it feels like some nodes should be more essential than others, regardless of their neighbors. Actually, it&#39;s quite surprising that we managed to correctly classify members of the karate club without this component. . What we&#39;re looking for is called self-attention, a powerful technique that recently became super popular in deep learning. In this article, we&#39;re gonna work with a new dataset of scientific publications and explore its graph. We will introduce the self-attention mechanism and how it works for GNNs. Finally, we&#39;ll use it in PyTorch Geometric (PyG) and compare the results of our Graph Attention Network (GAT) with those of a Graph Convolutional Network (GCN). First, let&#39;s install the required Python libraries like in the previous article. . import torch torchversion = torch.__version__ # Install PyTorch Scatter, PyTorch Sparse, and PyTorch Geometric !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torchversion}.html !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torchversion}.html !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git # Numpy for matrices import numpy as np np.random.seed(0) # Visualization import networkx as nx from sklearn.manifold import TSNE import matplotlib.pyplot as plt . &#127760; I. Graph data . There are three classic graph datasets we can use for this work. They can be found on the official website of the authors, the LINQS group at the University of California, Santa Cruz. Each of these datasets consists of a network of research papers, where each connection represents a citation. In other words, $u leftrightarrow v$ means that $u$ cited $v$ or $v$ cited $u$ (the graph is not directed). Each paper has features, which represent the presence of specific words. Based on these connections and features, the goal is to correctly classify nodes into predefined categories. The three datasets are the following: . Cora: this dataset consists of 2708 machine learning papers that belong to one of the following 7 categories: case-based reasoning, genetic algorithms, neural networks, probabilistic methods, reinforcement learning, rule learning, and theory. Node features represent the presence (1) or absence (0) of 1433 words in a paper (it&#39;s a binary bag of words). | CiteSeer: it is a bigger but similar dataset of 3312 scientific papers to classify into one of the following 6 categories: agents, AI, DB, IR, ML, and HCI. Once again, node features represent the presence (1) or absence (0) of 3703 words in a paper. | PubMed: it is an even bigger dataset with 19717 scientific publications about diabetes from PubMed&#39;s database. The goal is to classify them into one of the following 3 categories: diabetes mellitus experimental, diabetes mellitus type 1, diabetes mellitus type 2. Node features were calculated differently this time: instead of 0/1-valued word vector indicating the absence/presence of the corresponding word, they&#39;re described by a TF-IDF weighted word vector from a dictionary of 500 unique words. | . These datasets have been widely used by the scientific community. As a challenge, we can compare our future results to those obtained in the literature by Multilayer Perceptrons (MLPs), Graph Convolutional Networks (GCNs), and Graph Attention Networks (GATs). The following table summarizes classification accuracies for each model on each dataset: . Dataset üìùCora üìùCiteSeer üìùPubMed . MLP | 55.1% | 46.5% | 71.4% | . GCN | 81.5% | 70.3% | 79.0% | . GAT | 83.0% | 72.5% | 79.0% | . . PubMed is quite large so it would take longer to process it and train a GNN on it. Cora is the most studied one in the literature, so let&#39;s focus on CiteSeer as a middle ground. We can directly import any of them in PyTorch Geometric with the Planetoid class (we just have to change name to &quot;Cora&quot; or &quot;PubMed&quot;). . from torch_geometric.datasets import Planetoid # Import dataset from PyTorch Geometric dataset = Planetoid(root=&quot;.&quot;, name=&quot;CiteSeer&quot;) . Let&#39;s explore this dataset and check if our information was correct... . data = dataset[0] # Print information about the dataset print(f&#39;Dataset: {dataset}&#39;) print(&#39;-&#39;) print(f&#39;Number of graphs: {len(dataset)}&#39;) print(f&#39;Number of nodes: {data.x.shape[0]}&#39;) print(f&#39;Number of features: {dataset.num_features}&#39;) print(f&#39;Number of classes: {dataset.num_classes}&#39;) # Print information about the graph print(f&#39; nGraph:&#39;) print(&#39;&#39;) print(f&#39;Edges are directed: {data.is_directed()}&#39;) print(f&#39;Graph has isolated nodes: {data.has_isolated_nodes()}&#39;) print(f&#39;Graph has loops: {data.has_self_loops()}&#39;) . Dataset: CiteSeer() - Number of graphs: 1 Number of nodes: 3327 Number of features: 3703 Number of classes: 6 Graph: Edges are directed: False Graph has isolated nodes: True Graph has loops: False . Interestingly enough, we have 3327 nodes instead of 3312! I don&#39;t exactly know what happened, but PyTorch Geometric actually uses this paper&#39;s implementation of CiteSeer, which also displays 3327 nodes. We could investigate a little further, but we would get even more sidetracked, so let&#39;s pretend the mystery is solved for now... . More importantly, we have the correct number of classes and features. The edges are not directed and the graph doesn&#39;t have any loop, but some nodes are isolated! Based on what we know from this dataset, we can assume that correctly classifying these isolated nodes will be a challenge since we cannot rely on any aggregation or connections. It could even maybe explain why CiteSeer obtains lower accuracy scores than the two other datasets. There are several ways of counting the number of isolated nodes, such as using PyG&#39;s utility function remove_isolated_nodes. . from torch_geometric.utils import remove_isolated_nodes (remove_isolated_nodes(data[&#39;edge_index&#39;])[2] == False).sum(dim=0).item() . 48 . We see 48 isolated nodes in CiteSeer, which is just over 1% of the total dataset. Surely, it doesn&#39;t help but we cannot entirely blame them for the poor accuracy scores. Indeed, it cannot explain a difference of about 10% compared to the other datasets. Maybe there&#39;s another problem, or maybe the data is just of poor quality. ¬Ø _(„ÉÑ)_/¬Ø . To finish the data exploration stage, I&#39;d like to visualize our dataset. However, this step is quite computationally intensive so I&#39;m gonna comment the code (feel free to execute it if you want) and directly display the output. Actually, the first image of this article is also a representation of CiteSeer, made with yEd Live. . # from torch_geometric.utils import to_networkx # G = to_networkx(data, to_undirected=True) # plt.figure(figsize=(18,18)) # plt.axis(&#39;off&#39;) # nx.draw_networkx(G, # pos=nx.spring_layout(G, seed=0), # with_labels=False, # node_size=50, # node_color=data.y, # width=2, # edge_color=&quot;grey&quot; # ) # plt.show() . . This graph looks much more complex than Zachary&#39;s karate club. We can see a massive cluster of interconnected papers in the center, and small communities and isolated nodes on its periphery. This could be the real challenge of CiteSeer: these small communities might not provide enough information to correctly classify them. It would be interesting to check this hypothesis later with the results. . The graph gives us information about the number of connections of each node. But we can directly plot it in a more rigorous and presentable way with degree: . from torch_geometric.utils import degree from collections import Counter plt.rcParams[&#39;figure.dpi&#39;] = 300 plt.rcParams.update({&#39;font.size&#39;: 24}) # Get list of degrees for each node degrees = degree(data.edge_index[0]).numpy() # Count the number of nodes for each degree numbers = Counter(degrees) # Bar plot fig, ax = plt.subplots(figsize=(18, 6)) ax.set_xlabel(&#39;Node degree&#39;) ax.set_ylabel(&#39;Number of nodes&#39;) ax.set_facecolor(&#39;#EFEEEA&#39;) plt.bar(numbers.keys(), numbers.values(), color=&#39;#0A047A&#39;) . &lt;BarContainer object of 32 artists&gt; . We see that most nodes have 1 or 2 neighbors: this is not a lot! Hopefully, it will be enough for our GAT to get good results... . Alright, I think we built a good understanding of the dataset. We could also check the words in the dictionary and the different classes, but this is not what we&#39;re interested in in this article. Let&#39;s talk about self-attention instead. . &#9888;&#65039; II. Self-attention . A. How to calculate attention scores . Self-attention in GNNs was introduced by Veliƒçkoviƒá et al. in 2017. It relies on a simple idea: neighboring nodes shouldn&#39;t all have the same importance. The easiest way to achieve it is to assign a weighting factor (attention score) to each neighbor. What&#39;s even cooler with this approach is that we don&#39;t even need to normalize the result anymore, since the weighting factors can implicitly calculate it. Let&#39;s call $ alpha_{ij}$ the attention score between the nodes $i$ and $j$. We can replace the graph convolutional operation as follows: . $$h_i = sum_{j in mathcal{N}_i} dfrac{1}{ sqrt{deg(i)} sqrt{deg(j)}} mathbf{W} x_j$$ . becomes $$h_i= sum_{j in mathcal{N}_i} alpha_{ij} mathbf{W}x_j$$ . Perfect, it looks cleaner! Now that we understand why we want self-attention, how is it calculated? We could try to do it by ourselves, but there&#39;s a more clever way of doing it: we can let the GNN learn the best attention scores. Think about it about it for a second: currently, the graph convolutional layer just learns the best common weight matrix $ mathbf{W}$ for its task. But it could become more flexible by also learning the best attention scores to achieve it. . Let&#39;s take an example with the previous graph: we want to calculate $h_1$ the embedding of node 1. We still have shared weights $ mathbf{W}$ and feature vectors $x_1$, $x_2$, $x_3$, and $x_4$. Here&#39;s what the calculation looks like with the additional attention scores: . $$h_1 = alpha_{11} mathbf{W}x_1 + alpha_{12} mathbf{W}x_2 + alpha_{13} mathbf{W}x_3 + alpha_{14} mathbf{W}x_4$$ . In the previous article, we just normalized this sum (albeit in a clever way) and that was it. But we can refine these hidden feature vectors to extract more information out of them. Let&#39;s call them $ tilde{h_i}$ to show that they can be considered as inputs. . $$ tilde{h_i} = mathbf{W}x_i tag{1}$$ . The previous equation can be rewritten as follows: . $$h_1 = alpha_{11} tilde{h_1} + alpha_{12} tilde{h_2} + alpha_{13} tilde{h_3} + alpha_{14} tilde{h_4}$$ . Okay, what do we do with these hidden feature vectors? We can&#39;t just apply a second linear transformation on them: it wouldn&#39;t be very useful. If we remember the concept of attention scores, they are not weights you apply to node features, but to connections. It means that we need pairs of hidden feature vectors instead of single ones. . An easy way to create these pairs is to concatenate vectors. Only then can we apply a linear transformation with a dedicated matrix $W_{att}$ and, finally, an activation function (the authors chose $LeakyReLU$ in this case). . $$e_{ij} = LeakyReLU(W_{att}^t[ tilde{h}_i mathbin Vert tilde{h}_j]) tag{2}$$ . Oops, I lied: there&#39;s an extra step to obtain the actual $ alpha_{ij}$. What we calculated are un-normalized attention scores. But to make them easily comparable across different nodes, we have to normalize them with a $softmax$ function. . $$ alpha_{ij} = softmax_j(e_{ij}) = frac{exp(e_{ij})}{ sum_{k in mathcal{N}_i}{exp(e_{ik})}} tag{3}$$ . Here you have it: how to calculate every $ alpha_{ij}$. The only problem is... self-attention is not very stable. In order to improve performance, Vaswani et al. introduced multi-head attention in the Transformer architecture. Wait, Transformer? This is only slightly surprising since we&#39;ve only been talking about self-attention so far, but in reality Transformers are Graph Neural Networks in disguise. Their architecture is a little more complicated (they have three attention weight matrices for instance), but this is a topic for another day. Feel free to watch this video if you&#39;re interested in Transformers. . In GATs, multi-head attention consists of replicating the same operation several times in order to average or concatenate the results. That&#39;s it. Instead of a single $h_1$, we get one $h_1^k$ per attention head. One of the two following schemes can then be applied: . Average: we sum the different $h_i^k$ and normalize the result by the number of attention heads $n$; | . $$h_i = frac{1}{n} sum_{k=1}^n{h_i^k}$$ . Concatenation: we concatenate the different $h_i^k$. | . $$h_i = mathbin Vert_{k=1}^n{h_i^k}$$ . In practice, people tend to use the concatenation scheme when the graph attention layer is a hidden layer, and the average scheme when it&#39;s the last one. . &lt;/source&gt; B. (Optional) Reimplementing a GAT from scratch . Now that we know the theory, let&#39;s try to translate these equations into matrix multiplications. This part is optional since it&#39;s more advanced. You don&#39;t need to know how to reimplement a graph attention network from scratch in NumPy, but it can help you to better understand the nitty-gritty details. . We&#39;re going to use a graph with 4 nodes, whose feature vectors are just one-hot encoding of their indexes. We randomly initialize two matrices: W for the first linear transformation, and W_att for the attention weight matrix. . in_dim = 4 out_dim = 2 # Node features (one-hot encoding) X = np.eye(in_dim, in_dim) print(&#39;X =&#39;) print(X) # Adjacency matrix A = np.array([ [0, 1, 0, 1], # 1 -&gt; 2, 4 [1, 0, 1, 0], # 2 -&gt; 1, 3 [0, 1, 0, 0], # 3 -&gt; 2 [1, 0, 0, 0] # 4 -&gt; 1 ]) # Adjacency matrix with self-loops A = A + np.identity(A.shape[0]) print(&#39; nA_tilde =&#39;) print(A) # Randomly initialize the weight matrix W = np.random.uniform(-1, 1, (out_dim, X.shape[0])) print(&#39; nW =&#39;) print(W) # Randomly initialize the attention weight matrix W_att = np.random.uniform(-1, 1, (1, out_dim*2)) print(&#39; nAttention weight matrix W_att =&#39;) print(W_att) . X = [[1. 0. 0. 0.] [0. 1. 0. 0.] [0. 0. 1. 0.] [0. 0. 0. 1.]] A_tilde = [[1. 1. 0. 1.] [1. 1. 1. 0.] [0. 1. 1. 0.] [1. 0. 0. 1.]] W = [[ 0.09762701 0.43037873 0.20552675 0.08976637] [-0.1526904 0.29178823 -0.12482558 0.783546 ]] Attention weight matrix W_att = [[ 0.92732552 -0.23311696 0.58345008 0.05778984]] . Okay, we initialized all the variables. If we look back at the GAT equations, we have: . $$ tilde{h_i} = mathbf{W}x_i tag{1}$$ . As in regular neural networks, the first step is to calculate the linear transformation $ mathbf{W}X$ to obtain the hidden features $h$. Note that these hidden features only have 2 dimensions (out_dim) instead of 4 (in_dim). Usually, you want to increase the number of dimensions, but I chose 2 in this example to keep things readable. . h = X @ W.T print(&#39;h =&#39;) print(h) . h = [[ 0.09762701 -0.1526904 ] [ 0.43037873 0.29178823] [ 0.20552675 -0.12482558] [ 0.08976637 0.783546 ]] . In a simple feedforward layer, we would just feed $ tilde{h}$ to a non-linear function like $tanh$ or $ReLU$ and that would be it. But we have more work to do with this graph attention layer. The second equation calculates the un-normalized attention scores: . $$e_{ij} = LeakyReLU(W_{att}^t[ tilde{h}_i mathbin Vert tilde{h}_j]) tag{2}$$ . There are three operations in this equation: 1/ concatenating $ tilde{h}_i$ and $ tilde{h}_j$; 2/ multiplying $W_{att}^t $ by the result of the concatenation; 3/ applying the $LeakyReLU$ function to the result of the previous operation. . First, let&#39;s concatenate pairs of hidden features $ tilde{h}_i$ and $ tilde{h}_j$ from source and destination nodes. A simple way of obtaining pairs of source and destination nodes is to look at our adjacency matrix $ tilde{A}$ in COO format: rows store source nodes, and columns store destination nodes. Numpy provides a quick and efficient way of doing it with np.where(). . connections = np.where(A &gt; 0) print(f&#39;Source nodes = {connections[0]}&#39;) print(f&#39;Destination nodes = {connections[1]}&#39;) . Source nodes = [0 0 0 1 1 1 2 2 3 3] Destination nodes = [0 1 3 0 1 2 1 2 0 3] . Now we can just concatenate the hidden features of these nodes with np.concatenate(). . print(&#39;Hidden features of source nodes =&#39;) print(h[connections[0]]) print(&#39; nHidden features of destination nodes =&#39;) print(h[connections[1]]) concat = np.concatenate([h[connections[0]], h[connections[1]]], axis=1) print(&#39; nConcatenated hidden features =&#39;) print(concat) . Hidden features of source nodes = [[ 0.09762701 -0.1526904 ] [ 0.09762701 -0.1526904 ] [ 0.09762701 -0.1526904 ] [ 0.43037873 0.29178823] [ 0.43037873 0.29178823] [ 0.43037873 0.29178823] [ 0.20552675 -0.12482558] [ 0.20552675 -0.12482558] [ 0.08976637 0.783546 ] [ 0.08976637 0.783546 ]] Hidden features of destination nodes = [[ 0.09762701 -0.1526904 ] [ 0.43037873 0.29178823] [ 0.08976637 0.783546 ] [ 0.09762701 -0.1526904 ] [ 0.43037873 0.29178823] [ 0.20552675 -0.12482558] [ 0.43037873 0.29178823] [ 0.20552675 -0.12482558] [ 0.09762701 -0.1526904 ] [ 0.08976637 0.783546 ]] Concatenated hidden features = [[ 0.09762701 -0.1526904 0.09762701 -0.1526904 ] [ 0.09762701 -0.1526904 0.43037873 0.29178823] [ 0.09762701 -0.1526904 0.08976637 0.783546 ] [ 0.43037873 0.29178823 0.09762701 -0.1526904 ] [ 0.43037873 0.29178823 0.43037873 0.29178823] [ 0.43037873 0.29178823 0.20552675 -0.12482558] [ 0.20552675 -0.12482558 0.43037873 0.29178823] [ 0.20552675 -0.12482558 0.20552675 -0.12482558] [ 0.08976637 0.783546 0.09762701 -0.1526904 ] [ 0.08976637 0.783546 0.08976637 0.783546 ]] . Then, we have to multiply the attention weight matrix with this result. Technically, we&#39;re calculating its transpose to simplify calculations but this is not important. . a = W_att @ concat.T print(a) . [[ 0.17426327 0.39409364 0.22378193 0.37921693 0.5990473 0.44378135 0.48765606 0.33239011 -0.05127869 -0.00176003]] . Now we can apply the $ReLU$ function to the new result. . def leaky_relu(x, alpha=0.2): return np.maximum(alpha*x, x) e = leaky_relu(a) print(&#39;e =&#39;) print(e) . e = [[ 1.74263270e-01 3.94093638e-01 2.23781929e-01 3.79216929e-01 5.99047297e-01 4.43781347e-01 4.87656061e-01 3.32390111e-01 -1.02557380e-02 -3.52006176e-04]] . There&#39;s a slight technical problem with e: the un-normalized attention scores $e_{ij}$ are not properly stored in a matrix. It means we cannot multiply it with the other matrices. Fortunately enough, we know the sources $i$ and destinations $j$ thanks to connections. So the first value in e corresponds to $e_{00}$, the second value to $e_{01}$, the third one to $e_{03}$, etc. . E = np.zeros(A.shape) E[connections[0], connections[1]] = e[0] print(&#39;E =&#39;) print(E) . E = [[ 1.74263270e-01 3.94093638e-01 0.00000000e+00 2.23781929e-01] [ 3.79216929e-01 5.99047297e-01 4.43781347e-01 0.00000000e+00] [ 0.00000000e+00 4.87656061e-01 3.32390111e-01 0.00000000e+00] [-1.02557380e-02 0.00000000e+00 0.00000000e+00 -3.52006176e-04]] . Perfect, we have the correct un-normalized attention matrix $E$. The final step is just to normalize the attention scores of every neighbor. . $$ alpha_{ij} = softmax_j(e_{ij}) = frac{exp(e_{ij})}{ sum_{k in mathcal{N}_i}{exp(e_{ik})}} tag{3} $$I had to reimplement a softmax function for NumPy arrays (and fail a few times in the process), but this is the result $W_{ alpha}$: . def softmax2D(x, axis): e = np.exp(x - np.expand_dims(np.max(x, axis=axis), axis)) sum = np.expand_dims(np.sum(e, axis=axis), axis) return e / sum W_alpha = softmax2D(E, 1) print(&#39;W_alpha =&#39;) print(W_alpha) . W_alpha = [[0.24173822 0.30117327 0.2030784 0.25401011] [0.25019035 0.31170349 0.26687661 0.17122955] [0.19909248 0.32422104 0.27759399 0.19909248] [0.24810387 0.25066145 0.25066145 0.25057323]] . Alright, we can now combine our different matrices to translate the graph attention layer equation in terms of matrix multiplications. . $$h_i= sum_{j in mathcal{N}_i} alpha_{ij} mathbf{W}x_j$$ . becomes $$H = tilde{A}^TW_{ alpha}X mathbf{W}^T$$ . Where each row $i$ of $H$ contains the embedding of the node $i$. . H = A.T @ W_alpha @ X @ W.T print(&#39;H =&#39;) print(H) . H = [[0.65266686 0.57855356] [0.68045463 0.56380155] [0.46269627 0.33915461] [0.42387004 0.42495061]] . Well done, we calculated the hidden features of our 4 nodes with NumPy from scratch! . Multi-head attention consists of doing the same operation for each head with a different attention weight matrix. Therefore, each head outputs its own hidden feature matrix $H_i$. Our two previous schemes can be implemented as follows: . Average: we sum the hidden feature matrices and normalize the result by the number of attention heads $n$; | . $$H = frac{1}{n} sum_{i=1}^n{H_i}$$ . Concatenation: we concatenate the different $H_i$, which means that the output dimension will be $n times out _dim$ instead of $out _dim$. | . $$H = mathbin Vert_{i=1}^n{H_i}$$ . We&#39;re really done now, no more matrix multiplications! We&#39;re gonna use PyTorch Geometric, which abstracts all these calculations into a single customizable layer. . &#129504; III. Graph Attention Networks . There are two graph attention layers in PyTorch Geometric: GATConv and GATv2Conv. What we talked about so far is the GatConv layer, but in 2021 Brody et al. introduced a simple improvement by modifying the order of operations. The weight matrix $ mathbf{W}$ is applied after the concatenation and the attention weight matrix after the $LeakyReLU$ function. In summary: . GatConv: $e_{ij} = LeakyReLU(W_{att}^t[ mathbf{W}x_i mathbin Vert mathbf{W}x_j])$ | Gatv2Conv: $e_{ij} = W_{att}^tLeakyReLU( mathbf{W}[x_i mathbin Vert x_j])$ | . Which one should you use? According to numerous experiments, Gatv2Conv consistently outperforms GatConv and thus should be preferred. . Let&#39;s finally classify the papers from CiteSeer! I tried to roughly reproduce the experiments of the original authors without adding too much complexity. You can find the official implementation of GAT on GitHub. Note that we use graph attention layers in two configurations: the first one concatenates 8 outputs (multi-head attention), and the second one only has 1 head, which produces our final embeddings. We&#39;re also gonna train and test a GCN to compare the accuracy scores and see if it corresponds to the authors&#39; claims. . import torch.nn.functional as F from torch.nn import Linear, Dropout from torch_geometric.nn import GCNConv, GATv2Conv class GCN(torch.nn.Module): &quot;&quot;&quot;Graph Convolutional Network&quot;&quot;&quot; def __init__(self, dim_in, dim_h, dim_out): super().__init__() self.gcn1 = GCNConv(dim_in, dim_h) self.gcn2 = GCNConv(dim_h, dim_out) self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4) def forward(self, x, edge_index): h = F.dropout(x, p=0.5, training=self.training) h = self.gcn1(h, edge_index) h = torch.relu(h) h = F.dropout(h, p=0.5, training=self.training) h = self.gcn2(h, edge_index) return h, F.log_softmax(h, dim=1) class GAT(torch.nn.Module): &quot;&quot;&quot;Graph Attention Network&quot;&quot;&quot; def __init__(self, dim_in, dim_h, dim_out, heads=8): super().__init__() self.gat1 = GATv2Conv(dim_in, dim_h, heads=heads) self.gat2 = GATv2Conv(dim_h*heads, dim_out, heads=1) self.optimizer = torch.optim.Adam(self.parameters(), lr=0.005, weight_decay=5e-4) def forward(self, x, edge_index): h = F.dropout(x, p=0.6, training=self.training) h = self.gat1(x, edge_index) h = F.elu(h) h = F.dropout(h, p=0.6, training=self.training) h = self.gat2(h, edge_index) return h, F.log_softmax(h, dim=1) def accuracy(pred_y, y): &quot;&quot;&quot;Calculate accuracy.&quot;&quot;&quot; return ((pred_y == y).sum() / len(y)).item() def train(model, data): &quot;&quot;&quot;Train a GNN model and return the trained model.&quot;&quot;&quot; criterion = torch.nn.CrossEntropyLoss() optimizer = model.optimizer epochs = 200 model.train() for epoch in range(epochs+1): # Training optimizer.zero_grad() _, out = model(data.x, data.edge_index) loss = criterion(out[data.train_mask], data.y[data.train_mask]) acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask]) loss.backward() optimizer.step() # Validation val_loss = criterion(out[data.val_mask], data.y[data.val_mask]) val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask]) # Print metrics every 10 epochs if(epoch % 10 == 0): print(f&#39;Epoch {epoch:&gt;3} | Train Loss: {loss:.3f} | Train Acc: &#39; f&#39;{acc*100:&gt;6.2f}% | Val Loss: {val_loss:.2f} | &#39; f&#39;Val Acc: {val_acc*100:.2f}%&#39;) return model def test(model, data): &quot;&quot;&quot;Evaluate the model on test set and print the accuracy score.&quot;&quot;&quot; model.eval() _, out = model(data.x, data.edge_index) acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask]) return acc . %%time # Initialize, train and test GCN model gcn = GCN(dataset.num_features, 16, dataset.num_classes) print(gcn) train(gcn, data) acc = test(gcn, data) print(f&#39; nGCN test accuracy: {acc*100:.2f}% n&#39;) . GCN( (gcn1): GCNConv(3703, 16) (gcn2): GCNConv(16, 6) ) Epoch 0 | Train Loss: 1.782 | Train Acc: 20.83% | Val Loss: 1.79 | Val Acc: 17.40% Epoch 10 | Train Loss: 0.580 | Train Acc: 89.17% | Val Loss: 1.31 | Val Acc: 55.40% Epoch 20 | Train Loss: 0.165 | Train Acc: 95.00% | Val Loss: 1.30 | Val Acc: 56.20% Epoch 30 | Train Loss: 0.113 | Train Acc: 97.50% | Val Loss: 1.49 | Val Acc: 54.40% Epoch 40 | Train Loss: 0.069 | Train Acc: 99.17% | Val Loss: 1.66 | Val Acc: 54.60% Epoch 50 | Train Loss: 0.037 | Train Acc: 100.00% | Val Loss: 1.65 | Val Acc: 55.60% Epoch 60 | Train Loss: 0.053 | Train Acc: 99.17% | Val Loss: 1.50 | Val Acc: 56.60% Epoch 70 | Train Loss: 0.084 | Train Acc: 97.50% | Val Loss: 1.50 | Val Acc: 58.00% Epoch 80 | Train Loss: 0.054 | Train Acc: 100.00% | Val Loss: 1.67 | Val Acc: 54.40% Epoch 90 | Train Loss: 0.048 | Train Acc: 98.33% | Val Loss: 1.54 | Val Acc: 57.80% Epoch 100 | Train Loss: 0.062 | Train Acc: 99.17% | Val Loss: 1.62 | Val Acc: 56.20% Epoch 110 | Train Loss: 0.082 | Train Acc: 96.67% | Val Loss: 1.52 | Val Acc: 56.60% Epoch 120 | Train Loss: 0.043 | Train Acc: 100.00% | Val Loss: 1.66 | Val Acc: 55.00% Epoch 130 | Train Loss: 0.058 | Train Acc: 98.33% | Val Loss: 1.55 | Val Acc: 59.80% Epoch 140 | Train Loss: 0.058 | Train Acc: 98.33% | Val Loss: 1.68 | Val Acc: 58.40% Epoch 150 | Train Loss: 0.031 | Train Acc: 100.00% | Val Loss: 1.65 | Val Acc: 58.40% Epoch 160 | Train Loss: 0.037 | Train Acc: 100.00% | Val Loss: 1.44 | Val Acc: 64.20% Epoch 170 | Train Loss: 0.025 | Train Acc: 100.00% | Val Loss: 1.58 | Val Acc: 58.40% Epoch 180 | Train Loss: 0.036 | Train Acc: 99.17% | Val Loss: 1.65 | Val Acc: 58.00% Epoch 190 | Train Loss: 0.041 | Train Acc: 97.50% | Val Loss: 1.69 | Val Acc: 57.60% Epoch 200 | Train Loss: 0.093 | Train Acc: 95.83% | Val Loss: 1.73 | Val Acc: 56.80% GCN test accuracy: 67.70% CPU times: user 25.1 s, sys: 847 ms, total: 25.9 s Wall time: 32.4 s . %%time # Initialize, train and test GAT model gat = GAT(dataset.num_features, 8, dataset.num_classes) print(gat) train(gat, data) acc = test(gat, data) print(f&#39; nGAT test accuracy: {acc*100:.2f}% n&#39;) . GAT( (gat1): GATv2Conv(3703, 8, heads=8) (gat2): GATv2Conv(64, 6, heads=1) ) Epoch 0 | Train Loss: 1.790 | Train Acc: 17.50% | Val Loss: 1.81 | Val Acc: 12.80% Epoch 10 | Train Loss: 0.114 | Train Acc: 96.67% | Val Loss: 1.05 | Val Acc: 67.20% Epoch 20 | Train Loss: 0.040 | Train Acc: 98.33% | Val Loss: 1.21 | Val Acc: 64.80% Epoch 30 | Train Loss: 0.021 | Train Acc: 99.17% | Val Loss: 1.30 | Val Acc: 65.80% Epoch 40 | Train Loss: 0.027 | Train Acc: 99.17% | Val Loss: 1.20 | Val Acc: 67.20% Epoch 50 | Train Loss: 0.012 | Train Acc: 99.17% | Val Loss: 1.18 | Val Acc: 67.20% Epoch 60 | Train Loss: 0.009 | Train Acc: 100.00% | Val Loss: 1.11 | Val Acc: 67.00% Epoch 70 | Train Loss: 0.007 | Train Acc: 100.00% | Val Loss: 1.19 | Val Acc: 64.80% Epoch 80 | Train Loss: 0.013 | Train Acc: 100.00% | Val Loss: 1.16 | Val Acc: 66.80% Epoch 90 | Train Loss: 0.009 | Train Acc: 100.00% | Val Loss: 1.10 | Val Acc: 66.60% Epoch 100 | Train Loss: 0.013 | Train Acc: 100.00% | Val Loss: 1.07 | Val Acc: 67.20% Epoch 110 | Train Loss: 0.012 | Train Acc: 100.00% | Val Loss: 1.14 | Val Acc: 67.20% Epoch 120 | Train Loss: 0.014 | Train Acc: 100.00% | Val Loss: 1.12 | Val Acc: 66.40% Epoch 130 | Train Loss: 0.009 | Train Acc: 100.00% | Val Loss: 1.12 | Val Acc: 68.20% Epoch 140 | Train Loss: 0.007 | Train Acc: 100.00% | Val Loss: 1.19 | Val Acc: 65.40% Epoch 150 | Train Loss: 0.008 | Train Acc: 100.00% | Val Loss: 1.14 | Val Acc: 66.80% Epoch 160 | Train Loss: 0.007 | Train Acc: 100.00% | Val Loss: 1.16 | Val Acc: 68.40% Epoch 170 | Train Loss: 0.008 | Train Acc: 100.00% | Val Loss: 1.11 | Val Acc: 68.20% Epoch 180 | Train Loss: 0.006 | Train Acc: 100.00% | Val Loss: 1.13 | Val Acc: 68.60% Epoch 190 | Train Loss: 0.005 | Train Acc: 100.00% | Val Loss: 1.14 | Val Acc: 68.60% Epoch 200 | Train Loss: 0.007 | Train Acc: 100.00% | Val Loss: 1.13 | Val Acc: 68.40% GAT test accuracy: 70.00% CPU times: user 53.4 s, sys: 2.68 s, total: 56.1 s Wall time: 55.9 s . This experiment is not super rigorous: we&#39;d need to repeat it 100 times and take the average accuracy as the final result. But we can see in this example that the GAT outperforms the GCN. The authors obtained 72.5% for the GAT and 70.3% for the GCN, which is clearly better than we did. The difference can be explained by some tweaks in the models and a different training setting, where they used a patience of 100 instead of a fixed number of epochs for instance. However, the GAT takes longer to train, which can cause scalability issues very large graphs. . Let&#39;s visualize what the GAT learned. This time, we&#39;re gonna use t-SNE, a powerful method to plot high-dimensional data (3703 features in our case) in 2D or 3D. First, let&#39;s see what the embeddings looked like before any training: it should be absolutely random since they&#39;re produced by randomly initialized weight matrices. . untrained_gat = GAT(dataset.num_features, 8, dataset.num_classes) # Get embeddings h, _ = untrained_gat(data.x, data.edge_index) # Train TSNE tsne = TSNE(n_components=2, learning_rate=&#39;auto&#39;, init=&#39;pca&#39;).fit_transform(h.detach()) # Plot TSNE plt.figure(figsize=(10, 10)) plt.axis(&#39;off&#39;) plt.scatter(tsne[:, 0], tsne[:, 1], s=50, c=data.y) plt.show() . /usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence. FutureWarning, . Indeed, there&#39;s no apparent structure. But does our trained model perform better? Here&#39;s the t-SNE plot of our final embeddings: . h, _ = gat(data.x, data.edge_index) # Train TSNE tsne = TSNE(n_components=2, learning_rate=&#39;auto&#39;, init=&#39;pca&#39;).fit_transform(h.detach()) # Plot TSNE plt.figure(figsize=(10, 10)) plt.axis(&#39;off&#39;) plt.scatter(tsne[:, 0], tsne[:, 1], s=50, c=data.y) plt.show() . /usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:986: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence. FutureWarning, . The difference is noticeable: nodes belonging to the same classes cluster together. There are outliers, but this was to be expected: our accuracy score is far from being perfect. . I forgot something: during the data exploration stage, I speculated that isolated communities might be the source of the poor performance of CiteSeer compared to the other two datasets. Demonstrating this rigorously would require comparative studies, something we do not have time to do in this article. Even successfully identifying nodes belonging to isolated communities is time-consuming and difficult. I propose to do something else: calculate the model&#39;s accuracy for each degree. In other words, nodes with one neighbor ($deg(i) = 1$), two neighbors ($deg(i) = 2$), etc. . It&#39;s not the same thing, but isolated nodes and communities have few or no connections to neighbors. Intuitively, they should be harder to classify. For this purpose, it is possible to calculate the degree of each node in CiteSeer with PyG&#39;s utility function degree. We can then use np.where() to create a mask (like train_mask and test_mask) for this particular degree. Finally, we just have to compute the accuracy scores and plot them for a swell visualization. . from torch_geometric.utils import degree # Get model&#39;s classifications _, out = gat(data.x, data.edge_index) # Calculate the degree of each node degrees = degree(data.edge_index[0]).numpy() # Store accuracy scores and sample sizes accuracies = [] sizes = [] # Accuracy for degrees between 0 and 5 for i in range(0, 6): mask = np.where(degrees == i)[0] accuracies.append(accuracy(out.argmax(dim=1)[mask], data.y[mask])) sizes.append(len(mask)) # Accuracy for degrees &gt; 5 mask = np.where(degrees &gt; 5)[0] accuracies.append(accuracy(out.argmax(dim=1)[mask], data.y[mask])) sizes.append(len(mask)) # Bar plot fig, ax = plt.subplots(figsize=(18, 9)) ax.set_xlabel(&#39;Node degree&#39;) ax.set_ylabel(&#39;Accuracy score&#39;) ax.set_facecolor(&#39;#EFEEEA&#39;) plt.bar([&#39;0&#39;,&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;,&#39;&gt;5&#39;], accuracies, color=&#39;#0A047A&#39;) for i in range(0, 7): plt.text(i, accuracies[i], f&#39;{accuracies[i]*100:.2f}%&#39;, ha=&#39;center&#39;, color=&#39;#0A047A&#39;) for i in range(0, 7): plt.text(i, accuracies[i]//2, sizes[i], ha=&#39;center&#39;, color=&#39;white&#39;) . Nodes with few neighbors are indeed harder to classify. This is due to the nature of GNNs: the more relevant connections you have, the more information you can aggregate. Performance stop increasing after a certain theshold: in this example, having more than 4 or 5 neighbors doesn&#39;t help that much. . &#128282; IV. Conclusion . While they take longer to train, Graph Attention Networks are a substantial improvement over Graph Convolutional Nets in terms of accuracy. The self-attention mechanism automatically calculates weighting factors instead of static coefficients to produce better embeddings. In this article, . We explored a new graph dataset commonly used in research, and we analyzed more properties like node degrees; | We learned about the self-attention mechanism applied to GNNs with the concatenation of hidden features to produce an attention score that is normalized for each connection; | We reimplemented a graph attention layer in NumPy to get a better understanding of how it works; | We compared two architectures with PyTorch Geometric, a GCN and a GAT, to see the improvement brought by the attention mechanism; | We visualized what the GAT learns with the t-SNE plot of our trained and untrained embeddings; | Finally, we performed a quick error analysis based on node degrees. | . GATs are the de facto standard in most GNN applications. However, their slow training time can become a problem when applied to massive graph datasets. Scalability is an important factor in deep learning: most often, more data can lead to better performance than better models. In the next article, we&#39;ll see how to improve scalability with mini-batching and a new GNN architecture called GraphSAGE. . If you enjoyed this tutorial, please share it on social media and follow me on Twitter for more GNN content. Thank you and see you in the next article! üì£ .",
            "url": "https://mlabonne.github.io/blog/gat/",
            "relUrl": "/gat/",
            "date": " ‚Ä¢ Mar 9, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Introduction to Linear Programming in Python",
            "content": "In real warfare and strategy games, strategists are faced with a common problem: how to allocate their resources to be the most efficient possible? Let&#39;s take an example where we have three resources: üåæfood, ü™µwood, and ü™ôgold. We can produce three types of units: üó°Ô∏èswordsmen, üèπbowmen, and üêéhorsemen. üêéHorsemen are better than üèπbowmen, who are in turn better than üó°Ô∏èswordsmen. The following table provides the cost and power of each unit (they&#39;re freely inspired by a real strategy game, Age of Empires IV): . Unit üåæFood ü™µWood ü™ôGold üí™Power . üó°Ô∏èSwordsman | 60 | 20 | 0 | 70 | . üèπBowman | 80 | 10 | 40 | 95 | . üêéHorseman | 140 | 0 | 100 | 230 | . . Imagine that we have 1200 üåæfood, 800 ü™µwood, and 600 ü™ôgold. How to maximize the power of our army considering these resources? We could simply find the unit with the best power/cost ratio, take as many of them as possible, and repeat the process with the other two units. But this &quot;guess and check&quot; solution might not even be optimal. Now imagine we have millions of units and resources: this greedy strategy is very likely to completely miss the optimal solution. It is possible to use a machine learning (ML) algorithm (a metaheuristic like a genetic algorithm) to solve this problem, but we have no guarantee that the solution will be optimal either. . Fortunately for us, there is a method that can solve our problem in an optimal way: linear programming (or linear optimization), which is part of the field of operations research (OR). In this article, we&#39;re going to find the best solution possible to this problem. We&#39;ll use OR-Tools, a powerful Python library specialized in optimization. It will help us to model this problem in linear programming terms and output an optimal solution. Finally, we will write a model that can take on a bigger challenge and actually solve a whole class of optimization problems. . &#129351; I. Finding the optimal solution . Okay, let&#39;s play with linear programming in Python. There are different libraries such as the multi-purposed SciPy, the beginner-friendly PuLP, the exhaustive pyomo, and many others. Today, we are going to use Google OR-Tools, which is quite user-friendly, comes with several prepackaged solvers, and has by far the most stars on GitHub. If the installation doesn&#39;t work, please restart the kernel and try again: it can fail sometimes. ¬Ø _(„ÉÑ)_/¬Ø . !python -m pip install --upgrade --user -q ortools . All these libraries have a hidden benefit: they act as interfaces to use the same model with different solvers. Solvers like Gurobi, Cplex, or SCIP have their own APIs, but the models they create are tied to a specific solver. OR-Tools allows us to use an abstract and quite pythonic way of modeling our problems, and then choose one or several solvers to find an optimal solution. . &lt;/source&gt; OR-Tools comes with its own linear programming solver, called GLOP for Google Linear Optimization Package. It is an open-source project created by Google&#39;s Operations Research Team and written in C++. According to Google, it is used to stabilize YouTube videos and in various projects such as a cooperative multi-agent reinforcement learning architecture. Other solvers are available, such as SCIP, an excellent non-commercial solver created in 2005 and updated and maintained to this day. We could also use commercial options like Gurobi and Cplex but we would need to install them on top of OR-Tools and get the appropriate licenses (which can be quite costly). For now, let&#39;s try GLOP. . from ortools.linear_solver import pywraplp # Create a solver using the GLOP backend solver = pywraplp.Solver(&#39;Maximize army power&#39;, pywraplp.Solver.GLOP_LINEAR_PROGRAMMING) . We created an instance of the OR-Tools solver using GLOP. The first thing we want to define is the variables we want to optimize. In our example, we have three variables: the number of üó°Ô∏èswordsmen, üèπbowmen, and üêéhorsemen in the army. OR-Tools accepts three types of variables: . NumVar for continuous variables; | IntVar for integer variables; | BoolVar for boolean variables. | . We&#39;re looking for round numbers of units, so let&#39;s choose IntVar. We then need to specify lower and upper bounds for these variables. We want at least 0 unit, but we don&#39;t really have an upper bound. So we can say that our upper bound is infinity and use solver.infinity(). Actually $ infty$ might be a little too much considering the number of resources we have. We could also take $1000$ (for example) as an upper bound, since we can&#39;t go that high anyway. But let&#39;s keep $‚àû$ for now. . Let&#39;s call the number of üó°Ô∏èswordsmen $swordsmen$, the number of üèπbowmen $bowmen$, and the number of üêéhorsemen $horsemen$. We can formally write: . $$0 leq swordsmen &lt; infty 0 leq bowmen &lt; infty 0 leq horsemen &lt; infty$$Good, let&#39;s translate it into code. The syntax is quite straightforward with OR-Tools. . swordsmen = solver.IntVar(0, solver.infinity(), &#39;swordsmen&#39;) bowmen = solver.IntVar(0, solver.infinity(), &#39;bowmen&#39;) horsemen = solver.IntVar(0, solver.infinity(), &#39;horsemen&#39;) . We defined our variables, but another essential parameter is the constraints. In our case, we have a limited number of resources we can use to produce units. In other words, we can&#39;t spend more resources than we have: for instance, the üåæfood spent to recruit units cannot be higher than 1200. The same is true with ü™µwood (800) and ü™ôgold (600). . According to our table, units have the following costs: . 1 swordsman = üåæ60 + ü™µ20; | 1 bowman = üåæ80 + ü™µ10 + ü™ô40; | 1 horseman = üåæ140 + ü™ô100. | . We can write one constraint per resource as follows: . $$60 times swordsmen + 80 times bowmen + 140 times horsemen leq 1200 20 times swordsmen + 10 times bowmen leq 800 40 times bowmen + 100 times horsemen leq 600$$In OR-Tools, we simply add the constraints to our solver instance with solver.Add(). . solver.Add(swordsmen*60 + bowmen*80 + horsemen*140 &lt;= 1200) # Food solver.Add(swordsmen*20 + bowmen*10 &lt;= 800) # Wood solver.Add(bowmen*40 + horsemen*100 &lt;= 600) # Gold . &lt;ortools.linear_solver.pywraplp.Constraint; proxy of &lt;Swig Object of type &#39;operations_research::MPConstraint *&#39; at 0x7f68c2688c90&gt; &gt; . Now that we have our variables and constraints, we want to define our goal (or objective function). In linear programming, this function has to be linear, so of the form $f(x, y, z) = ax + by + cz + d$. In our example, the objective is quite clear: we want to recruit the army with the highest power. The table gives us the following power values: . 1 swordsman = üí™70; | 1 bowman = üí™95; | 1 horseman = üí™230. | . Maximizing the power of the army amounts to maximizing the sum of the power of each unit. Let&#39;s call $units$ the tuple $(swordsmen, bowmen, horsemen)$, where every element is the number of the corresponding unit. Our objective function can be written as: . $$max f(units) = 70 times swordsmen + 95 times bowmen + 230 times horsemen$$ . In general, there are two types of objective functions: minimizing and maximizing. In OR-Tools, we declare this goal with solver.Maximize() or solver.Minimize(). . solver.Maximize(swordsmen*70 + bowmen*95 + horsemen*230) . And we&#39;re done! There are three steps to model an optimization problem: . Declaring the variables to optimize with lower and upper bounds; | Adding constraints to these variables; | Defining the objective function to maximize or to minimize. | Now that is clear, we can ask the solver to find an optimal solution for us. This is done with solver.Solve() and returns a status. This status can then be used to check that the solution is indeed optimal. Let&#39;s print the highest total power we can get with the best army configuration. . status = solver.Solve() # If an optimal solution has been found, print results if status == pywraplp.Solver.OPTIMAL: print(&#39;================= Solution =================&#39;) print(f&#39;Solved in {solver.wall_time():.2f} milliseconds in {solver.iterations()} iterations&#39;) print() print(f&#39;Optimal power = {solver.Objective().Value()} üí™power&#39;) print(&#39;Army:&#39;) print(f&#39; - üó°Ô∏èSwordsmen = {swordsmen.solution_value()}&#39;) print(f&#39; - üèπBowmen = {bowmen.solution_value()}&#39;) print(f&#39; - üêéHorsemen = {horsemen.solution_value()}&#39;) else: print(&#39;The solver could not find an optimal solution.&#39;) . ================= Solution ================= Solved in 87.00 milliseconds in 2 iterations Optimal power = 1800.0 üí™power Army: - üó°Ô∏èSwordsmen = 6.0000000000000036 - üèπBowmen = 0.0 - üêéHorsemen = 5.999999999999999 . Great! The solver found an optimal solution: our army has a total power of üí™1800 with 6 üó°Ô∏èswordsmen and 6 üêéhorsemen (sorry bowmen!). . Let&#39;s unpack this result: the solver decided to take the maximum number of üêéhorsemen (6, since we only have ü™ô600 and they each cost ü™ô100). The remaining resources are spent in üó°Ô∏èswordsmen: we have $1200 - 6 times 140 = 360$ üåæfood left, which is why the solver chose 6 üó°Ô∏èswordsmen. We can deduce that the horsemen are the best unit and the bowmen are the worst one because they haven&#39;t been chosen at all. . Okay, but there&#39;s something quite weird: these numbers are not round, even though we specified that we wanted integers. So what happened? This strange behavior is due to the GLOP solver: it is a linear programming (LP) solver, and not an integer linear programming (ILP) one. In summary, it can only use real numbers and not integers as variables. So why did we declare our variables as integers if it doesn&#39;t take it into account? . GLOP cannot solve ILP problems, but other solvers can. Actually, a lot of them are mixed integer linear programming (MILP, commonly called MIP) solvers. This means that they can consider both continuous (real numbers) and discrete (integers) variables. A particular case of discrete values is boolean variables to represent decisions with 0-1 values. . We talked about SCIP earlier, which is a good example since it can solve both MILP and MINLP (mixed integer nonlinear programming) problems. Another potential candidate is CBC, an open-source MIP solver directly available through OR-Tools. Thanks to this library, we can use the same model and just change the solver to SCIP or CBC. . solver = pywraplp.Solver(&#39;Maximize army power&#39;, pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING) # 1. Create the variables we want to optimize swordsmen = solver.IntVar(0, solver.infinity(), &#39;swordsmen&#39;) bowmen = solver.IntVar(0, solver.infinity(), &#39;bowmen&#39;) horsemen = solver.IntVar(0, solver.infinity(), &#39;horsemen&#39;) # 2. Add constraints for each resource solver.Add(swordsmen*60 + bowmen*80 + horsemen*140 &lt;= 1200) solver.Add(swordsmen*20 + bowmen*10 &lt;= 800) solver.Add(bowmen*40 + horsemen*100 &lt;= 600) # 3. Maximize the objective function solver.Maximize(swordsmen*70 + bowmen*95 + horsemen*230) # Solve problem status = solver.Solve() # If an optimal solution has been found, print results if status == pywraplp.Solver.OPTIMAL: print(&#39;================= Solution =================&#39;) print(f&#39;Solved in {solver.wall_time():.2f} milliseconds in {solver.iterations()} iterations&#39;) print() print(f&#39;Optimal value = {solver.Objective().Value()} üí™power&#39;) print(&#39;Army:&#39;) print(f&#39; - üó°Ô∏èSwordsmen = {swordsmen.solution_value()}&#39;) print(f&#39; - üèπBowmen = {bowmen.solution_value()}&#39;) print(f&#39; - üêéHorsemen = {horsemen.solution_value()}&#39;) else: print(&#39;The solver could not find an optimal solution.&#39;) . ================= Solution ================= Solved in 3.00 milliseconds in 0 iterations Optimal value = 1800.0 üí™power Army: - üó°Ô∏èSwordsmen = 6.0 - üèπBowmen = 0.0 - üêéHorsemen = 6.0 . Strictly speaking, our variables are still floats (type(swordsmen.solution_value()) = float) but we can see that they don&#39;t have weird decimals anymore: the CBC solver really considered them as integers. In general, we can just round up these values since the error is insignificant, but it is important to remember to choose the appropriate solver according to the studied problem: LP (continuous variables) or MIP (combination of continuous and discrete variables). There are other types such as quadratic (QP) or nonlinear (NLP or MINLP, with an exponential objective function or constraints for instance) problems. . &#129521; III. Building a general model . But what if our resources change? Or if the cost of a unit evolved? What if we upgraded horsemen and their power increased? One of the best perks of OR-Tools is that it uses a general-purpose programming language like Python. Instead of static numbers, we can store our parameters in objects like dictionaries or lists. The code won&#39;t be as readable, but it becomes much more flexible: actually, it can be so flexible that we can solve an entire class of optimization problems without changing the model (just the parameters). . Let&#39;s transform our input parameters into Python lists and feed them to the solver through a function. . UNITS = [&#39;üó°Ô∏èSwordsmen&#39;, &#39;üèπBowmen&#39;, &#39;üêéHorsemen&#39;] DATA = [[60, 20, 0, 70], [80, 10, 40, 95], [140, 0, 100, 230]] RESOURCES = [1200, 800, 600] def solve_army(UNITS, DATA, RESOURCES): # Create the linear solver using the CBC backend solver = pywraplp.Solver(&#39;Maximize army power&#39;, pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING) # 1. Create the variables we want to optimize units = [solver.IntVar(0, solver.infinity(), unit) for unit in UNITS] # 2. Add constraints for each resource for r, _ in enumerate(RESOURCES): solver.Add(sum(DATA[u][r] * units[u] for u, _ in enumerate(units)) &lt;= RESOURCES[r]) # 3. Maximize the objective function solver.Maximize(sum(DATA[u][-1] * units[u] for u, _ in enumerate(units))) # Solve problem status = solver.Solve() # If an optimal solution has been found, print results if status == pywraplp.Solver.OPTIMAL: print(&#39;================= Solution =================&#39;) print(f&#39;Solved in {solver.wall_time():.2f} milliseconds in {solver.iterations()} iterations&#39;) print() print(f&#39;Optimal value = {solver.Objective().Value()} üí™power&#39;) print(&#39;Army:&#39;) for u, _ in enumerate(units): print(f&#39; - {units[u].name()} = {units[u].solution_value()}&#39;) else: print(&#39;The solver could not find an optimal solution.&#39;) solve_army(UNITS, DATA, RESOURCES) . ================= Solution ================= Solved in 2.00 milliseconds in 0 iterations Optimal value = 1800.0 üí™power Army: - üó°Ô∏èSwordsmen = 6.0 - üèπBowmen = 0.0 - üêéHorsemen = 6.0 . Good, the results are the same: our code seems to work. Now let&#39;s change the parameters to tackle a slightly more complex problem. . Imagine we have a lot more resources: üåæ183000, ü™µ90512, and ü™ô80150, so we can also produce a lot more units! This is the new table: . Unit üåæFood ü™µWood ü™ôGold üí™Attack ‚ù§Ô∏èHealth . üó°Ô∏èSwordsman | 60 | 20 | 0 | 6 | 70 | . üõ°Ô∏èMan-at-arms | 100 | 0 | 20 | 12 | 155 | . üèπBowman | 30 | 50 | 0 | 5 | 70 | . ‚ùåCrossbowman | 80 | 0 | 40 | 12 | 80 | . üî´Handcannoneer | 120 | 0 | 120 | 35 | 150 | . üêéHorseman | 100 | 20 | 0 | 9 | 125 | . ‚ôûKnight | 140 | 0 | 100 | 24 | 230 | . üêèBattering ram | 0 | 300 | 0 | 200 | 700 | . üéØSpringald | 0 | 250 | 250 | 30 | 200 | . . Notice that we transformed the üí™power into two values: üí™attack and ‚ù§Ô∏è health, which is a little more detailed. Health values are higher than attack values, which is why we want to add a weighting factor to make them more comparable. Let&#39;s take 10 as an example, so $power = 10 times attack + health$. We call the tuple of variables to optimize $units = (swordsmen, dots, springalds)$, where every element is the number of the corresponding unit. Our objective function becomes: . $$max f(units) = sum_{u in units} (10 times power + health) cdot u$$ . Adapting our code to this new problem is actually quite simple: we just have to change the input parameters and update the objective function. . UNITS = [ &#39;üó°Ô∏èSwordsmen&#39;, &#39;üõ°Ô∏èMen-at-arms&#39;, &#39;üèπBowmen&#39;, &#39;‚ùåCrossbowmen&#39;, &#39;üî´Handcannoneers&#39;, &#39;üêéHorsemen&#39;, &#39;‚ôûKnights&#39;, &#39;üêèBattering rams&#39;, &#39;üéØSpringalds&#39;, &#39;ü™®Mangonels&#39;, ] DATA = [ [60, 20, 0, 6, 70], [100, 0, 20, 12, 155], [30, 50, 0, 5, 70], [80, 0, 40, 12, 80], [120, 0, 120, 35, 150], [100, 20, 0, 9, 125], [140, 0, 100, 24, 230], [0, 300, 0, 200, 700], [0, 250, 250, 30, 200], [0, 400, 200, 12*3, 240] ] RESOURCES = [183000, 90512, 80150] def solve_army(UNITS, DATA, RESOURCES): # Create the linear solver using the CBC backend solver = pywraplp.Solver(&#39;Maximize army power&#39;, pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING) # 1. Create the variables we want to optimize units = [solver.IntVar(0, solver.infinity(), unit) for unit in UNITS] # 2. Add constraints for each resource for r, _ in enumerate(RESOURCES): solver.Add(sum(DATA[u][r] * units[u] for u, _ in enumerate(units)) &lt;= RESOURCES[r]) # 3. Maximize the new objective function solver.Maximize(sum((10*DATA[u][-2] + DATA[u][-1]) * units[u] for u, _ in enumerate(units))) # Solve problem status = solver.Solve() # If an optimal solution has been found, print results if status == pywraplp.Solver.OPTIMAL: print(&#39;================= Solution =================&#39;) print(f&#39;Solved in {solver.wall_time():.2f} milliseconds in {solver.iterations()} iterations&#39;) print() print(f&#39;Optimal value = {solver.Objective().Value()} üí™power&#39;) print(&#39;Army:&#39;) for u, _ in enumerate(units): print(f&#39; - {units[u].name()} = {units[u].solution_value()}&#39;) else: print(&#39;The solver could not find an optimal solution.&#39;) solve_army(UNITS, DATA, RESOURCES) . ================= Solution ================= Solved in 74.00 milliseconds in 412 iterations Optimal value = 1393145.0 üí™power Army: - üó°Ô∏èSwordsmen = 2.0 - üõ°Ô∏èMen-at-arms = 1283.0 - üèπBowmen = 3.0 - ‚ùåCrossbowmen = 0.0 - üî´Handcannoneers = 454.0 - üêéHorsemen = 0.0 - ‚ôûKnights = 0.0 - üêèBattering rams = 301.0 - üéØSpringalds = 0.0 - ü™®Mangonels = 0.0 . This problem would take a long time for humans to address, but the ILP solver did it in the blink of an eye. Better than that: it also gives us the guarantee that our solution is optimal, which means that our enemy cannot find a better army compositon for the same cost! We could increase the number of units, give billions of resources but you get the picture: it would just take longer to obtain a solution, but it wouldn&#39;t change the problem. . &#9876;&#65039; IV. Time for war . Now, let&#39;s say we scouted our enemy and know that their army has a üí™power of 1,000,000. We could build a much better army, but our resources are precious and it wouldn&#39;t be very efficient: all we have to do is to build an army with a üí™power higher than 1,000,000 (even 1,000,001 would be enough). In other words, it means we&#39;re looking for the most cost-efficient unit and we want the minimum number of them to defeat the enemy army. Knowing the number of units to produce will also tell us how many resources we need. . We can reuse our input parameters since they didn&#39;t change. But the constraint is different this time: we want a üí™power higher than 1,000,000. It means that the sum of the power of the selected units must be higher than this number. . $$ sum_{u in units} (10 times attack + health) cdot u &gt; 1 ,000 ,000$$ . In code, we can loop through our units and resources to design this constraint. . The objective function also has to change. Our goal is to minimize the sum of resources spent to build the army. . $$min f(units) = sum_{u in units} (food + wood + gold) cdot u$$ . Once again, we can loop through our resources to implement it in OR-Tools. . def solve_army(UNITS, DATA, RESOURCES): # Create the linear solver using the CBC backend solver = pywraplp.Solver(&#39;Minimize resource consumption&#39;, pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING) # 1. Create the variables we want to optimize units = [solver.IntVar(0, solver.infinity(), unit) for unit in UNITS] # 2. Add constraints for each resource for r, _ in enumerate(RESOURCES): solver.Add(sum((10 * DATA[u][-2] + DATA[u][-1]) * units[u] for u, _ in enumerate(units)) &gt;= 1000001) # 3. Minimize the objective function solver.Minimize(sum((DATA[u][0] + DATA[u][1] + DATA[u][2]) * units[u] for u, _ in enumerate(units))) # Solve problem status = solver.Solve() # If an optimal solution has been found, print results if status == pywraplp.Solver.OPTIMAL: print(&#39;================= Solution =================&#39;) print(f&#39;Solved in {solver.wall_time():.2f} milliseconds in {solver.iterations()} iterations&#39;) print() power = sum((10 * DATA[u][-2] + DATA[u][-1]) * units[u].solution_value() for u, _ in enumerate(units)) print(f&#39;Optimal value = {solver.Objective().Value()} üåæü™µü™ôresources&#39;) print(f&#39;Power = üí™{power}&#39;) print(&#39;Army:&#39;) for u, _ in enumerate(units): print(f&#39; - {units[u].name()} = {units[u].solution_value()}&#39;) print() food = sum((DATA[u][0]) * units[u].solution_value() for u, _ in enumerate(units)) wood = sum((DATA[u][1]) * units[u].solution_value() for u, _ in enumerate(units)) gold = sum((DATA[u][2]) * units[u].solution_value() for u, _ in enumerate(units)) print(&#39;Resources:&#39;) print(f&#39; - üåæFood = {food}&#39;) print(f&#39; - ü™µWood = {wood}&#39;) print(f&#39; - ü™ôGold = {gold}&#39;) else: print(&#39;The solver could not find an optimal solution.&#39;) solve_army(UNITS, DATA, RESOURCES) . ================= Solution ================= Solved in 4.00 milliseconds in 0 iterations Optimal value = 111300.0 üåæü™µü™ôresources Power = üí™1001700.0 Army: - üó°Ô∏èSwordsmen = 0.0 - üõ°Ô∏èMen-at-arms = 0.0 - üèπBowmen = 0.0 - ‚ùåCrossbowmen = 0.0 - üî´Handcannoneers = 0.0 - üêéHorsemen = 0.0 - ‚ôûKnights = 0.0 - üêèBattering rams = 371.0 - üéØSpringalds = 0.0 - ü™®Mangonels = 0.0 Resources: - üåæFood = 0.0 - ü™µWood = 111300.0 - ü™ôGold = 0.0 . The solver found an optimal solution: we need to build 371 üêèbattering rams for a total cost of 111,300 ü™µwood. Wait, what if we don&#39;t have that much wood? In the previous section, we only had ü™µ90512: we cannot produce 371 üêèbattering rams. üò± . So is it possible to take these limited resources into account and still try to build the best army? Actually, it&#39;s super easy: we just have to copy/paste the constraints from the previous section. . def solve_army(UNITS, DATA, RESOURCES): # Create the linear solver using the CBC backend solver = pywraplp.Solver(&#39;Minimize resource consumption&#39;, pywraplp.Solver.CBC_MIXED_INTEGER_PROGRAMMING) # 1. Create the variables we want to optimize units = [solver.IntVar(0, solver.infinity(), unit) for unit in UNITS] # 2. Add constraints for each resource for r, _ in enumerate(RESOURCES): solver.Add(sum((10 * DATA[u][-2] + DATA[u][-1]) * units[u] for u, _ in enumerate(units)) &gt;= 1000001) # Old constraints for limited resources for r, _ in enumerate(RESOURCES): solver.Add(sum(DATA[u][r] * units[u] for u, _ in enumerate(units)) &lt;= RESOURCES[r]) # 3. Minimize the objective function solver.Minimize(sum((DATA[u][0] + DATA[u][1] + DATA[u][2]) * units[u] for u, _ in enumerate(units))) # Solve problem status = solver.Solve() # If an optimal solution has been found, print results if status == pywraplp.Solver.OPTIMAL: print(&#39;================= Solution =================&#39;) print(f&#39;Solved in {solver.wall_time():.2f} milliseconds in {solver.iterations()} iterations&#39;) print() power = sum((10 * DATA[u][-2] + DATA[u][-1]) * units[u].solution_value() for u, _ in enumerate(units)) print(f&#39;Optimal value = {solver.Objective().Value()} üåæü™µü™ôresources&#39;) print(f&#39;Power = üí™{power}&#39;) print(&#39;Army:&#39;) for u, _ in enumerate(units): print(f&#39; - {units[u].name()} = {units[u].solution_value()}&#39;) print() food = sum((DATA[u][0]) * units[u].solution_value() for u, _ in enumerate(units)) wood = sum((DATA[u][1]) * units[u].solution_value() for u, _ in enumerate(units)) gold = sum((DATA[u][2]) * units[u].solution_value() for u, _ in enumerate(units)) print(&#39;Resources:&#39;) print(f&#39; - üåæFood = {food}&#39;) print(f&#39; - ü™µWood = {wood}&#39;) print(f&#39; - ü™ôGold = {gold}&#39;) else: print(&#39;The solver could not find an optimal solution.&#39;) solve_army(UNITS, DATA, RESOURCES) . ================= Solution ================= Solved in 28.00 milliseconds in 1 iterations Optimal value = 172100.0 üåæü™µü™ôresources Power = üí™1000105.0 Army: - üó°Ô∏èSwordsmen = 1.0 - üõ°Ô∏èMen-at-arms = 681.0 - üèπBowmen = 0.0 - ‚ùåCrossbowmen = 0.0 - üî´Handcannoneers = 0.0 - üêéHorsemen = 0.0 - ‚ôûKnights = 0.0 - üêèBattering rams = 301.0 - üéØSpringalds = 0.0 - ü™®Mangonels = 0.0 Resources: - üåæFood = 68160.0 - ü™µWood = 90320.0 - ü™ôGold = 13620.0 . Since we now have a limited resource of ü™µwood, the number of üêèbattering rams sadly dropped from 371 to 301. In exchange, we got 681 üõ°Ô∏èmen-at-arms and 1 lost üó°Ô∏èswordsman (welcome to them). The total cost of the army is 172,100, which is much higher than the 111,300 we previously found (+65% increase) but it truly is the optimal solution under these constraints. It shows that we should produce more wood because these üêèbattering rams are extremely cost-efficient! . &#128282; V. Conclusion . Optimization is often neglected in favor of machine learning techniques, but both have their merits: linear programming can produce an optimal solution in an undetermined amount of time, while machine learning can approximate complex functions in no time. There is no training in LP, but an expert is required to build a mathematical model. Machine learning needs data, but the models can be used as black boxes to solve a problem. As a rule of thumb, problems that do not have a particular time constraint and/or are not extremely complex can be advantageously solved with linear programming. . In this article, . We learned about interfaces for optimization, and especially about Google OR-Tools; | We talked about solvers and types of optimization problems: LP, MIP, NLP; | We modeled and solved an extremely common optimization problem in an optimal way and generalized our model through a function; | We reframed this problem and merged two sets of constraints to obtain the best army composition for the lowest price. | . There are a lot more problems where optimization can be applied: for instance, how to create school timetables that satisfy everybody&#39;s requirements? How to deliver 1,000 different orders in a minimum amount of time? Where to create a new metro line to maximize its usefulness? In future articles, we&#39;ll talk about new types of applications for these techniques, including satisfiability and nonlinear problems. . I hope you enjoyed this article! Feel free to share it and spread the knowledge about linear optimization. Don&#39;t forget to follow me on Twitter where I post summaries of these articles. Cheers! .",
            "url": "https://mlabonne.github.io/blog/linearoptimization/",
            "relUrl": "/linearoptimization/",
            "date": " ‚Ä¢ Mar 2, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Introduction to Graph Neural Networks",
            "content": "Graph Neural Networks (GNNs) are one of the most interesting architectures in deep learning. In just a few years, they managed to apply neural networks to a whole new set of problems: drug discovery where they found a new antibiotic (Halicin), in Google Maps where they outperform any other method to estimate the time of arrival, and they now even recommend every article, video, or connection you see on all big social media. GNNs are a technology with tremendous potential and countless applications still untapped and yet to be discovered. . Despite these achievements, GNNs are not as widely taught as other architectures like Convolutional Neural Networks (CNNs). Because of their young age, educational resources are scarcer and more research-oriented, which can discourage many practitioners. In this article, we&#39;re going to do node classification with graph data using PyTorch Geometric (or PyG). Once we understand how to interact with this specific type of data, we will try to understand the core of GNNs: the message passing layer. Finally, we&#39;ll implement a GNN and visualize how it learns and understands our graph data. . The first step is to install the libraries we&#39;ll need throughout this article. We assume that PyTorch is already installed, which is the case on Google Colab. If you don&#39;t have PyTorch in your environment, please follow the instructions given in the official documentation. Moreover, some extensions of PyTorch can be quite difficult to install because it depends on your version of PyTorch and CUDA. If the installation does not work for you, please check PyTorch Geometric&#39;s documentation. . import torch torchversion = torch.__version__ # Install PyTorch Scatter, PyTorch Sparse, and PyTorch Geometric !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torchversion}.html !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torchversion}.html !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git # Numpy for matrices import numpy as np # Visualization libraries import matplotlib.pyplot as plt import networkx as nx . |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.9 MB 7.9 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.5 MB 40.4 MB/s Building wheel for torch-geometric (setup.py) ... done . &#127760; I. Graph data . Graphs are a type of data you can find pretty much everywhere: social networks, computer networks, molecules, etc. Even text and images can be seen as graphs, as we&#39;re gonna see later. In this article, we&#39;re gonna study the infamous and much-used Zachary&#39;s karate club dataset. It has somewhat of a special aura in the community, and no course about graphs would be complete without it. . Zachary&#39;s karate club simply represents the relationships within a karate club studied by Wayne W. Zachary in the 1970s. It is a kind of social network, where every node is a member, and members who interacted outside the club are connected together. In this example, the club is divided into 4 groups: we would like to assign the right group to every member (node classification) just by looking at their connections. You can see it&#39;s quite similar to friend suggestions on social media, which is another task you can perform with GNNs: link prediction. . Let&#39;s import the dataset with PyG&#39;s built-in function and try to understand the Datasets object it uses. . from torch_geometric.datasets import KarateClub # Import dataset from PyTorch Geometric dataset = KarateClub() # Print information print(dataset) print(&#39;&#39;) print(f&#39;Number of graphs: {len(dataset)}&#39;) print(f&#39;Number of features: {dataset.num_features}&#39;) print(f&#39;Number of classes: {dataset.num_classes}&#39;) . KarateClub() Number of graphs: 1 Number of features: 34 Number of classes: 4 . This dataset only has 1 graph, where each node has a feature vector of 34 dimensions and is part of one out of 4 classes (our 4 groups). Actually, the Datasets object can be seen as a collection of Data (graph) objects. We can further inspect our unique graph to know more about it. . print(f&#39;Graph: {dataset[0]}&#39;) . Graph: Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34]) . The Data object is particularly interesting. Printing it offers a good summary of the graph we&#39;re studying: . x=[34, 34] is the node feature matrix with shape (number of nodes, number of features). In our case, it means that we have 34 nodes (our 34 members), each node being associated to a 34-dim feature vector. | edge_index=[2, 156] represents the graph connectivity (how the nodes are connected) with shape (2, number of directed edges). | y=[34] is the node ground-truth labels. In this problem, every node is assigned to one class (group), so we have one value for each node. | train_mask=[34] is an optional attribute that tells which nodes should be used for training with a list of True or False statements. | . Let&#39;s print each of these parameters to understand what they store. . data = dataset[0] # Print x print(f&#39;x = {data.x.shape}&#39;) print(data.x) . x = torch.Size([34, 34]) tensor([[1., 0., 0., ..., 0., 0., 0.], [0., 1., 0., ..., 0., 0., 0.], [0., 0., 1., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 1., 0., 0.], [0., 0., 0., ..., 0., 1., 0.], [0., 0., 0., ..., 0., 0., 1.]]) . Here, the node feature matrix x is actually an identity matrix: it doesn&#39;t contain any relevant information about the nodes. It could contain things like age, skill level, etc. but this is not the case in this dataset. We&#39;ll have to classify our nodes just by looking at their connections. . print(f&#39;edge_index = {data.edge_index.shape}&#39;) print(data.edge_index) . edge_index = torch.Size([2, 156]) tensor([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 8, 9, 9, 10, 10, 10, 11, 12, 12, 13, 13, 13, 13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 19, 20, 20, 21, 21, 22, 22, 23, 23, 23, 23, 23, 24, 24, 24, 25, 25, 25, 26, 26, 27, 27, 27, 27, 28, 28, 28, 29, 29, 29, 29, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33], [ 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 17, 19, 21, 31, 0, 2, 3, 7, 13, 17, 19, 21, 30, 0, 1, 3, 7, 8, 9, 13, 27, 28, 32, 0, 1, 2, 7, 12, 13, 0, 6, 10, 0, 6, 10, 16, 0, 4, 5, 16, 0, 1, 2, 3, 0, 2, 30, 32, 33, 2, 33, 0, 4, 5, 0, 0, 3, 0, 1, 2, 3, 33, 32, 33, 32, 33, 5, 6, 0, 1, 32, 33, 0, 1, 33, 32, 33, 0, 1, 32, 33, 25, 27, 29, 32, 33, 25, 27, 31, 23, 24, 31, 29, 33, 2, 23, 24, 33, 2, 31, 33, 23, 26, 32, 33, 1, 8, 32, 33, 0, 24, 25, 28, 32, 33, 2, 8, 14, 15, 18, 20, 22, 23, 29, 30, 31, 33, 8, 9, 13, 14, 15, 18, 19, 20, 22, 23, 26, 27, 28, 29, 30, 31, 32]]) . The edge_index has a quite counter-intuitive way of storing the graph connectivity. Here, we have 156 directed edges (78 bidirectional edges) twice because the first 156-dim array contains the sources and the second one the destinations. It is called a coordinate list (COO) and is just one way of efficiently storing a sparse matrix. A more intuitive way to represent the graph connectivity would be a simple adjacency matrix $A$, where nodes are rows and columns and 1s indicate a connection between two nodes. . The adjacency matrix can actually be calculated from the edge_index with a utility function. . from torch_geometric.utils import to_dense_adj A = to_dense_adj(data.edge_index)[0].numpy().astype(int) print(f&#39;A = {A.shape}&#39;) print(A) . A = (34, 34) [[0 1 1 ... 1 0 0] [1 0 1 ... 0 0 0] [1 1 0 ... 0 1 0] ... [1 0 0 ... 0 1 1] [0 0 1 ... 1 0 1] [0 0 0 ... 1 1 0]] . Unfortunately, it is not an optimal way of storing this information, since nodes are not highly interconnected. It means that our adjacency matrix is filled with zeros (sparse matrix) to indicate that there&#39;s no connection between nodes. Storing so many zeros is not efficient at all, which is why the COO format is adopted by PyG. . print(f&#39;y = {data.y.shape}&#39;) print(data.y) . y = torch.Size([34]) tensor([1, 1, 1, 1, 3, 3, 3, 1, 0, 1, 3, 1, 1, 1, 0, 0, 3, 1, 0, 1, 0, 1, 0, 0, 2, 2, 0, 0, 2, 0, 0, 2, 0, 0]) . On the contrary, node ground-truth labels y are quite straightforward: they simply encode the group number (0, 1, 2, 3) for each node. . print(f&#39;train_mask = {data.train_mask.shape}&#39;) print(data.train_mask) . train_mask = torch.Size([34]) tensor([ True, False, False, False, True, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False]) . Finally, the train_mask shows which nodes are supposed to be used for training with True statements. This is not super useful because we don&#39;t have test data anyway... üëÄ . But we&#39;re not done yet! The Data object has a lot more to offer: numerous properties about graphs can be checked using utility functions. For instance, is_directed() can tell you if the graph is directed, which means that all the edges are directed only in one direction. Another useful one is isolated_nodes() to check if some nodes are not connected to the rest of the graph. Finally, has_self_loops() can tell you if at least one node is connected to itself. This is not the same as loops: loops mean that you can take a path that starts and ends at the same node. . print(f&#39;Edges are directed: {data.is_directed()}&#39;) print(f&#39;Graph has isolated nodes: {data.has_isolated_nodes()}&#39;) print(f&#39;Graph has loops: {data.has_self_loops()}&#39;) . Edges are directed: False Graph has isolated nodes: False Graph has loops: False . One of the coolest utility functions available in PyG is to_networkx. It allows you to convert your Data instance into a networkx.Graph to easily visualize it. We can use matplotlib to plot the graph with colors corresponding to the label of every node. . from torch_geometric.utils import to_networkx G = to_networkx(data, to_undirected=True) plt.figure(figsize=(12,12)) plt.axis(&#39;off&#39;) nx.draw_networkx(G, pos=nx.spring_layout(G, seed=0), with_labels=True, node_size=800, node_color=data.y, cmap=&quot;hsv&quot;, vmin=-2, vmax=3, width=0.8, edge_color=&quot;grey&quot;, font_size=14 ) plt.show() . Okay, we plotted the Zachary&#39;s karate club dataset and can visualize our 34 nodes, 78 edges, and 4 labels with 4 different colours. I think our understanding of the problem is now strong enough to talk a bit more about GNNs and their core component: message passing. . &#9993;&#65039; II. Message Passing . 1. Redesigning message passing from convolution . In image processing, filters to blur, sharpen, or detect edges are all based on the same operation: a convolution between a matrix and an image. In the case of a $3 times 3$ matrix, this operation looks at the 8 pixels around each pixel and apply weights to their values. These values can be a single number, or a vector containing multiple numbers. In the previous figure, the same weight is applied to every neighboring pixel: each pixel becomes an average of its neighbors, which is why the result looks blurry. Different filters can be designed just by changing the value of these weights. Now, we could think of each pixel as a node and the entire image as a graph. The operation would be exactly the same. . Why is that relevant? In graphs, similar nodes are more likely to be connected to each other than dissimilar ones (it&#39;s called network homophily). Information about each node is stored in a feature vector, but we can get even more information about a node just by aggregating its features with those of its neighbors. This operation is called convolution for images, and message passing in GNNs. Message passing is a confusing name: the idea behind it is that nodes send &quot;messages&quot; (feature vectors) to each other, but it is just a general framework for neighborhood aggregation. . Let&#39;s take an example: we want to aggregate the feature vectors of the pixel $5$ with those of its 8 neighbors. Usually, pixels contain 3 values to encode the 3 primary colors red, green, and blue, so each feature vector can be noted $x_i = [r_i, g_i, b_i]$. As in a convolution operation, we multiply $x_5$ and the neighboring nodes&#39; feature vectors by a matrix $ mathbf{W}$. Note that in this case, $W_i$ are not single values, but vectors because they have to take into account three values for each $x_i$ ($r_i$, $g_i$, $b_i$). In other words, we have a weight matrix $ mathbf{W}$ containing 9 weight vectors $W_i$. We call the list of pixel $5$ and its neighbors $ mathcal{N}_5 = {1, 2, 3, 4, 5, 6, 7, 8, 9 }$. The result $h_5$ of this aggregation can be calculated as follows: . $$h_5 = W_1x_1 + dots + W_9x_9 = sum_{i in mathcal{N}_5} {W_ix_i}$$ . We could apply the same idea to graphs in general, but there&#39;s a problem: nodes do not have a consistent order like pixels do. If we change the order from $ {1, 2, 3, 4, 5, 6, 7, 8, 9 }$ to $ {9, 3, 1, 4, 7, 6, 2, 5, 8 }$, we would obtain a different result. It makes sense with images, but nodes can&#39;t be to the left or to the right of other nodes, they don&#39;t have positions! It means that we never know if the nodes are going to be in the correct order, so we can&#39;t assign $W_i$ to $x_i$. Okay then, a simple solution is to reuse the same weight vector $ mathbf{W}$ for every $x_i$, instead of having a unique weight vector $W_i$ for each node. That way, the same operation is performed whatever the node is. The formula now becomes: . $$h_5 = sum_{i in mathcal{N}_5} mathbf{W}x_i$$ . But unlike pixels, nodes do not have a fixed number of neighbors. What if one node only has 1 neighbor, and another one has 500 of them? We would add 500 values instead of just one, which means that the result $h$ would be much larger for the node with 500 neighbors. But it doesn&#39;t make sense: nodes should always be comparable so they need to have a similar range of values. To address this issue, we can normalize the result based on the number of neighbors. In graph theory, this number is called a degree. Let&#39;s divide the result of the previous formula by the number of neighbors of node $5$ (noted $deg(5)$): . $$h_5 = dfrac{1}{deg(5)} sum_{i in mathcal{N}_5} mathbf{W} x_i$$ . Excellent! We now have a message passing layer. There are other types of message passing, but this one is simple and works fairly well. We had to overcome two issues compared to a convolution operation: . Nodes don&#39;t have any idea of order, which forced us to use a common weight $ mathbf{W}$; | Nodes have a variable number of neighbors, which pushed us to normalize our result by dividing it by the number of neighbors $deg(5)$. | 2. (Optional) Message passing with matrices . Naturally, this way of computing message passing node by node is not very efficient. Since we&#39;re talking about neural networks, it means that calculations are performed through matrix multiplication. Let&#39;s go back to our dataset and translate the previous operations into matrices. First, we have data.x, which is the feature node matrix $X$. In this particular dataset, $X$ is the identity matrix $I$ but, in general, it could store any information about the nodes. . # a numpy array with integers instead of floats X = data.x.numpy().astype(int) print(f&#39;X = {X.shape}&#39;) print(X) . X = (34, 34) [[1 0 0 ... 0 0 0] [0 1 0 ... 0 0 0] [0 0 1 ... 0 0 0] ... [0 0 0 ... 1 0 0] [0 0 0 ... 0 1 0] [0 0 0 ... 0 0 1]] . Now we need $W$, the learnable weight matrix (like a filter) of our message passing layer. Our layer is not trained yet, so we can initialize $W$ with arbitrary values. To keep things simple, let&#39;s take the identity matrix again, so $W X = I cdot I = I$. . W = np.identity(X.shape[0], dtype=int) print(f&#39;W = {W.shape}&#39;) print(W) . W = (34, 34) [[1 0 0 ... 0 0 0] [0 1 0 ... 0 0 0] [0 0 1 ... 0 0 0] ... [0 0 0 ... 1 0 0] [0 0 0 ... 0 1 0] [0 0 0 ... 0 0 1]] . We can apply the weights in $W$ to every feature vector by calculating $WX$, but this is not what we want to do! Indeed, we only need to include the neighboring nodes and not every node in the graph. A nice way to find these neighbors is to look at the adjacency matrix $A$ we saw earlier. . print(f&#39;A = {A.shape}&#39;) print(A) . A = (34, 34) [[0 1 1 ... 1 0 0] [1 0 1 ... 0 0 0] [1 1 0 ... 0 1 0] ... [1 0 0 ... 0 1 1] [0 0 1 ... 1 0 1] [0 0 0 ... 1 1 0]] . In our case, $A$ is symmetric because edges in this graph are bidirectional. This wouldn&#39;t be the case if connections were unidirectional. This is why we use $A^T X$ and not $AX$ in general in order to obtain the feature vectors of the neighboring nodes. This operation does not only select the relevant vectors, it also sums them in the process. . &lt;/source&gt; We just miss the &quot;central&quot; node itself, which can be fixed by adding a connection to itself (self-loops) in the adjacency matrix: $ tilde{A} = A + I$. . A_tilde = A + np.identity(A.shape[0], dtype=int) print(f&#39; nA_tilde = {A_tilde.shape}&#39;) print(A_tilde) . A_tilde = (34, 34) [[1 1 1 ... 1 0 0] [1 1 1 ... 0 0 0] [1 1 1 ... 0 1 0] ... [1 0 0 ... 1 1 1] [0 0 1 ... 1 1 1] [0 0 0 ... 1 1 1]] . $ tilde{A}^TX$ selects the feature vectors of the neighboring nodes (and the node itself) and $WX$ applies the weights in $W$ to every feature vector in the graph. We can combine both expressions to apply these weights to the feature vectors of the neighboring nodes (and the node itself) with $ tilde{A}^T X W^T$. We can rewrite the previous operation as follows: . $$h_i = sum_{j in mathcal{N}_i} mathbf{W} x_j$$ . becomes $$H = tilde{A}^T X W^T$$ . H = A_tilde.T @ X @ W.T print(f&#39;H = A_tilde.T @ X @ W.T {H.shape}&#39;) print(H) . H = A_tilde.T @ X @ W.T (34, 34) [[1 1 1 ... 1 0 0] [1 1 1 ... 0 0 0] [1 1 1 ... 0 1 0] ... [1 0 0 ... 1 1 1] [0 0 1 ... 1 1 1] [0 0 0 ... 1 1 1]] . Now we would like to normalize $H$ by the number of neighbors as seen previously. We can use the degree matrix $D$ that counts the number of neighbors for each node. In our case, we want the matrix $ tilde{D}$, based on $ tilde{A}$ instead of $A$. . D = np.zeros(A.shape, dtype=int) np.fill_diagonal(D, A.sum(axis=0)) print(f&#39;D = {D.shape}&#39;) print(D) D_tilde = np.zeros(D.shape, dtype=int) np.fill_diagonal(D_tilde, A_tilde.sum(axis=0)) print(f&#39; nD_tilde = {D_tilde.shape}&#39;) print(D_tilde) . D = (34, 34) [[16 0 0 ... 0 0 0] [ 0 9 0 ... 0 0 0] [ 0 0 10 ... 0 0 0] ... [ 0 0 0 ... 6 0 0] [ 0 0 0 ... 0 12 0] [ 0 0 0 ... 0 0 17]] D_tilde = (34, 34) [[17 0 0 ... 0 0 0] [ 0 10 0 ... 0 0 0] [ 0 0 11 ... 0 0 0] ... [ 0 0 0 ... 7 0 0] [ 0 0 0 ... 0 13 0] [ 0 0 0 ... 0 0 18]] . We could use D to calculate either: . $ tilde{D}^{-1} tilde{A}$ to normalize every row in $ tilde{A}$; | $ tilde{A} tilde{D}^{-1}$ to normalize every column in $ tilde{A}$. | . In our case, $ tilde{A}$ is symmetric so the results would be the equivalent. We can thus translate the normalized operation as follows: . $$h_i = dfrac{1}{deg(i)} sum_{j in mathcal{N}_i} mathbf{W} x_j$$ . becomes $$H = tilde{D}^{-1} tilde{A}^T X W^T$$ . D_inv = np.linalg.inv(D_tilde) print(f&#39;D_inv = {D_inv.shape}&#39;) print(D_inv) H = D_inv @ A_tilde.T @ X @ W.T print(f&#39; nH = D_inv @ A.T @ X @ W.T {H.shape}&#39;) print(H) . D_inv = (34, 34) [[0.05882353 0. 0. ... 0. 0. 0. ] [0. 0.1 0. ... 0. 0. 0. ] [0. 0. 0.09090909 ... 0. 0. 0. ] ... [0. 0. 0. ... 0.14285714 0. 0. ] [0. 0. 0. ... 0. 0.07692308 0. ] [0. 0. 0. ... 0. 0. 0.05555556]] H = D_inv @ A.T @ X @ W.T (34, 34) [[0.05882353 0.05882353 0.05882353 ... 0.05882353 0. 0. ] [0.1 0.1 0.1 ... 0. 0. 0. ] [0.09090909 0.09090909 0.09090909 ... 0. 0.09090909 0. ] ... [0.14285714 0. 0. ... 0.14285714 0.14285714 0.14285714] [0. 0. 0.07692308 ... 0.07692308 0.07692308 0.07692308] [0. 0. 0. ... 0.05555556 0.05555556 0.05555556]] . And we&#39;re done! The previous operation is exactly what a Graph Neural Network could do. In reality, there are many ways to define a message passing layer. The good news is that it&#39;s handled by PyTorch Geometric, so we don&#39;t need to reimplement GNN layers with matrix multiplication from scratch. Now, let&#39;s see how it&#39;s done. . &#129504; III. Graph Neural Network . One of the simplest GNN layer is called GCNConv in PyG for Graph Convolutional Network, and was introduced in Semi-Supervised Classification with Graph Convolutional Networks. The main idea is that feature vectors from nodes with a lot of neighbors will spread very easily, unlike ones from more isolated nodes. The authors proposed to use a weighted average operation to counterbalance this effect, by giving bigger weights to feature vectors from nodes with few neighbors. This operation can be written as follows: . $$h_i = sum_{j in mathcal{N}_i} dfrac{1}{ sqrt{deg(i)} sqrt{deg(j)}} mathbf{W} x_j$$ . Or with matrices: . $$H = tilde{D}^{-1/2} tilde{A}^T tilde{D}^{-1/2} X W^T$$ . Notice that is $v$ and $i$ have the same number of neighbors, it is equivalent to our own message passing layer. . D_inv12 = np.linalg.inv(D_tilde) np.fill_diagonal(D_inv12, 1/ (D_tilde.diagonal()**0.5)) # New H H = D_inv12 @ A_tilde.T @ D_inv12 @ X @ W.T print(f&#39; nH = D_inv12 @ A.T @ D_inv12 @ X @ W.T {H.shape}&#39;) print(H) . H = D_inv12 @ A.T @ D_inv12 @ X @ W.T (34, 34) [[0.05882353 0.0766965 0.07312724 ... 0.09166985 0. 0. ] [0.0766965 0.1 0.09534626 ... 0. 0. 0. ] [0.07312724 0.09534626 0.09090909 ... 0. 0.0836242 0. ] ... [0.09166985 0. 0. ... 0.14285714 0.10482848 0.08908708] [0. 0. 0.0836242 ... 0.10482848 0.07692308 0.06537205] [0. 0. 0. ... 0.08908708 0.06537205 0.05555556]] . However, edges are not always bidirectional so nodes can have different numbers of neighbors. This is why the result of this operation is different from the one we designed. It is a clever trick to take into account another difference between graphs and images. . Actually, this layer is the only difference between GNNs and regular neural networks. Indeed, the result of the GCNConv layer is fed to a classic non-linear activation function, such as $tanh$ or $ReLU$ (otherwise it&#39;s just a fancy linear regression). Finally, since it&#39;s a classification task, we add a linear layer $Z$ with 4 neurons (our 4 groups) that outputs logits. In these logits, the maximum value of each row determines the class of a node. . $$X to H(X) to ReLU(H(X)) to Z(ReLU(H(X)))$$ . The definition of a GNN looks like a regular PyTorch code, with the addition of the GCNConv layer (courtesy of PyG). Here is a simple example with only one layer: . import torch from torch.nn import Linear from torch_geometric.nn import GCNConv class GNN(torch.nn.Module): def __init__(self): super().__init__() self.gcn = GCNConv(dataset.num_features, 3) self.out = Linear(3, dataset.num_classes) def forward(self, x, edge_index): h = self.gcn(x, edge_index) embedding = torch.relu(h) z = self.out(embedding) return h, embedding, z model = GNN() print(model) . GNN( (gcn): GCNConv(34, 3) (out): Linear(in_features=3, out_features=4, bias=True) ) . As you can imagine, adding a second GCN layer would allow the network to not only aggregate feature vectors from the neighbors of each node, but also from the neighbors of these neighbors. We can stack several message passing layers to aggregate more and more distant values, but there&#39;s a catch: if we add too many layers, the aggregation becomes so intense that all the nodes end up looking the same despite being vastly different. This phenomenon is called over-smoothing and can be a real problem. Fortunately, we don&#39;t have to worry about it in this example because we only have one layer. . Now that we&#39;ve defined our GNN, let&#39;s write a simple training loop with PyTorch. I chose a regular cross-entropy loss since it&#39;s a multi-class classification task, with Adam as optimizer. We could use the data.train_mask, but since there is no test data, we can solemnly ignore it for the purpose of this exploratory exercise. The training loop is very standard: we try to predict the correct labels, and we compare the results given by $Z(ReLU(H))$ to the labels data.y from the dataset. The error calculated by the cross-entropy loss is backpropagated with Adam to fine-tune our GNN&#39;s weights and biases. Finally, we print metrics to see the progression every 10 epochs. . model = GNN() criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.02) # Calculate accuracy def accuracy(pred_y, y): return (pred_y == y).sum() / len(y) # Data for animations embeddings = [] losses = [] accuracies = [] outputs = [] # Training loop for epoch in range(201): # Clear gradients optimizer.zero_grad() # Forward pass h, embedding, z = model(data.x, data.edge_index) # Calculate loss function loss = criterion(z, data.y) # Calculate accuracy acc = accuracy(z.argmax(dim=1), data.y) # Compute gradients loss.backward() # Tune parameters optimizer.step() # Store data for animations embeddings.append(embedding) losses.append(loss) accuracies.append(acc) outputs.append(z.argmax(dim=1)) # Print metrics every 10 epochs if epoch % 10 == 0: print(f&#39;Epoch {epoch:&gt;3} | Loss: {loss:.2f} | Acc: {acc*100:.2f}%&#39;) . Epoch 0 | Loss: 1.35 | Acc: 38.24% Epoch 10 | Loss: 1.21 | Acc: 38.24% Epoch 20 | Loss: 1.08 | Acc: 41.18% Epoch 30 | Loss: 0.92 | Acc: 70.59% Epoch 40 | Loss: 0.72 | Acc: 73.53% Epoch 50 | Loss: 0.54 | Acc: 88.24% Epoch 60 | Loss: 0.41 | Acc: 88.24% Epoch 70 | Loss: 0.33 | Acc: 88.24% Epoch 80 | Loss: 0.29 | Acc: 88.24% Epoch 90 | Loss: 0.26 | Acc: 88.24% Epoch 100 | Loss: 0.24 | Acc: 88.24% Epoch 110 | Loss: 0.23 | Acc: 88.24% Epoch 120 | Loss: 0.22 | Acc: 88.24% Epoch 130 | Loss: 0.22 | Acc: 88.24% Epoch 140 | Loss: 0.21 | Acc: 88.24% Epoch 150 | Loss: 0.20 | Acc: 88.24% Epoch 160 | Loss: 0.20 | Acc: 91.18% Epoch 170 | Loss: 0.19 | Acc: 97.06% Epoch 180 | Loss: 0.17 | Acc: 100.00% Epoch 190 | Loss: 0.14 | Acc: 100.00% Epoch 200 | Loss: 0.12 | Acc: 100.00% . Great! Without much surprise, we reach 100% accuracy on the training set. It means that our model learned to correctly assign every member of the karate club to its correct group. We can do something quite cool by animating the graph to see the evolution of the GNN&#39;s predictions during the training process. . %%capture from IPython.display import HTML from matplotlib import animation plt.rcParams[&quot;animation.bitrate&quot;] = 3000 def animate(i): G = to_networkx(data, to_undirected=True) nx.draw_networkx(G, pos=nx.spring_layout(G, seed=0), with_labels=True, node_size=800, node_color=outputs[i], cmap=&quot;hsv&quot;, vmin=-2, vmax=3, width=0.8, edge_color=&quot;grey&quot;, font_size=14 ) plt.title(f&#39;Epoch {i} | Loss: {losses[i]:.2f} | Acc: {accuracies[i]*100:.2f}%&#39;, fontsize=18, pad=20) fig = plt.figure(figsize=(12, 12)) plt.axis(&#39;off&#39;) anim = animation.FuncAnimation(fig, animate, np.arange(0, 200, 10), interval=500, repeat=True) html = HTML(anim.to_html5_video()) . display(html) . Your browser does not support the video tag. The GNN struggles a bit at the beginning, but it perfectly labels every node after a while. Indeed, the final graph is the same than the one we plotted at the end of the first part of this article. Okay this is nice, but what does the GNN really learn? . We talked about message passing as a generalized convolution operation, where feature vectors from each neighboring node contribute to a better understanding of the &quot;central&quot; node. This is exactly what GNNs learn: a representation of the information of every node, also called an embedding. In our model, the linear classifier $Z$ on top of it just learns how to use these embeddings to produce the best classifications, but the embeddings are the real product of GNNs. . Let&#39;s print the 3D embeddings for each of the 34 nodes with the result of the ReLU layer $ReLU(H(x))$, aptly named embedding. . print(f&#39;Final embeddings = {embedding.shape}&#39;) print(embedding) . Final embeddings = torch.Size([34, 3]) tensor([[2.3756e+00, 5.1330e-01, 0.0000e+00], [3.2511e+00, 1.4347e+00, 0.0000e+00], [2.0562e+00, 1.5209e+00, 0.0000e+00], [2.9461e+00, 1.1436e+00, 0.0000e+00], [0.0000e+00, 0.0000e+00, 0.0000e+00], [0.0000e+00, 0.0000e+00, 0.0000e+00], [0.0000e+00, 0.0000e+00, 0.0000e+00], [2.4295e+00, 1.0172e+00, 0.0000e+00], [6.3575e-01, 2.6594e+00, 0.0000e+00], [1.9876e+00, 1.3767e+00, 0.0000e+00], [0.0000e+00, 6.0713e-04, 0.0000e+00], [2.2577e+00, 1.1747e+00, 0.0000e+00], [2.3823e+00, 1.1449e+00, 0.0000e+00], [2.2940e+00, 1.3844e+00, 0.0000e+00], [1.9155e-01, 2.7673e+00, 0.0000e+00], [1.8206e-01, 2.7194e+00, 0.0000e+00], [0.0000e+00, 2.0684e-03, 0.0000e+00], [2.3367e+00, 1.1042e+00, 0.0000e+00], [1.7925e-01, 2.7942e+00, 0.0000e+00], [2.0630e+00, 1.4096e+00, 0.0000e+00], [1.9360e-01, 2.7587e+00, 0.0000e+00], [2.2845e+00, 1.1088e+00, 0.0000e+00], [1.8486e-01, 2.7376e+00, 0.0000e+00], [0.0000e+00, 2.8447e+00, 0.0000e+00], [0.0000e+00, 8.9724e-01, 0.0000e+00], [0.0000e+00, 9.5606e-01, 0.0000e+00], [2.1157e-01, 2.8055e+00, 0.0000e+00], [2.6385e-01, 2.4765e+00, 0.0000e+00], [2.9965e-01, 8.5145e-01, 0.0000e+00], [0.0000e+00, 3.3316e+00, 0.0000e+00], [4.0497e-01, 2.8716e+00, 0.0000e+00], [0.0000e+00, 6.8132e-01, 0.0000e+00], [0.0000e+00, 4.1963e+00, 0.0000e+00], [0.0000e+00, 3.8991e+00, 0.0000e+00]], grad_fn=&lt;ReluBackward0&gt;) . As you can see, embeddings do not need to have the same dimensions as feature vectors. Here, I chose to reduce the number of dimensions from 34 (dataset.num_features) to 3 to get a nice visualization in 3D. Let&#39;s plot these embeddings before any training happens, at epoch 0. . embed = embeddings[0].detach().cpu().numpy() fig = plt.figure(figsize=(12, 12)) ax = fig.add_subplot(projection=&#39;3d&#39;) ax.patch.set_alpha(0) plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False) ax.scatter(embed[:, 0], embed[:, 1], embed[:, 2], s=200, c=data.y, cmap=&quot;hsv&quot;, vmin=-2, vmax=3) plt.show() . Okay, we see every node from our graph, with the color corresponding to its real group (and not the model&#39;s prediction). For now, they&#39;re all over the place since the GNN is not trained yet. But if we plot these embeddings at each step of the training loop, we&#39;d be able to visualize what the GNN truly learns. We would see how the embeddings change over time, as the GNN gets better and better at classifying nodes. . This is exactly what the following animation does. . %%capture def animate(i): embed = embeddings[i].detach().cpu().numpy() ax.clear() ax.scatter(embed[:, 0], embed[:, 1], embed[:, 2], s=200, c=data.y, cmap=&quot;hsv&quot;, vmin=-2, vmax=3) plt.title(f&#39;Epoch {i} | Loss: {losses[i]:.2f} | Acc: {accuracies[i]*100:.2f}%&#39;, fontsize=18, pad=40) fig = plt.figure(figsize=(12, 12)) plt.axis(&#39;off&#39;) ax = fig.add_subplot(projection=&#39;3d&#39;) plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False) anim = animation.FuncAnimation(fig, animate, np.arange(0, 200, 10), interval=800, repeat=True) html = HTML(anim.to_html5_video()) . display(html) . Your browser does not support the video tag. Let&#39;s unpack it. During training, we know that GNNs learn to produce the best embeddings possible for a specific task. In practice, we see that it means nodes from the same class are grouped into nice clusters. It makes the job of the linear classification layer quite simple because you can easily draw planes to separate each cluster. . Embeddings are not unique to GNNs: they can be found everywhere in deep learning. They don&#39;t have to be 3D either: actually, they rarely are. For instance, language models like BERT produce embeddings with 768 or even 1024 dimensions (and other models go much higher!). Additional dimensions allow to store more information about nodes, text, images, etc. but they also create bigger models that are more difficult and take longer to train. This is why it&#39;s better to keep low-dimensional embeddings as long as it&#39;s possible. . &#128282; IV. Conclusion . Graph Neural Networks are a powerful architecture that is less difficult to understand than at first glance. In this article, . We learned to use the PyTorch Geometric library to explore graph data with the Datasets and Data objects; | We redesigned a message passing layer from scratch, based on a tweaked convolutional layer to work with graphs and not just images; | We implemented a real GNN with a GCN layer that is very close to our own design; | We visualized what and how our GNN learns with the clustering of the node embeddings. | . Zachary&#39;s karate club is not the best graph dataset (feature vectors are useless), but it is good enough to understand the most important concepts in graph data and GNNs. We only talked about node classification in this work, but there are other tasks GNNs can accomplish: link prediction (e.g., to recommend a friend), graph classification (e.g., to label molecules), etc. Beyond GCN, other GNN layers and architectures were proposed by researchers. In the next article, we&#39;re gonna talk about Graph Attention Networks (GATs), which implicitly compute GCN&#39;s normalization factor and the importance of each connection with an attention mechanism. . If you enjoyed this article, please share it with your colleagues and follow me on Twitter for more cool visualizations. Thank you and have a great day! üì£ .",
            "url": "https://mlabonne.github.io/blog/intrognn/",
            "relUrl": "/intrognn/",
            "date": " ‚Ä¢ Feb 20, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Q-learning for beginners",
            "content": "The goal of this article is to teach an AI how to solve the ‚ùÑÔ∏èFrozen Lake environment using reinforcement learning. Instead of reading Wikipedia articles and explaining formulas, we&#39;re going to start from scratch and try to recreate the ü§ñQ-learning algorithm by ourselves. We&#39;ll not just understand how it works, but more importantly why it works: why was it designed that way? What are the hidden assumptions, the details that are never explained in regular courses and tutorials? . At the end of this article, you&#39;ll master the Q-learning algorithm and be able to apply it to other environments and real-world problems. It&#39;s a cool mini-project that gives a better insight into how reinforcement learning works and can hopefully inspire ideas for original and creative applications. . Let&#39;s start by installing the ‚ùÑÔ∏èFrozen Lake environment and importing the necessary libraries: gym for the game, random to generate random numbers, and numpy to do some math. . !pip install -q gym !pip install -q matplotlib import gym import random import numpy as np . &#10052;&#65039; I. Frozen Lake . Now, let&#39;s talk about the game we&#39;re going to be solving in this tutorial. ‚ùÑÔ∏èFrozen Lake is a simple environment composed of tiles, where the AI has to move from an initial tile to a goal. Tiles can be a safe frozen lake ‚úÖ, or a hole ‚ùå that gets you stuck forever. The AI, or agent, has 4 possible actions: go ‚óÄÔ∏èLEFT, üîΩDOWN, ‚ñ∂Ô∏èRIGHT, or üîºUP. The agent must learn to avoid holes in order to reach the goal in a minimal number of actions. By default, the environment is always in the same configuration. In the environment&#39;s code, each tile is represented by a letter as follows: . S F F F (S: starting point, safe) F H F H (F: frozen surface, safe) F F F H (H: hole, stuck forever) H F F G (G: goal, safe) . &lt;/source&gt; We can try to manually solve the example above to understand the game. Let&#39;s see if the following sequence of actions is a correct solution: RIGHT $ to$ RIGHT $ to$ RIGHT $ to$ DOWN $ to$ DOWN $ to$ DOWN. Our agent starts on tile S, so we move right on a frozen surface ‚úÖ, then again ‚úÖ, then once more ‚úÖ, then we go down and find a hole ‚ùå. . Actually, it&#39;s really easy to find several correct solutions: RIGHT $ to$ RIGHT $ to$ DOWN $ to$ DOWN $ to$ DOWN $ to$ RIGHT is an obvious one. But we could make a sequence of actions that loops around a hole 10 times before reaching the goal. This sequence is valid, but it doesn&#39;t meet our final requirement: the agent needs to meet the goal in a minimum number of actions. In this example, the minimum number of actions to complete the game is 6. We need to remember this fact to check if our agent really masters ‚ùÑÔ∏èFrozen Lake or not. . &lt;/source&gt; Let&#39;s initialize the environment thanks to the gym library. There are two versions of the game: one with slippery ice, where selected actions have a random chance of being disregarded by the agent; and a non-slippery one, where actions cannot be ignored. We&#39;ll use the non-slippery one to begin with because it&#39;s easier to understand. . environment = gym.make(&quot;FrozenLake-v1&quot;, is_slippery=False) environment.reset() environment.render() . SFFF FHFH FFFH HFFG . We can see that the game that was created has the exact same configuration as in our example: it is the same puzzle. The position of our agent is indicated by a red rectangle. Solving this puzzle can be done with a simple script and if...else conditions, which would actually be useful to compare our AI to a simpler approach. However, we want to try a more exciting solution: reinforcement learning. . &#127937; II. Q-table . In ‚ùÑÔ∏èFrozen Lake, there are 16 tiles, which means our agent can be found in 16 different positions, called states. For each state, there are 4 possible actions: go ‚óÄÔ∏èLEFT, üîΩDOWN, ‚ñ∂Ô∏èRIGHT, and üîºUP. Learning how to play Frozen Lake is like learning which action you should choose in every state. To know which action is the best in a given state, we would like to assign a quality value to our actions. We have 16 states and 4 actions, so want to calculate $16 times 4 = 64$ values. . A nice way of representing it is using a table, known as a Q-table, where rows list every state $s$ and columns list every action $a$. In this Q-table, each cell contains a value $Q(s, a)$, which is the value (quality) of the action $a$ in the state $s$ (1 if it&#39;s the best action possible, 0 if it&#39;s really bad). When our agent is in a particular state $s$, it just has to check this table to see which action has the highest value. Taking the action with the highest value makes sense but we&#39;ll see later that we can design something even better... . State ‚óÄÔ∏èLEFT üîΩDOWN ‚ñ∂Ô∏èRIGHT üîºUP . S=0 | Q(0, ‚óÄÔ∏è) | Q(0, üîΩ) | Q(0, ‚ñ∂Ô∏è) | Q(0, üîº) | . 1 | Q(1, ‚óÄÔ∏è) | Q(1, üîΩ) | Q(1, ‚ñ∂Ô∏è) | Q(1, üîº) | . 2 | Q(2, ‚óÄÔ∏è) | Q(2, üîΩ) | Q(2, ‚ñ∂Ô∏è) | Q(2, üîº) | . ... | ... | ... | ... | ... | . 14 | Q(14, ‚óÄÔ∏è) | Q(14, üîΩ) | Q(14, ‚ñ∂Ô∏è) | Q(14, üîº) | . G=15 | Q(15, ‚óÄÔ∏è) | Q(15, üîΩ) | Q(15, ‚ñ∂Ô∏è) | Q(15, üîº) | . Example of Q-table, where each cell contains the value $Q(a, s)$ of the action $a$ (column) in a given state $s$ (row) . Let&#39;s create our Q-table and fill it with zeros since we still have no idea of the value of each action in each state. . # Our table has the following dimensions: # (rows x columns) = (states x actions) = (16 x 4) qtable = np.zeros((16, 4)) # Alternatively, the gym library can also directly g # give us the number of states and actions using # &quot;env.observation_space.n&quot; and &quot;env.action_space.n&quot; nb_states = environment.observation_space.n # = 16 nb_actions = environment.action_space.n # = 4 qtable = np.zeros((nb_states, nb_actions)) # Let&#39;s see how it looks print(&#39;Q-table =&#39;) print(qtable) . Q-table = [[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]] . Great! We have our Q-table with 16 rows (our 16 states) and 4 columns (our 4 actions) as expected. Let&#39;s try to see what we can do next: every value is set to zero, so we have no information at all. Let&#39;s say that the agent takes a random action: ‚óÄÔ∏èLEFT, üîΩDOWN, ‚ñ∂Ô∏èRIGHT, or üîºUP. . We can use the random library with the choice method to randomly choose an action. . random.choice([&quot;LEFT&quot;, &quot;DOWN&quot;, &quot;RIGHT&quot;, &quot;UP&quot;]) . &#39;LEFT&#39; . Wait, actually the agent is currently on the initial state S, which means only two actions are possible: ‚ñ∂Ô∏èRIGHT and üîΩDOWN. The agent can also take the actions üîºUP and ‚óÄÔ∏èLEFT, but it won&#39;t move: its state doesn&#39;t change. Therefore, we do not put any constraint on what actions are possible: the agent will naturally understand that some of them don&#39;t do anything. . We can keep using random.choice(), but the gym library already implements a method to randomly choose an action. It might save us some hassle later, so let&#39;s try it. . environment.action_space.sample() . 0 . Oops... this time it&#39;s a number. We could read gym&#39;s documentation but it is quite scarce unfortunately. No worries though, we can check the source code on GitHub to understand what these numbers mean. It&#39;s actually super straightforward: . ‚óÄÔ∏è LEFT = 0 üîΩ DOWN = 1 ‚ñ∂Ô∏è RIGHT = 2 üîº UP = 3 . &lt;/source&gt; Okay, now that we understand how gym connects numbers to directions, let&#39;s try to use it to move our agent to the right ‚ñ∂Ô∏è. This time, it can be performed using the step(action) method. We can try to directly provide it the number 2, corresponding to the direction we chose (right), and check if the agent moved. . environment.step(2) environment.render() . (Right) SFFF FHFH FFFH HFFG . Huzzah! The red square moved from the initial state S to the right: our prediction was correct. And that&#39;s all we need to know in order to interact with the environment: . How to randomly choose an action using action_space.sample(); | How to implement this action and move our agent in the desired direction with step(action). | To be completely exhaustive, we can add: . How to display the current map to see what we&#39;re doing with render(); | How to restart the game when the agent falls into a hole or reaches the goal G with reset(). | Now that we understand how to interact with our gym environment, let&#39;s go back to our algorithm. In reinforcement learning, agents are rewarded by the environment when they accomplish a predefined goal. In ‚ùÑÔ∏èFrozen Lake, the agent is only rewarded when it reaches the state G (see the source code). We cannot control this reward, it is set in the environment: it&#39;s 1 when the agent reaches G, and 0 otherwise. . Let&#39;s print it every time we implement an action. The reward is given by the method step(action). . action = environment.action_space.sample() # 2. Implement this action and move the agent in the desired direction new_state, reward, done, info = environment.step(action) # Display the results (reward and map) environment.render() print(f&#39;Reward = {reward}&#39;) . (Left) SFFF FHFH FFFH HFFG Reward = 0.0 . The reward is indeed 0... üò± wow, I guess we&#39;re in a pickle, because only one state can give us a positive reward in the entire game. How are we supposed to take the right directions at the very beginning when the only validation we have is at the very end? If we ever want to see a reward of 1, we&#39;d need to be lucky enough to find the correct sequence of actions by chance. Unfortunately, that&#39;s exactly how it works... the Q-table will remain filled with zeros until the agent randomly reaches the goal G. . The problem would be much simpler if we could have intermediate, smaller rewards to guide our path towards the goal G. Alas, this is actually one of the main issues of reinforcement learning: this phenomenon, called sparse rewards, makes agents very difficult to train on problems where the only reward is at the end of a long sequence of actions. Different techniques were proposed to mitigate this issue, but we&#39;ll talk about it another time. . &#129302; III. Q-learning . Let&#39;s go back to our problem. Okay, we need to be lucky enough to find the goal G by accident. But once it&#39;s done, how to backpropagate the information to the initial state? The ü§ñQ-learning algorithm offers a clever solution to this issue. We need to update the value of our state-action pairs (each cell in the Q-table) considering 1/ the reward for reaching the next state, and 2/ the highest possible value in the next state. . &lt;/source&gt; We know we get a reward of 1 when we move to G. As we just said, the value of the state next to G (let&#39;s call it G-1) with the relevant action to reach G is increased thanks to the reward. Okay good, end of the episode: the agent won and we restart the game. Now, the next time the agent is in a state next to G-1, it will increase the value of this state (let&#39;s call it G-2) with the relevant action to reach G-1. The next time the agent is in a state next to G-2, it will do the same. Rinse and repeat, until the update reaches the initial state S. . Let&#39;s try to find the update formula to backpropagate the values from G to S. Remember: values denote the quality of an action in a specific state (0 if it&#39;s terrible, 1 if it&#39;s the best action possible in this state). We try to update the value of the action $a_t$ (for example, $a_t = 0$ if the action is left) in the state $s_t$ (for example, $s_t = 0$ when the agent is in the initial state S). This value is just a cell in our Q-table, corresponding to the row number $s_t$ and the column number $a_t$: this value is formally called $Q(s_t, a_t)$. . As we said previously, we need to update it using 1/ the reward for the next state (formally noted $r_t$), and 2/ the maximum possible value in the next state ($max_aQ(s_{t+1},a)$). Therefore, the update formula must look like: . $$Q_{new}(s_t, a_t) = Q(s_t, a_t) + r_t + max_aQ(s_{t+1}, a)$$ . The new value is the current one + the reward + the highest value in the next state. We can manually try our formula to check if it looks correct: let&#39;s pretend our agent is in the state G-1 next to the goal G for the first time. We can update the value corresponding to the winning action in this state G-1 with $Q_{new}(G-1, a_t) = Q(G-1, a_t) + r_t + max_aQ(G, a)$, where $Q(G-1, a_t) = 0$ and $max_aQ(G, a) = 0$ because the Q-table is empty, and $r_t = 1$ because we get the only reward in this environment. We obtain $Q_{new}(G-1, a_t) = 1$. The next time the agent is in a state next to this one (G-2), we update it too using the formula and get the same result: $Q_{new}(G-2, a_t) = 1$. In the end, we backpropagate ones in the Q-table from G to S. Okay it works, but the result is binary: either it&#39;s the wrong state-action pair or the best one. We would like more nuance... . Actually, we almost found the true Q-learning update formula with common sense. The nuance we&#39;re looking for adds two parameters: . $ alpha$ is the üí°learning rate (between $0$ and $1$), which is how much we should change the original $Q(s_t, a_t)$ value. If $ alpha = 0$, the value never changes, but if $ alpha = 1$, the value changes extremely fast. In our attempt, we didn&#39;t limit the learning rate so $ alpha = 1$. But this is too fast in reality: the reward and the maximum value in the next state quickly overpower the current value. We need to find a balance between the importance of past and new knowledge. | $ gamma$ is the üìâdiscount factor (between $0$ and $1$), which determines how much the agent cares about future rewards compared to immediate ones (as the saying goes, &quot;a bird in the hand is worth two in the bush&quot;). If $ gamma = 0$, the agent only focuses on immediate rewards , but if $ gamma = 1$, any potential future reward has the same value than current ones. In ‚ùÑÔ∏èFrozen Lake, we want a high discount factor since there&#39;s only one possible reward at the very end of the game. | . With the real Q-learning algorithm, the new value is calculated as follows: . $$Q_{new}(s_t, a_t) = Q(s_t, a_t) + alpha cdot (r_t + gamma cdot max_aQ(s_{t+1},a) - Q(s_t, a_t))$$ . Okay, let&#39;s try this new formula before implementing it. Once again, we can pretend that our agent is next to the goal G for the first time. We can update the state-action pair to win the game using our formula: $Q_{new}(G-1, a_t) = 0 + alpha cdot (1 + gamma cdot 0 - 0)$. We can assign arbitrary values to $ alpha$ and $ gamma$ to calculate the result. With $ alpha = 0.5$ and $ gamma = 0.9$, we get $Q_{new}(G-1, a_t) = 0 + 0.5 cdot (1 + 0.9 cdot 0 - 0) = 0.5$. The second time the agent is in this state, we would get: $Q_{new}(G-1, a_t) = 0.5 + 0.5 cdot (1 + 0.9 cdot 0 - 0.5) = 0.75$, then $0.875$, $0.9375$, $0.96875$, etc. . &lt;/source&gt; So training our agent in code means: . Choosing a random action (using action_space.sample()) if the values in the current state are just zeros. Otherwise, we take the action with the highest value in the current state with the function np.argmax(); | Implementing this action by moving in the desired direction with step(action); | Updating the value of the original state with the action we took, using information about the new state and the reward given by step(action); | We keep repeating these 3 steps until the agent gets stuck in a hole or reaches the goal G. When it happens, we just restart the environment with reset() and start a new episode until we hit 1,000 episodes. Additionally, we can plot the outcome of each run (failure if it didn&#39;t reach the goal, success otherwise) to observe the progress of our agent. . import matplotlib.pyplot as plt plt.rcParams[&#39;figure.dpi&#39;] = 300 plt.rcParams.update({&#39;font.size&#39;: 17}) # We re-initialize the Q-table qtable = np.zeros((environment.observation_space.n, environment.action_space.n)) # Hyperparameters episodes = 1000 # Total number of episodes alpha = 0.5 # Learning rate gamma = 0.9 # Discount factor # List of outcomes to plot outcomes = [] print(&#39;Q-table before training:&#39;) print(qtable) # Training for _ in range(episodes): state = environment.reset() done = False # By default, we consider our outcome to be a failure outcomes.append(&quot;Failure&quot;) # Until the agent gets stuck in a hole or reaches the goal, keep training it while not done: # Choose the action with the highest value in the current state if np.max(qtable[state]) &gt; 0: action = np.argmax(qtable[state]) # If there&#39;s no best action (only zeros), take a random one else: action = environment.action_space.sample() # Implement this action and move the agent in the desired direction new_state, reward, done, info = environment.step(action) # Update Q(s,a) qtable[state, action] = qtable[state, action] + alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action]) # Update our current state state = new_state # If we have a reward, it means that our outcome is a success if reward: outcomes[-1] = &quot;Success&quot; print() print(&#39;===========================================&#39;) print(&#39;Q-table after training:&#39;) print(qtable) # Plot outcomes plt.figure(figsize=(12, 5)) plt.xlabel(&quot;Run number&quot;) plt.ylabel(&quot;Outcome&quot;) ax = plt.gca() ax.set_facecolor(&#39;#efeeea&#39;) plt.bar(range(len(outcomes)), outcomes, color=&quot;#0A047A&quot;, width=1.0) plt.show() . Q-table before training: [[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]] =========================================== Q-table after training: [[0. 0. 0.59049 0. ] [0. 0. 0.6561 0. ] [0. 0.729 0. 0. ] [0. 0. 0. 0. ] [0. 0.02050313 0. 0. ] [0. 0. 0. 0. ] [0. 0.81 0. 0. ] [0. 0. 0. 0. ] [0. 0. 0.17085938 0. ] [0. 0. 0.49359375 0. ] [0. 0.9 0. 0. ] [0. 0. 0. 0. ] [0. 0. 0. 0. ] [0. 0. 0. 0. ] [0. 0. 1. 0. ] [0. 0. 0. 0. ]] . The agent is trained! Each blue bar on the figure corresponds to a win, so we can see that the agent had a hard time finding the goal at the beginning of the training. But once it found it several times in a row, it began to consistently win. ü•≥ The trained Q-table is also very interesting: these values indicate the unique sequence of actions the agent learned to reach the goal. . Now let&#39;s see how it performs by evaluating it on 100 episodes. We consider that the training is over, so we don&#39;t need to update the Q-table anymore. To see how the agent performs, we can calculate the percentage of times the it managed to reach the goal (success rate). . episodes = 100 nb_success = 0 # Evaluation for _ in range(100): state = environment.reset() done = False # Until the agent gets stuck or reaches the goal, keep training it while not done: # Choose the action with the highest value in the current state if np.max(qtable[state]) &gt; 0: action = np.argmax(qtable[state]) # If there&#39;s no best action (only zeros), take a random one else: action = environment.action_space.sample() # Implement this action and move the agent in the desired direction new_state, reward, done, info = environment.step(action) # Update our current state state = new_state # When we get a reward, it means we solved the game nb_success += reward # Let&#39;s check our success rate! print (f&quot;Success rate = {nb_success/episodes*100}%&quot;) . Success rate = 100.0% . Not only our agent has been trained, but it manages to hit a 100% success rate. Great job everyone, the non-slippery ‚ùÑÔ∏èFrozen Lake is solved! . We can even visualize the agent moving on the map by executing the code below and print the sequence of actions it took to check if it&#39;s the best one. . from IPython.display import clear_output import time state = environment.reset() done = False sequence = [] while not done: # Choose the action with the highest value in the current state if np.max(qtable[state]) &gt; 0: action = np.argmax(qtable[state]) # If there&#39;s no best action (only zeros), take a random one else: action = environment.action_space.sample() # Add the action to the sequence sequence.append(action) # Implement this action and move the agent in the desired direction new_state, reward, done, info = environment.step(action) # Update our current state state = new_state # Update the render clear_output(wait=True) environment.render() time.sleep(1) print(f&quot;Sequence = {sequence}&quot;) . (Right) SFFF FHFH FFFH HFFG Sequence = [2, 2, 1, 1, 1, 2] . The agent can learn several correct sequence of actions: [2, 2, 1, 1, 1, 2], [1, 1, 2, 2, 1, 2], etc. The good thing is there&#39;s only 6 actions in our sequence, which was the minimum possible number of actions we counted: it means that our agent learned to solve the game in an optimal way. In the case of [2, 2, 1, 1, 1, 2], which corresponds to RIGHT $ to$ RIGHT $ to$ DOWN $ to$ DOWN $ to$ DOWN $ to$ RIGHT, it&#39;s exactly the sequence we predicted at the very beginning of the article. üì£ . &#128208; IV. Epsilon-Greedy algorithm . Despite this success, there&#39;s something that bothers me with our previous approach: the agent always chooses the action with the highest value. So whenever a state-action pair starts having a non-zero value, the agent will always choose it. The other actions will never be taken, which means we&#39;ll never update their value... But what if one of these actions was better than the one the agent always takes? Shouldn&#39;t we encourage the agent to try news things from time to time and see if it can improve? . In other words, we want to allow our agent to either: . Take the action with the highest value (exploitation); | Choose a random action to try to find even better ones (exploration). | . A tradeoff between these two behaviors is important: if the agent only focuses on exploitation, it cannot try new solutions and thus doesn&#39;t learn anymore. On the other hand, if the agent only takes random actions, the training is pointless since it doesn&#39;t use the Q-table. So we want to change this parameter over time: at the beginning of the training, we want to explore the environment as much as possible. But exploration becomes less and less interesting, as the agent already knows every possible state-action pairs. This parameter represents the amount of randomness in the action selection. . This technique is commonly called the epsilon-greedy algorithm, where epsilon is our parameter. It is a simple but extremely efficient method to find a good tradeoff. Every time the agent has to take an action, it has a probability $Œµ$ of choosing a random one, and a probability $1-Œµ$ of choosing the one with the highest value. We can decrease the value of epsilon at the end of each episode by a fixed amount (linear decay), or based on the current value of epsilon (exponential decay). . &lt;/source&gt; Let&#39;s implement a linear decay. Beforehand, I&#39;d like to see how the curve looks like with arbitrary parameters. We&#39;ll start with $Œµ = 1$ to be in full exploration mode, and decrease this value by $0.001$ after each episode. . Okay now that we have a sound understanding of it, we can implement it for real and see how it changes the agent&#39;s behavior. . qtable = np.zeros((environment.observation_space.n, environment.action_space.n)) # Hyperparameters episodes = 1000 # Total number of episodes alpha = 0.5 # Learning rate gamma = 0.9 # Discount factor epsilon = 1.0 # Amount of randomness in the action selection epsilon_decay = 0.001 # Fixed amount to decrease # List of outcomes to plot outcomes = [] print(&#39;Q-table before training:&#39;) print(qtable) # Training for _ in range(episodes): state = environment.reset() done = False # By default, we consider our outcome to be a failure outcomes.append(&quot;Failure&quot;) # Until the agent gets stuck in a hole or reaches the goal, keep training it while not done: # Generate a random number between 0 and 1 rnd = np.random.random() # If random number &lt; epsilon, take a random action if rnd &lt; epsilon: action = environment.action_space.sample() # Else, take the action with the highest value in the current state else: action = np.argmax(qtable[state]) # Implement this action and move the agent in the desired direction new_state, reward, done, info = environment.step(action) # Update Q(s,a) qtable[state, action] = qtable[state, action] + alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action]) # Update our current state state = new_state # If we have a reward, it means that our outcome is a success if reward: outcomes[-1] = &quot;Success&quot; # Update epsilon epsilon = max(epsilon - epsilon_decay, 0) print() print(&#39;===========================================&#39;) print(&#39;Q-table after training:&#39;) print(qtable) # Plot outcomes plt.figure(figsize=(12, 5)) plt.xlabel(&quot;Run number&quot;) plt.ylabel(&quot;Outcome&quot;) ax = plt.gca() ax.set_facecolor(&#39;#efeeea&#39;) plt.bar(range(len(outcomes)), outcomes, color=&quot;#0A047A&quot;, width=1.0) plt.show() . Q-table before training: [[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]] =========================================== Q-table after training: [[0.531441 0.59049 0.59049 0.531441 ] [0.531441 0. 0.6561 0.56396466] [0.58333574 0.729 0.56935151 0.65055117] [0.65308668 0. 0.33420534 0.25491326] [0.59049 0.6561 0. 0.531441 ] [0. 0. 0. 0. ] [0. 0.81 0. 0.65519631] [0. 0. 0. 0. ] [0.6561 0. 0.729 0.59049 ] [0.6561 0.81 0.81 0. ] [0.72899868 0.9 0. 0.72711067] [0. 0. 0. 0. ] [0. 0. 0. 0. ] [0. 0.81 0.9 0.729 ] [0.81 0.9 1. 0.81 ] [0. 0. 0. 0. ]] . Hey, the agent takes more time to consistently win the game now! And the Q-table has a lot more non-zero values than the previous one, which means the agent has learned several sequences of actions to reach the goal. It is understandable, since this new agent is forced to explore state-action pairs instead of always exploiting ones with non-zero values. . Let&#39;s see if it&#39;s as successful as the previous one to win the game. In evaluation mode, we don&#39;t want exploration anymore because the agent is trained now. . episodes = 100 nb_success = 0 # Evaluation for _ in range(100): state = environment.reset() done = False # Until the agent gets stuck or reaches the goal, keep training it while not done: # Choose the action with the highest value in the current state action = np.argmax(qtable[state]) # Implement this action and move the agent in the desired direction new_state, reward, done, info = environment.step(action) # Update our current state state = new_state # When we get a reward, it means we solved the game nb_success += reward # Let&#39;s check our success rate! print (f&quot;Success rate = {nb_success/episodes*100}%&quot;) . Success rate = 100.0% . Phew, it&#39;s another 100% success rate! We didn&#39;t degrade the model. üòå The benefits of this approach might not be obvious in this example, but our model became less static and more flexible. It learned different paths (sequences of actions) from S to G instead of just one as in the previous approach. More exploration can degrade performance but it&#39;s necessary to train agents that can adapt to new environments. . &#10052;&#65039; IV. Challenge: slippery Frozen Lake . We didn&#39;t solve the entire ‚ùÑÔ∏èFrozen Lake environment: we only trained an agent on the non-slippery version, using is_slippery = False during initialization. In the slippery variant, the action the agent takes only has 33% chance of succeeding. In case of failure, one of the three other actions is randomly taken instead. This feature adds a lot of randomness to the training, which makes things more difficult for our agent. Let&#39;s see how well our code is doing in this new environment... . environment = gym.make(&quot;FrozenLake-v1&quot;, is_slippery=True) environment.reset() # We re-initialize the Q-table qtable = np.zeros((environment.observation_space.n, environment.action_space.n)) # Hyperparameters episodes = 1000 # Total number of episodes alpha = 0.5 # Learning rate gamma = 0.9 # Discount factor epsilon = 1.0 # Amount of randomness in the action selection epsilon_decay = 0.001 # Fixed amount to decrease # List of outcomes to plot outcomes = [] print(&#39;Q-table before training:&#39;) print(qtable) # Training for _ in range(episodes): state = environment.reset() done = False # By default, we consider our outcome to be a failure outcomes.append(&quot;Failure&quot;) # Until the agent gets stuck in a hole or reaches the goal, keep training it while not done: # Generate a random number between 0 and 1 rnd = np.random.random() # If random number &lt; epsilon, take a random action if rnd &lt; epsilon: action = environment.action_space.sample() # Else, take the action with the highest value in the current state else: action = np.argmax(qtable[state]) # Implement this action and move the agent in the desired direction new_state, reward, done, info = environment.step(action) # Update Q(s,a) qtable[state, action] = qtable[state, action] + alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action]) # Update our current state state = new_state # If we have a reward, it means that our outcome is a success if reward: outcomes[-1] = &quot;Success&quot; # Update epsilon epsilon = max(epsilon - epsilon_decay, 0) print() print(&#39;===========================================&#39;) print(&#39;Q-table after training:&#39;) print(qtable) # Plot outcomes plt.figure(figsize=(12, 5)) plt.xlabel(&quot;Run number&quot;) plt.ylabel(&quot;Outcome&quot;) ax = plt.gca() ax.set_facecolor(&#39;#efeeea&#39;) plt.bar(range(len(outcomes)), outcomes, color=&quot;#0A047A&quot;, width=1.0) plt.show() episodes = 100 nb_success = 0 # Evaluation for _ in range(100): state = environment.reset() done = False # Until the agent gets stuck or reaches the goal, keep training it while not done: # Choose the action with the highest value in the current state action = np.argmax(qtable[state]) # Implement this action and move the agent in the desired direction new_state, reward, done, info = environment.step(action) # Update our current state state = new_state # When we get a reward, it means we solved the game nb_success += reward # Let&#39;s check our success rate! print (f&quot;Success rate = {nb_success/episodes*100}%&quot;) . Q-table before training: [[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]] =========================================== Q-table after training: [[0.06208723 0.02559574 0.02022059 0.01985828] [0.01397208 0.01425862 0.01305446 0.03333396] [0.01318348 0.01294602 0.01356014 0.01461235] [0.01117016 0.00752795 0.00870601 0.01278227] [0.08696239 0.01894036 0.01542694 0.02307306] [0. 0. 0. 0. ] [0.09027682 0.00490451 0.00793372 0.00448314] [0. 0. 0. 0. ] [0.03488138 0.03987256 0.05172554 0.10780482] [0.12444437 0.12321815 0.06462294 0.07084008] [0.13216145 0.09460133 0.09949734 0.08022573] [0. 0. 0. 0. ] [0. 0. 0. 0. ] [0.1606242 0.18174032 0.16636549 0.11444442] [0.4216631 0.42345944 0.40825367 0.74082329] [0. 0. 0. 0. ]] . Success rate = 17.0% . Oof it&#39;s not so good. But can you improve the performance by tweaking the different parameters we talked about? I encourage you to take this little challenge and do it on your own to have fun with reinforcement learning and check if you understood everything we said about Q-learning. And why not implementing exponential decay for the epsilon-greedy algorithm too? During this quick exercise, you might realise that slightly modifying the hyperparameters can completely destroy the results. This is another quirk of reinforcement learning: hyperparameters are quite moody, and it is important to understand their meaning if you want to tweak them. It&#39;s always good to test and try new combinations to build your intuition and become more efficient. Good luck and have fun! . &#128282; V. Conclusion . Q-learning is a simple yet powerful algorithm at the core of reinforcement learning. In this article, . We learned to interact with the gym environment to choose actions and move our agent; | We introduced the idea of a Q-table, where rows are states, columns are actions, and cells are the value of an action in a given state; | We experimentally recreated the Q-learning update formula to tackle the sparse reward problem; | We implemented an entire training and evaluation process, that solved the ‚ùÑÔ∏èFrozen Lake environment with 100% success rate; | We implemented the famous epsilon-greedy algorithm in order to create a tradeoff between the exploration of unknown state-action pairs and the exploitation of the most successful ones. | . The ‚ùÑÔ∏èFrozen Lake is a very simple environment, but others can have so many states and actions that it becomes impossible to store the Q-table in memory. This is especially the case in environments where events are not discrete, but continuous (like Super Mario Bros. or Minecraft). When the problem arises, a popular technique consists of training a deep neural network to approximate the Q-table. This method adds several layers of complexity, since the neural networks are not very stable. But I will cover it in another tutorial with different techniques to stabilize them. . Until then, share this article if it helped you and follow me on Twitter and Medium for more practical content around machine learning and deep learning. üì£ .",
            "url": "https://mlabonne.github.io/blog/reinforcement%20learning/q-learning/frozen%20lake/gym/tutorial/2022/02/13/Q_learning.html",
            "relUrl": "/reinforcement%20learning/q-learning/frozen%20lake/gym/tutorial/2022/02/13/Q_learning.html",
            "date": " ‚Ä¢ Feb 13, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "üëã Hi, my name is Maxime Labonne and I‚Äôm a research scientist in machine learning &amp; deep learning at ‚úàÔ∏è Airbus Defence and Space. I have a PhD in machine learning applied to cyber security from the Polytechnic Institute of Paris (number 49 in QS World University Rankings). I‚Äôve worked for the French Atomic Energy Commission, where I developed new machine learning architectures dedicated to anomaly detection and that are now sold and used in various industrial, national, and European projects. . In this blog, I want to write tutorials and guides about AI in a different way. I want to start with practice and implementation, and then move on to explanations. Even better, I would like to experimentally retrace the thought process of the original authors of the algorithms we use everyday to understand their design, the importance of each component, rather than using them as black boxes. Implementation is at the core of this process, since it is only by implementing by yourself that you discover all the assumptions and other details that are often ignored, but are nonetheless essential. Finally, I think this process is fun and allows you to practice tackling difficult problems to design new and suitable machine learning solutions. . I hope we can have fun together and learn a few things along the way. :) . If you want to contact me, feel free to send a mail at this address. . . Credits: . Thanks to Hamel Husain and all the contributors of fastpages that powers this blog. License: Apache License 2.0. | Emojis used in figures are designed by OpenMoji, the open-source emoji and icon project. License: CC BY-SA 4.0. | Vector icons are provided by Streamline (https://streamlinehq.com). License: CC BY-SA 4.0. | .",
          "url": "https://mlabonne.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Publications",
          "content": "üìú Thesis . Anomaly-based network intrusion detection using machine learningMaxime LabonnePolytechnic Institute of Paris (Institut Polytechnique de Paris), 2020 | . üìù Proceedings . Anomaly detection in vehicle-to-infrastructure communicationsMichele Russo, Maxime Labonne, Alexis Olivereau, Mohammad Rmayti2018 IEEE 87th Vehicular Technology Conference (VTC Spring) | A Cascade-structured Meta-Specialists Approach for Neural Network-based Intrusion DetectionMaxime Labonne, Alexis Olivereau, Baptiste Polv√©, Djamal Zeghlache2019 16th IEEE Annual Consumer Communications &amp; Networking Conference (CCNC) | Unsupervised protocol-based intrusion detection for real-world networksMaxime Labonne, Alexis Olivereau, Baptise Polv√©, Djamal Zeghlache2020 International Conference on Computing, Networking and Communications (ICNC) | Predicting Bandwidth Utilization on Network Links Using Machine LearningMaxime Labonne, Charalampos Chatzinakis, Alexis Olivereau2020 European Conference on Networks and Communications (EuCNC) | Priority flow admission and routing in sdn: Exact and heuristic approachesJorge L√≥pez, Maxime Labonne, Claude Poletti, Dallal Belabed2020 IEEE 19th International Symposium on Network Computing and Applications (NCA) | Short-Term Flow-Based Bandwidth Forecasting using Machine LearningMaxime Labonne, Jorge L√≥pez, Claude Poletti, Jean-Baptiste Munier2021 IEEE 22nd International Symposium on a World of Wireless, Mobile and Multimedia Networks (WoWMoM) | Toward Formal Data Set Verification for Building Effective Machine Learning ModelsJorge L√≥pez, Maxime Labonne, Claude PolettiKDIR 2021 - 13th International Conference on Knowledge Discovery and Information Retrieval | .",
          "url": "https://mlabonne.github.io/blog/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "",
          "content": "User-agent: * Sitemap: https://mlabonne.github.io/blog/sitemap.xml .",
          "url": "https://mlabonne.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

  

  
  

  
  

  
  

  
  

}