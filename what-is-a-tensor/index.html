<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">What is a Tensor in Machine Learning?</h1><p class="page-description">The difference between tensors, arrays, andÂ matrices</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-03-28T00:00:00-05:00" itemprop="datePublished">
        <img src="/blog/images/icons/icon-watch.svg" class="icon-watch"> Mar 28, 2022
      </time>â€¢ 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Maxime Labonne</span></span>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span>â€¢ <a href= https://colab.research.google.com/drive/1azq12DApWgLgdWuYB3wh0TcwJVAfVxCL?usp=sharing target="_blank"> <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"></a>
    </p>

    

    <!-- 
      
        <div class="badges">
          <div class="runthecode">
            Run the code along the article without installating anything
          </div>
          <div class="d-flex flex-justify-center">
            <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fmlabonne%2Fblog%2Fblob%2Fmaster%2F_notebooks%2F2022-03-28-What_is_a_Tensor_in_Deep_Learning.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/blog/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

            <div class="px-2">
    <a href="https://colab.research.google.com/github/mlabonne/blog/blob/master/_notebooks/2022-03-28-What_is_a_Tensor_in_Deep_Learning.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
            <div class="px-2">
    <a href="https://mybinder.org/v2/gh/mlabonne/blog/master?filepath=_notebooks%2F2022-03-28-What_is_a_Tensor_in_Deep_Learning.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

            <div class="px-2">

    <a href="https://github.com/mlabonne/blog/tree/master/_notebooks/2022-03-28-What_is_a_Tensor_in_Deep_Learning.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          </div>
        </div>
      -->
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-03-28-What_is_a_Tensor_in_Deep_Learning.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<center><img alt="Thumbnail" src="https://mlabonne.github.io/blog/images/tensor/thumbnail.png" /></center><p>What is a tensor, exactly?</p>
<p>Most deep learning practitioners know about them but can't pinpoint an <strong>exact definition</strong>.</p>
<p>TensorFlow, PyTorch: every deep learning framework relies on the same basic object: <strong>tensors</strong>. They're used to store almost everything in deep learning: input data, weights, biases, predictions, etc.</p>
<p>And yet, their definition is incredibly fuzzy: the <a href="https://en.wikipedia.org/wiki/Category:Tensors">Wikipedia category</a> alone has <strong>over 100 pages</strong> related to tensors.</p>
<p>In this article, we'll give a <strong>definitive answer</strong> to the following question: what is a tensor in neural networks?</p>
<h2 id="&#128187;-Tensors-in-computer-science">&#128187; Tensors in computer science<a class="anchor-link" href="#&#128187;-Tensors-in-computer-science"> </a></h2><p>So why are there so many definitions?</p>
<p>It's quite simple: different fields have different definitions. Tensors in <strong>mathematics</strong> are not quite the same as tensors in <strong>physics</strong>, which are different from tensors in <strong>computer science</strong>.</p>
<center><img alt="Data structure vs. Objects" src="https://mlabonne.github.io/blog/images/tensor/datastructure.png" width="800" /></center><p>These definitions can be divided into two categories: tensors as a data structure or as objects (in an <a href="https://en.wikipedia.org/wiki/Object-oriented_programming">object-oriented programming</a> sense).</p>
<ul>
<li><strong>Data structure</strong>: this is the definition we use in computer science. Tensors are <a href="https://en.wikipedia.org/wiki/Array_data_type#Multi-dimensional_arrays">multidimensional arrays</a> that store a specific type of value.</li>
<li><strong>Objects</strong>: this is the definition used in other fields. In <a href="https://en.wikipedia.org/wiki/Tensor_(intrinsic_definition)">mathematics</a> and <a href="https://en.wikipedia.org/wiki/Infinitesimal_strain_theory">physics</a>, tensors are not just a data structure: they also have a list of properties, like a specific product.</li>
</ul>
<p>This is why you see a lot of people (sometimes quite pedantically) saying "<em>tensors are <strong>not</strong> n-dimensional arrays/matrices</em>": they don't talk about data structures, but about <strong>objects with properties</strong>.</p>
<p>Even the same words have <strong>different meanings</strong>. For instance, in computer science, a 2D tensor is a matrix (it's a tensor of rank 2). In linear algebra, a tensor with 2 dimensions means it only stores two values. The rank also has a completely different definition: it is the maximum number of its linearly independent column (or row) vectors.</p>
<p>In computer science, we're only interested in a definition focused on the <strong>data structure</strong>. From this point of view, tensors truly are a generalization in $n$ dimensions of matrices.</p>
<p>But we're still missing an important nuance when talking about tensors specifically in the context of deep learning...</p>
<h2 id="&#129504;-Tensors-in-deep-learning">&#129504; Tensors in deep learning<a class="anchor-link" href="#&#129504;-Tensors-in-deep-learning"> </a></h2><center><img alt="Array vs. Tensor" src="https://mlabonne.github.io/blog/images/tensor/arrayvstensor.png" width="800" />
<i>Icons created by Freepik and smashingstocks - <a href="https://www.flaticon.com/">Flaticon</a></i></center><p>So why are they called "tensors" instead of "multidimensional arrays"? Ok, it is shorter, but is it all there is to it? Actually, people make an <strong>implicit assumption</strong> when they talk about tensors.</p>
<p>PyTorch's <a href="https://pytorch.org/tutorials/beginner/examples_tensor/polynomial_tensor.html#:~:text=PyTorch%3A%20Tensors,-A%20third%20order&amp;text=A%20PyTorch%20Tensor%20is%20basically,used%20for%20arbitrary%20numeric%20computation.">official documentation</a> gives us a practical answer:</p>
<blockquote><p>The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either <strong>CPU or GPU</strong>.</p>
</blockquote>
<p>In deep learning, we need performance to compute a lot of matrix multiplications in a highly parallel way. These matrices (and n-dimensional arrays in general) are generally stored and processed on GPUs to speed up training and inference times.</p>
<p>This is what was missing in our previous definition:tensors in deep learning are not just n-dimensional arrays, there's also the implicit assumption they can be <strong>run on a GPU</strong>.</p>
<h2 id="&#9876;&#65039;-NumPy-vs-PyTorch">&#9876;&#65039; NumPy vs PyTorch<a class="anchor-link" href="#&#9876;&#65039;-NumPy-vs-PyTorch"> </a></h2><p>Let's see the difference between NumPy arrays and PyTorch tensors.</p>
<center><img alt="Sclar, vector, matrix" src="https://mlabonne.github.io/blog/images/tensor/scalar.png" width="800" /></center><p>These two objects are very similar: we can initialize a <strong>1D array</strong> and a <strong>1D tensor</strong> with nearly the same syntax. They also share a lot of methods and can be easily converted into one another.</p>
<p>You can find the code used in this article <a href="https://github.com/mlabonne/how-to-data-science/blob/main/What_is_a_Tensor_in_Deep_Learning.ipynb">at this address</a></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;NumPy Array: </span><span class="si">{</span><span class="n">array</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;PyTorch Tensor: </span><span class="si">{</span><span class="n">tensor</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>NumPy Array: [1 2 3]
PyTorch Tensor: tensor([1, 2, 3])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Initializing 2D arrays and 2D tensors is not more complicated.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;NumPy Array:</span><span class="se">\n</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">PyTorch Tensor:</span><span class="se">\n</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>NumPy Array:
[[1 2 3]
 [4 5 6]]

PyTorch Tensor:
tensor([[1, 2, 3],
        [4, 5, 6]])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We said that the only difference between tensors and arrays was the fact that tensors can be <strong>run on GPUs</strong>. So in the end, this distinction is based on performance. But is this boost that important?</p>
<p>Let's compare the performance between NumPy arrays and PyTorch tensors on matrix multiplication. In the following example, we randomly initialize <strong>4D arrays/tensors and multiply them</strong>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="c1"># 4D arrays</span>
<span class="n">array1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">array2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># 4D tensors</span>
<span class="n">tensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">tensor2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="o">%%</span><span class="n">timeit</span>
<span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">array1</span><span class="p">,</span> <span class="n">array2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>1 loop, best of 5: 1.32 s per loop
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="o">%%</span><span class="n">timeit</span>
<span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>1000 loops, best of 5: 25.2 ms per loop
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As we can see, PyTorch tensors completed outperformed NumPy arrays: they completed the multiplication <strong>52 times faster</strong>!</p>
<p>We could attribute this performance to different factors, such as:</p>
<ul>
<li>NumPy arrays use a <code>float64</code> format, whereas PyTorch tensors leverage the more efficient <code>float32</code> format. However, even when NumPy arrays are converted to <code>float32</code>, PyTorch tensors are still 40 times faster.</li>
<li>PyTorch tensors are stored on a GPU, unlike NumPy arrays. But if we repeat the same experiment on a CPU, PyTorch tensors still manage to be 2.8 times faster on average.</li>
</ul>
<p>Even when combining both factors, PyTorch tensors prove to be 1.4 times faster, showing that NumPy arrays are truly less performant for matrix multiplication.</p>
<p>This is the true power of tensors: they're <strong>blazingly fast</strong>! Performance might vary depending on the dimensions, the implementation, and the hardware, but this speed is the reason why tensors (and not arrays) are so common in deep learning.</p>
<h2 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion"> </a></h2><p>In this article, we wrote a definition of tensors based on:</p>
<ol>
<li>Their use in <strong>computer science</strong> (data structure);</li>
<li>More specifically, in <strong>deep learning</strong> (they can run on GPUs).</li>
</ol>
<p>Here's how we can summarize it in one sentence:</p>
<blockquote><p>Tensors are <strong>n-dimensional arrays</strong> with the implicit assumption that they can <strong>run on a GPU</strong>.</p>
</blockquote>
<p>Finally, we saw the difference in performance between tensors and arrays, which motivates the need for tensors in deep learning.</p>
<p>So next time someone tries to explain to you that tensors are not exactly a generalization of matrices, you'll know that they're right in a particular definition of tensors, but not in the computer science/deep learning one.</p>
<p>If you're looking for more data science and machine learning content in n-dimensions, please <strong>follow me on twitter <a href="https://twitter.com/maximelabonne">@maximelabonne</a></strong>. ðŸ“£</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="mlabonne/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/what-is-a-tensor/" hidden></a>
</article>
